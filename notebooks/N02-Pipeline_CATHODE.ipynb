{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfeee46",
   "metadata": {},
   "source": [
    "# Notebook 02: Pipeline for CATHODE results\n",
    "\n",
    "This notebook goes through the pipeline for obtaining results using the idealized CATHODE (Classifying Anomalies THrough Outer Density Estimation) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f366276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from run_ANODE_training import main as train_DE\n",
    "from run_classifier_data_creation import main as create_data\n",
    "from run_classifier_training import main as train_classifier\n",
    "from run_ANODE_evaluation import main as eval_ANODE\n",
    "from evaluation_utils import full_single_evaluation, classic_ANODE_eval, minimum_val_loss_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c5c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'CATHODE'\n",
    "data_dir = '../separated_data'\n",
    "save_dir = 'CATHODE_models'\n",
    "# Shift on jet mass variables to be applied.\n",
    "datashift = 0.\n",
    "# Shift is not correlated to the actual mjj but randomized.\n",
    "random_shift = False\n",
    "# Whether to apply an (ANODE paper) fiducial cut on the data (and samples).\n",
    "fiducial_cut = False\n",
    "# Suppress the processing of the extra signal sample.\n",
    "no_extra_signal = True\n",
    "verbose = False\n",
    "\n",
    "# ANODE model config file (.yml).\n",
    "DE_config_file = '../DE_MAF_model.yml'\n",
    "# 'Number of Density Estimation training epochs.'\n",
    "DE_epochs = 100\n",
    "# Batch size during density estimation training.\n",
    "DE_batch_size = 256\n",
    "# Skips the density estimation (loads existing files instead).\n",
    "DF_skip = False\n",
    "# Turns off the logit transform in the density estimator.\n",
    "DE_no_logit = False\n",
    "\n",
    "# File name for the density estimator.\n",
    "DE_file_name = 'my_ANODE_model'\n",
    "\n",
    "# Classifier model config file (.yml).\n",
    "cf_config_file = '../classifier.yml'\n",
    "\n",
    "# Number of classifier training epochs\n",
    "cf_epochs = 100\n",
    "# Number of samples to be generated. Currently the samples will be cut down to match data proportion.\n",
    "cf_n_samples = 400000\n",
    "\n",
    "# Sample the conditional from a KDE fit rather than a uniform distribution.\n",
    "cf_realistic_conditional = True\n",
    "# Bandwith of the KDE fit (used when realistic_conditional is selected)\n",
    "cf_KDE_bandwidth = 0.01\n",
    "# Add the full number of samples to the training set rather than mixing it in equal parts with data.\n",
    "cf_oversampling = True\n",
    "# Turns off logit tranform in the classifier.\n",
    "cf_no_logit = True\n",
    "# Space-separated list of pre-sampled npy files of physical variables if the sampling has been done externally. The format is \n",
    "# (mjj, mj1, dmj, tau21_1, tau21_2)\n",
    "cf_external_samples = \"\"\n",
    "# Lower boundary of signal region.\n",
    "cf_SR_min = 3.3\n",
    "# Upper boundary of signal region.\n",
    "cf_SR_max = 3.7\n",
    "# Number of independent classifier training runs.\n",
    "cf_n_runs = 1\n",
    "# Batch size during classifier training.\n",
    "cf_batch_size = 128\n",
    "# Use the conditional variable as classifier input during training.\n",
    "cf_use_mjj = False\n",
    "# Weight the classes according to their occurence in the training set. \n",
    "# Necessary if the training set was intentionally oversampled.'\n",
    "cf_use_class_weights = True\n",
    "# Central value of signal region. Must only be given for using CWoLa with weights.\n",
    "cf_SR_center = 3.5\n",
    "# Make use of extra background (for supervised and idealized AD).\n",
    "cf_extra_bkg = False\n",
    "# Define a separate validation set to pick the classifier epochs.\n",
    "cf_separate_val_set = True\n",
    "# Save the tensorflow model after each epoch instead of saving predictions.\n",
    "cf_save_model = True\n",
    "# Skips the creation of the classifier dataset (loads existing files instead).\n",
    "cf_skip_create = False\n",
    "# Skips the training of the classifier (loads existing files instead).\n",
    "cf_skip_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b287156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from data_handler import LHCORD_data_handler\n",
    "from ANODE_training_utils import train_ANODE, plot_ANODE_losses\n",
    "from density_estimator import DensityEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9233f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_processing_dict(source):\n",
    "    out = {}\n",
    "    out['max'] = source['max']\n",
    "    out['min'] = source['min']\n",
    "    out['mean2'] = source['mean2']\n",
    "    out['std2'] = source['std2']\n",
    "    out['std2_logit_fix'] = source['std2_logit_fix']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f03152",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'config_file': DE_config_file,\n",
    "    'data_dir': data_dir,\n",
    "    'savedir': save_dir,\n",
    "    'datashift': datashift,\n",
    "    'random_shift': random_shift,\n",
    "    'verbose': verbose,\n",
    "    'model_file_name': DE_file_name,\n",
    "    'epochs': DE_epochs,\n",
    "    'batch_size': DE_batch_size,\n",
    "    'no_logit': DE_no_logit,\n",
    "    'inner_model': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517f2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DE(**kwargs):\n",
    "    \n",
    "    # for debugging:\n",
    "    # torch.manual_seed(2104)\n",
    "    # np.random.seed(2104)\n",
    "\n",
    "    # selecting appropriate device\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    print(\"cuda available:\", CUDA)\n",
    "    device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "\n",
    "    # checking for data separation\n",
    "    data_files = os.listdir(kwargs['data_dir'])\n",
    "    if \"innerdata_val.npy\" in data_files:\n",
    "        finer_data_split = True\n",
    "    else:\n",
    "        finer_data_split = False\n",
    "\n",
    "    if finer_data_split:\n",
    "        innerdata_val_path = os.path.join(kwargs['data_dir'], 'innerdata_val.npy')\n",
    "    else:\n",
    "        innerdata_val_path = os.path.join(kwargs['data_dir'], 'innerdata_test.npy')\n",
    "\n",
    "    # data preprocessing\n",
    "    data = LHCORD_data_handler(os.path.join(kwargs['data_dir'], 'innerdata_train.npy'),\n",
    "                               innerdata_val_path,\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_train.npy'),\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_test.npy'),\n",
    "                               None,\n",
    "                               batch_size=kwargs['batch_size'],\n",
    "                               device=device)\n",
    "    if kwargs['datashift'] != 0:\n",
    "        print(\"applying a datashift of\", kwargs['datashift'])\n",
    "        data.shift_data(kwargs['datashift'], constant_shift=False, random_shift=kwargs['random_shift'],\n",
    "                        shift_mj1=True, shift_dm=True, additional_shift=False)\n",
    "    data.preprocess_ANODE_data(no_logit=kwargs['no_logit'],\n",
    "                               no_mean_shift=kwargs['no_logit'])\n",
    "    if kwargs['inner_model']:\n",
    "        train_loader = data.inner_ANODE_datadict_train['loader']\n",
    "        test_loader = data.inner_ANODE_datadict_test['loader']\n",
    "        data_std = data.inner_ANODE_datadict_train['std2_logit_fix']\n",
    "        train_data_processing = prepare_processing_dict(data.inner_ANODE_datadict_train)\n",
    "    else:\n",
    "        train_loader = data.outer_ANODE_datadict_train['loader']\n",
    "        test_loader = data.outer_ANODE_datadict_test['loader']\n",
    "        data_std = data.outer_ANODE_datadict_train['std2_logit_fix']\n",
    "        train_data_processing = prepare_processing_dict(data.outer_ANODE_datadict_train)\n",
    "    pickle.dump(train_data_processing, open(os.path.join(kwargs['savedir'], 'data_processing.p'), 'wb'))\n",
    "\n",
    "    # actual training\n",
    "    anode = DensityEstimator(kwargs['config_file'], device=device,\n",
    "                             verbose=kwargs['verbose'], bound=kwargs['no_logit'])\n",
    "    model, optimizer = anode.model, anode.optimizer\n",
    "\n",
    "    train_ANODE(model, optimizer, train_loader, test_loader, kwargs['model_file_name'],\n",
    "                kwargs['epochs'], savedir=kwargs['savedir'], device=device, verbose=kwargs['verbose'],\n",
    "                no_logit=kwargs['no_logit'], data_std=data_std)\n",
    "\n",
    "    # plot losses\n",
    "    train_losses = np.load(os.path.join(kwargs['savedir'], kwargs['model_file_name']+\"_train_losses.npy\"))\n",
    "    val_losses = np.load(os.path.join(kwargs['savedir'], kwargs['model_file_name']+\"_val_losses.npy\"))\n",
    "    plot_ANODE_losses(train_losses, val_losses, yrange=None,\n",
    "                      savefig=os.path.join(kwargs['savedir'], kwargs['model_file_name']+\"_loss_plot\"),\n",
    "                      suppress_show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebcbf13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: False\n",
      "DensityEstimator has 274800 parameters\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.839945674247761\n",
      "val_loss =  5.839027536881937\n",
      "\n",
      "Epoch: 0\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.347782872424023\n",
      "val_loss =  5.340106931892601\n",
      "\n",
      "Epoch: 1\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.325487768468568\n",
      "val_loss =  5.336295588596447\n",
      "\n",
      "Epoch: 2\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.322919966737317\n",
      "val_loss =  5.334799012622318\n",
      "\n",
      "Epoch: 3\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.321476017453504\n",
      "val_loss =  5.333713773134592\n",
      "\n",
      "Epoch: 4\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.320213466970277\n",
      "val_loss =  5.333408603797087\n",
      "\n",
      "Epoch: 5\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.319652804100296\n",
      "val_loss =  5.332585028699927\n",
      "\n",
      "Epoch: 6\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.318870215899185\n",
      "val_loss =  5.332615278862618\n",
      "\n",
      "Epoch: 7\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.318315258486376\n",
      "val_loss =  5.332469183045465\n",
      "\n",
      "Epoch: 8\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.318106327512674\n",
      "val_loss =  5.33072027000221\n",
      "\n",
      "Epoch: 9\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.317384602108454\n",
      "val_loss =  5.3309398277385815\n",
      "\n",
      "Epoch: 10\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.317093981879058\n",
      "val_loss =  5.331522538855269\n",
      "\n",
      "Epoch: 11\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.316098530540545\n",
      "val_loss =  5.329419213372308\n",
      "\n",
      "Epoch: 12\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.316040367869222\n",
      "val_loss =  5.330939524882549\n",
      "\n",
      "Epoch: 13\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.315973657449245\n",
      "val_loss =  5.329532932590794\n",
      "\n",
      "Epoch: 14\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.315080015429468\n",
      "val_loss =  5.329419532337704\n",
      "\n",
      "Epoch: 15\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.315009072424561\n",
      "val_loss =  5.329101198428386\n",
      "\n",
      "Epoch: 16\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.3142524032278855\n",
      "val_loss =  5.3298166829186515\n",
      "\n",
      "Epoch: 17\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.313993375734826\n",
      "val_loss =  5.3307892148559155\n",
      "\n",
      "Epoch: 18\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.313997088478482\n",
      "val_loss =  5.329455530321276\n",
      "\n",
      "Epoch: 19\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.313514869069989\n",
      "val_loss =  5.3288247424202995\n",
      "\n",
      "Epoch: 20\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.313668168636689\n",
      "val_loss =  5.329253338478707\n",
      "\n",
      "Epoch: 21\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.313046287682368\n",
      "val_loss =  5.329506577672185\n",
      "\n",
      "Epoch: 22\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.312694693913555\n",
      "val_loss =  5.32886169730006\n",
      "\n",
      "Epoch: 23\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.312773194553782\n",
      "val_loss =  5.329228352855992\n",
      "\n",
      "Epoch: 24\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.312547612313336\n",
      "val_loss =  5.328566918501982\n",
      "\n",
      "Epoch: 25\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.3118434054704355\n",
      "val_loss =  5.328816613635501\n",
      "\n",
      "Epoch: 26\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.311670161946156\n",
      "val_loss =  5.328221559524536\n",
      "\n",
      "Epoch: 27\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.3114643894087825\n",
      "val_loss =  5.328449668111028\n",
      "\n",
      "Epoch: 28\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.311484523603048\n",
      "val_loss =  5.329302501034093\n",
      "\n",
      "Epoch: 29\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.311161776234168\n",
      "val_loss =  5.328399007384841\n",
      "\n",
      "Epoch: 30\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.311323360826342\n",
      "val_loss =  5.329999601518786\n",
      "\n",
      "Epoch: 31\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.310492898000286\n",
      "val_loss =  5.329462918075356\n",
      "\n",
      "Epoch: 32\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.310663653370233\n",
      "val_loss =  5.329620267893817\n",
      "\n",
      "Epoch: 33\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.310363305071253\n",
      "val_loss =  5.328491694218403\n",
      "\n",
      "Epoch: 34\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.310107634804056\n",
      "val_loss =  5.328873067288785\n",
      "\n",
      "Epoch: 35\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.309600372566184\n",
      "val_loss =  5.328734790956652\n",
      "\n",
      "Epoch: 36\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.309325591757\n",
      "val_loss =  5.329530235883352\n",
      "\n",
      "Epoch: 37\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.30899867513975\n",
      "val_loss =  5.328762421736846\n",
      "\n",
      "Epoch: 38\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.30905570452603\n",
      "val_loss =  5.328717795578209\n",
      "\n",
      "Epoch: 39\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.308788509637987\n",
      "val_loss =  5.329384877875045\n",
      "\n",
      "Epoch: 40\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.308545555122133\n",
      "val_loss =  5.327920601174638\n",
      "\n",
      "Epoch: 41\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.307878872424978\n",
      "val_loss =  5.328731221121711\n",
      "\n",
      "Epoch: 42\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.307749640317421\n",
      "val_loss =  5.328793155180441\n",
      "\n",
      "Epoch: 43\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.30723167765521\n",
      "val_loss =  5.32866505996601\n",
      "\n",
      "Epoch: 44\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.306754245936363\n",
      "val_loss =  5.327671608409366\n",
      "\n",
      "Epoch: 45\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.3069923046678324\n",
      "val_loss =  5.330032686929445\n",
      "\n",
      "Epoch: 46\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.305950719978327\n",
      "val_loss =  5.330254210008158\n",
      "\n",
      "Epoch: 47\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.306137728453649\n",
      "val_loss =  5.328286222509436\n",
      "\n",
      "Epoch: 48\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.306220378051294\n",
      "val_loss =  5.328385610838194\n",
      "\n",
      "Epoch: 49\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.305864379321024\n",
      "val_loss =  5.328871627111693\n",
      "\n",
      "Epoch: 50\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.304817072498346\n",
      "val_loss =  5.3295010618261385\n",
      "\n",
      "Epoch: 51\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.304787548470621\n",
      "val_loss =  5.328568732416308\n",
      "\n",
      "Epoch: 52\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.304374763959011\n",
      "val_loss =  5.328438948940587\n",
      "\n",
      "Epoch: 53\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.303807940620814\n",
      "val_loss =  5.327824785902694\n",
      "\n",
      "Epoch: 54\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.303681446984515\n",
      "val_loss =  5.328260550627837\n",
      "\n",
      "Epoch: 55\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.3035739215845545\n",
      "val_loss =  5.3282861677376\n",
      "\n",
      "Epoch: 56\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.302925627748284\n",
      "val_loss =  5.32814947656683\n",
      "\n",
      "Epoch: 57\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.303299351338302\n",
      "val_loss =  5.328370796667563\n",
      "\n",
      "Epoch: 58\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.302773743917055\n",
      "val_loss =  5.328290559150077\n",
      "\n",
      "Epoch: 59\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.3020660296436395\n",
      "val_loss =  5.328808223879015\n",
      "\n",
      "Epoch: 60\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.302722774413149\n",
      "val_loss =  5.327352282163259\n",
      "\n",
      "Epoch: 61\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.302008714354602\n",
      "val_loss =  5.328161423270767\n",
      "\n",
      "Epoch: 62\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.302021777756209\n",
      "val_loss =  5.329200970159994\n",
      "\n",
      "Epoch: 63\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.301675107851574\n",
      "val_loss =  5.3275022184526595\n",
      "\n",
      "Epoch: 64\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.302071054514291\n",
      "val_loss =  5.328513776933825\n",
      "\n",
      "Epoch: 65\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.3011053138290505\n",
      "val_loss =  5.329560221852483\n",
      "\n",
      "Epoch: 66\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.301682038308687\n",
      "val_loss =  5.328319008285935\n",
      "\n",
      "Epoch: 67\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.301181721909126\n",
      "val_loss =  5.328054550531748\n",
      "\n",
      "Epoch: 68\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.301257294278733\n",
      "val_loss =  5.3277435850452735\n",
      "\n",
      "Epoch: 69\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.300867103934344\n",
      "val_loss =  5.32881722256944\n",
      "\n",
      "Epoch: 70\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.300607112102856\n",
      "val_loss =  5.328756303400607\n",
      "\n",
      "Epoch: 71\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.300468232622636\n",
      "val_loss =  5.329131500141041\n",
      "\n",
      "Epoch: 72\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.300425398738369\n",
      "val_loss =  5.328373164743991\n",
      "\n",
      "Epoch: 73\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.300101448802766\n",
      "val_loss =  5.3283396991523535\n",
      "\n",
      "Epoch: 74\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299771242408105\n",
      "val_loss =  5.328344538405135\n",
      "\n",
      "Epoch: 75\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299902738617278\n",
      "val_loss =  5.328210524610571\n",
      "\n",
      "Epoch: 76\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299489357327884\n",
      "val_loss =  5.328791695672113\n",
      "\n",
      "Epoch: 77\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299453128104243\n",
      "val_loss =  5.328182755289851\n",
      "\n",
      "Epoch: 78\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299711412311368\n",
      "val_loss =  5.328707156954585\n",
      "\n",
      "Epoch: 79\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299413933409345\n",
      "val_loss =  5.328430839487024\n",
      "\n",
      "Epoch: 80\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299691016698264\n",
      "val_loss =  5.328735506212389\n",
      "\n",
      "Epoch: 81\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298727571802711\n",
      "val_loss =  5.328030827883127\n",
      "\n",
      "Epoch: 82\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.299075249611687\n",
      "val_loss =  5.328536004633517\n",
      "\n",
      "Epoch: 83\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298353802521684\n",
      "val_loss =  5.328095726064734\n",
      "\n",
      "Epoch: 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298797841396159\n",
      "val_loss =  5.329059410739589\n",
      "\n",
      "Epoch: 85\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298588411643772\n",
      "val_loss =  5.32975714271133\n",
      "\n",
      "Epoch: 86\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298495100003247\n",
      "val_loss =  5.328940259443747\n",
      "\n",
      "Epoch: 87\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.2984599942832915\n",
      "val_loss =  5.327746349412042\n",
      "\n",
      "Epoch: 88\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298545998047918\n",
      "val_loss =  5.328874072513065\n",
      "\n",
      "Epoch: 89\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.2983193115412535\n",
      "val_loss =  5.328588340733503\n",
      "\n",
      "Epoch: 90\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298404072887395\n",
      "val_loss =  5.328005648948051\n",
      "\n",
      "Epoch: 91\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298261877798108\n",
      "val_loss =  5.32799870259053\n",
      "\n",
      "Epoch: 92\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298302890795035\n",
      "val_loss =  5.3280216455459595\n",
      "\n",
      "Epoch: 93\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.298035208914103\n",
      "val_loss =  5.327960004677644\n",
      "\n",
      "Epoch: 94\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.29785388251083\n",
      "val_loss =  5.328637522620124\n",
      "\n",
      "Epoch: 95\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.297339446449357\n",
      "val_loss =  5.328432382764043\n",
      "\n",
      "Epoch: 96\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.29754323188736\n",
      "val_loss =  5.329485023343885\n",
      "\n",
      "Epoch: 97\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.297492351706664\n",
      "val_loss =  5.328228721747527\n",
      "\n",
      "Epoch: 98\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.296913349299406\n",
      "val_loss =  5.328477608191\n",
      "\n",
      "Epoch: 99\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "train_loss =  5.297083140292295\n",
      "val_loss =  5.327958235869536\n"
     ]
    }
   ],
   "source": [
    "train_DE(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f1644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_epochs(num_models, **kwargs):\n",
    "    \"\"\" looks through saved val-losses and creates list-of-paths of num_models best ones\"\"\"\n",
    "    val_losses = np.load(os.path.join(kwargs['savedir'], kwargs['model_file_name']+\"_val_losses.npy\"))\n",
    "    idx = np.argpartition(val_losses, num_models)[:num_models] #faster than argsort\n",
    "    ret_list = []\n",
    "    for index in idx:\n",
    "        ret_list.append(os.path.join(kwargs['savedir'],\n",
    "                                     kwargs['model_file_name']+'_epoch_'+str(index-1)+'.par'))\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e8df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_creation_kwargs = {\n",
    "    'savedir': save_dir,\n",
    "    'datashift': datashift,\n",
    "    'data_dir': data_dir,\n",
    "    'random_shift': random_shift,\n",
    "    'config_file': DE_config_file,\n",
    "    'verbose': verbose,\n",
    "    'fiducial_cut': fiducial_cut,\n",
    "    'n_samples': cf_n_samples,\n",
    "    'realistic_conditional': cf_realistic_conditional,\n",
    "    'KDE_bandwidth': cf_KDE_bandwidth,\n",
    "    'oversampling': cf_oversampling,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'CWoLa': False,\n",
    "    'supervised': False,\n",
    "    'idealized_AD': False,\n",
    "    'no_logit': cf_no_logit,\n",
    "    'no_logit_trained': DE_no_logit,\n",
    "    'external_samples': cf_external_samples,\n",
    "    'SR_min': cf_SR_min,\n",
    "    'SR_max': cf_SR_max,\n",
    "    'extra_bkg': cf_extra_bkg,\n",
    "    'separate_val_set': cf_separate_val_set,\n",
    "    'ANODE_models': find_best_epochs(10, **kwargs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c62eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data_handler import LHCORD_data_handler, sample_handler, mix_data_samples, plot_data_sample_comparison\n",
    "from density_estimator import DensityEstimator\n",
    "\n",
    "def create_data(**kwargs):\n",
    "\n",
    "    assert not ((not (kwargs['supervised'] or kwargs['idealized_AD'] or kwargs['CWoLa']) and\\\n",
    "                 kwargs['external_samples'] == \"\") and kwargs['ANODE_models'] == \"\"), (\n",
    "                     \"ANODE models need to be given unless CWoLa, supervised, idealized_AD or\"\n",
    "                     \" external sampling is used.\")\n",
    "\n",
    "    # selecting appropriate device\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    print(\"cuda available:\", CUDA)\n",
    "    device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "\n",
    "    # checking for data separation\n",
    "    data_files = os.listdir(kwargs['data_dir'])\n",
    "    if \"innerdata_val.npy\" in data_files:\n",
    "        finer_data_split = True\n",
    "    else:\n",
    "        finer_data_split = False\n",
    "\n",
    "    if finer_data_split:\n",
    "        innerdata_train_path = [os.path.join(kwargs['data_dir'], 'innerdata_train.npy')]\n",
    "        innerdata_val_path = [os.path.join(kwargs['data_dir'], 'innerdata_val.npy')]\n",
    "        innerdata_test_path = [os.path.join(kwargs['data_dir'], 'innerdata_test.npy')]\n",
    "        if \"innerdata_extrabkg_test.npy\" in data_files:\n",
    "            innerdata_test_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_test.npy'))\n",
    "        extrasig_path = None\n",
    "        if kwargs['supervised']:\n",
    "            innerdata_train_path = []\n",
    "            innerdata_val_path = []\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_val.npy'))\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy'))\n",
    "            extra_bkg = None\n",
    "        elif kwargs['idealized_AD']:\n",
    "            extra_bkg = [os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'),\n",
    "                         os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy')]\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "\n",
    "    else:\n",
    "        innerdata_train_path = os.path.join(kwargs['data_dir'], 'innerdata_train.npy')\n",
    "        extrasig_path = os.path.join(kwargs['data_dir'], 'innerdata_extrasig.npy')\n",
    "        if kwargs['extra_bkg']:\n",
    "            extra_bkg = os.path.join(kwargs['data_dir'], 'innerdata_extrabkg.npy')\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "        innerdata_val_path = None\n",
    "        innerdata_test_path = os.path.join(kwargs['data_dir'], 'innerdata_test.npy')\n",
    "\n",
    "    # data preprocessing\n",
    "    data = LHCORD_data_handler(innerdata_train_path,\n",
    "                               innerdata_test_path,\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_train.npy'),\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_test.npy'),\n",
    "                               extrasig_path,\n",
    "                               inner_extrabkg_path=extra_bkg,\n",
    "                               inner_val_path=innerdata_val_path,\n",
    "                               batch_size=256,\n",
    "                               device=device)\n",
    "    if kwargs['datashift'] != 0:\n",
    "        print(\"applying a datashift of\", kwargs['datashift'])\n",
    "        data.shift_data(kwargs['datashift'], constant_shift=False, random_shift=kwargs['random_shift'],\n",
    "                        shift_mj1=True, shift_dm=True, additional_shift=False)\n",
    "\n",
    "    if kwargs['CWoLa']:\n",
    "        # data preprocessing\n",
    "        samples = None\n",
    "        data.preprocess_CWoLa_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit'],\n",
    "                                   outer_range=(kwargs['SR_min']-0.2, kwargs['SR_max']+0.2))\n",
    "\n",
    "    else:\n",
    "        # data preprocessing\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'],\n",
    "                                   no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # model instantiation\n",
    "        if len(kwargs['external_samples']) > 0:\n",
    "            model_list = None\n",
    "            loaded_samples = [np.load(sample_path) for sample_path in kwargs['external_samples']]\n",
    "            external_sample = np.concatenate(loaded_samples)\n",
    "        else:\n",
    "            model_list = []\n",
    "            for model_path in kwargs['ANODE_models']:\n",
    "                anode = DensityEstimator(kwargs['config_file'],\n",
    "                                         eval_mode=True,\n",
    "                                         load_path=model_path,\n",
    "                                         device=device, verbose=kwargs['verbose'],\n",
    "                                         bound=kwargs['no_logit_trained'])\n",
    "                model_list.append(anode.model)\n",
    "            external_sample = None\n",
    "\n",
    "        # generate samples\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            uniform_cond = not kwargs['realistic_conditional']\n",
    "            samples = sample_handler(model_list, kwargs['n_samples'], data, cond_min=kwargs['SR_min'],\n",
    "                                     cond_max=kwargs['SR_max'], uniform_cond=uniform_cond,\n",
    "                                     external_sample=external_sample,\n",
    "                                     device=device, no_logit=kwargs['no_logit_trained'],\n",
    "                                     no_mean_shift=kwargs['no_logit_trained'],\n",
    "                                     KDE_bandwidth=kwargs['KDE_bandwidth'])\n",
    "        else:\n",
    "            samples = None\n",
    "\n",
    "        # redo data preprocessing if the classifier should not use logit but ANODE did\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # sample preprocessing\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            samples.preprocess_samples(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                       no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "\n",
    "    # sample mixing\n",
    "    X_train, y_train, X_test, y_test, X_extrasig, y_extrasig = mix_data_samples(\n",
    "        data, samples_handler=samples, oversampling=kwargs['oversampling'],\n",
    "        savedir=kwargs['savedir'], CWoLa=kwargs['CWoLa'], supervised=kwargs['supervised'],\n",
    "        idealized_AD=kwargs['idealized_AD'], separate_val_set=kwargs['separate_val_set'] or finer_data_split)\n",
    "\n",
    "    # sanity checks\n",
    "    if not kwargs['CWoLa'] and not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "        samples.sanity_check(savefig=os.path.join(kwargs['savedir'], \"sanity_check\"), suppress_show=True)\n",
    "        samples.sanity_check_after_cuts(savefig=os.path.join(kwargs['savedir'], \"sanity_check_cuts\"),\n",
    "                                        suppress_show=True)\n",
    "\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "        X_val = X_extrasig\n",
    "        if kwargs['supervised']:\n",
    "            y_train = X_train[:, -1]\n",
    "            y_test = X_test[:, -1]\n",
    "            y_val = X_val[:, -1]\n",
    "        else:\n",
    "            y_val = X_val[:, -2]\n",
    "        plot_data_sample_comparison(X_val, y_val, title=\"validation set\",\n",
    "                                    savefig=os.path.join(kwargs['savedir'],\n",
    "                                                         \"data_sample_comparison_val\"),\n",
    "                                    suppress_show=True)\n",
    "\n",
    "    plot_data_sample_comparison(X_train, y_train, title=\"training set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_train\"),\n",
    "                                suppress_show=True)\n",
    "    plot_data_sample_comparison(X_test, y_test, title=\"test set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_test\"),\n",
    "                                suppress_show=True)\n",
    "\n",
    "    print(\"number of training data =\", X_train.shape[0])\n",
    "    print(\"number of test data =\", X_test.shape[0])\n",
    "    if not kwargs['no_extra_signal']:\n",
    "        if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "            print(\"number of validation data =\", X_val.shape[0])\n",
    "        elif extrasig_path is not None:\n",
    "            print(\"number of extra signal data =\", X_extrasig.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be19c8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: False\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_53.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_60.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_68.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_40.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_44.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_63.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_87.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_99.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_93.par\n",
      "DensityEstimator has 274800 parameters\n",
      "Loading model parameters from CATHODE_models/my_ANODE_model_epoch_91.par\n",
      "sampling from model 1/10\n",
      "sampling from model 2/10\n",
      "sampling from model 3/10\n",
      "sampling from model 4/10\n",
      "sampling from model 5/10\n",
      "sampling from model 6/10\n",
      "sampling from model 7/10\n",
      "sampling from model 8/10\n",
      "sampling from model 9/10\n",
      "sampling from model 10/10\n",
      "averaged the samples to a tensor of shape: (400000, 4)\n",
      "using the full number of samples...\n",
      "number of training data = 259941\n",
      "number of test data = 359883\n"
     ]
    }
   ],
   "source": [
    "create_data(**data_creation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf4c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_kwargs = {\n",
    "    'config_file': cf_config_file,\n",
    "    'data_dir': save_dir,\n",
    "    'savedir': save_dir,\n",
    "    'verbose': verbose,\n",
    "    'epochs': cf_epochs,\n",
    "    'n_runs': cf_n_runs,\n",
    "    'batch_size': cf_batch_size,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'use_mjj': cf_use_mjj,\n",
    "    'supervised': False,\n",
    "    'use_class_weights': cf_oversampling or cf_use_class_weights,\n",
    "    'CWoLa': False,\n",
    "    'SR_center': cf_SR_center,\n",
    "    'save_model': cf_save_model,\n",
    "    'separate_val_set': cf_separate_val_set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f962882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier_training_utils import train_n_models, plot_classifier_losses\n",
    "from evaluation_utils import minimum_val_loss_model_evaluation\n",
    "import matplotlib as mpl\n",
    "\n",
    "def train_classifier(**kwargs):\n",
    "\n",
    "    # loading the data\n",
    "    # TODO get rid of the y's since the information is fully included in X\n",
    "    X_train = np.load(os.path.join(kwargs['data_dir'], 'X_train.npy'))\n",
    "    X_test = np.load(os.path.join(kwargs['data_dir'], 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(kwargs['data_dir'], 'y_train.npy'))\n",
    "    y_test = np.load(os.path.join(kwargs['data_dir'], 'y_test.npy'))\n",
    "    if kwargs['no_extra_signal'] or kwargs['supervised']:\n",
    "        X_extrasig = None\n",
    "    else:\n",
    "        X_extrasig = np.load(os.path.join(kwargs['data_dir'], 'X_extrasig.npy'))\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set']:\n",
    "        X_val = np.load(os.path.join(kwargs['data_dir'], 'X_validation.npy'))        \n",
    "    else:\n",
    "        X_val = None\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        if not os.path.exists(kwargs['savedir']):\n",
    "            os.makedirs(kwargs['savedir'])\n",
    "        save_model = os.path.join(kwargs['savedir'], \"model\")\n",
    "    else:\n",
    "        save_model = None\n",
    "\n",
    "    # actual training\n",
    "    loss_matris, val_loss_matris = train_n_models(\n",
    "        kwargs['n_runs'], kwargs['config_file'], kwargs['epochs'], X_train, y_train, X_test, y_test,\n",
    "        X_extrasig=X_extrasig, X_val=X_val, use_mjj=kwargs['use_mjj'], batch_size=kwargs['batch_size'],\n",
    "        supervised=kwargs['supervised'], use_class_weights=kwargs['use_class_weights'],\n",
    "        CWoLa=kwargs['CWoLa'], SR_center=kwargs['SR_center'], verbose=kwargs['verbose'],\n",
    "        savedir=kwargs['savedir'], save_model=save_model)\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        minimum_val_loss_model_evaluation(kwargs['data_dir'], kwargs['savedir'], n_epochs=10,\n",
    "                                use_mjj=kwargs['use_mjj'], extra_signal=not kwargs['no_extra_signal'])\n",
    "\n",
    "    for i in range(loss_matris.shape[0]):\n",
    "        plot_classifier_losses(\n",
    "            loss_matris[i], val_loss_matris[i],\n",
    "            savefig=save_model+\"_run\"+str(i)+\"_loss_plot\",\n",
    "            suppress_show=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "942ef098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model nr 0...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932564973831177\n",
      "validation loss: 0.6931200623512268\n",
      "training epoch nr 1\n",
      "training loss: 0.6931014060974121\n",
      "validation loss: 0.6932196021080017\n",
      "training epoch nr 2\n",
      "training loss: 0.6930747032165527\n",
      "validation loss: 0.6930397748947144\n",
      "training epoch nr 3\n",
      "training loss: 0.6930102705955505\n",
      "validation loss: 0.6930329203605652\n",
      "training epoch nr 4\n",
      "training loss: 0.6929495334625244\n",
      "validation loss: 0.6932007074356079\n",
      "training epoch nr 5\n",
      "training loss: 0.6928628087043762\n",
      "validation loss: 0.6929649710655212\n",
      "training epoch nr 6\n",
      "training loss: 0.6927865147590637\n",
      "validation loss: 0.6930946707725525\n",
      "training epoch nr 7\n",
      "training loss: 0.6926776766777039\n",
      "validation loss: 0.6928848028182983\n",
      "training epoch nr 8\n",
      "training loss: 0.6926499605178833\n",
      "validation loss: 0.6929194927215576\n",
      "training epoch nr 9\n",
      "training loss: 0.6926344037055969\n",
      "validation loss: 0.6928948760032654\n",
      "training epoch nr 10\n",
      "training loss: 0.6925844550132751\n",
      "validation loss: 0.6928314566612244\n",
      "training epoch nr 11\n",
      "training loss: 0.6925485730171204\n",
      "validation loss: 0.6927652955055237\n",
      "training epoch nr 12\n",
      "training loss: 0.6924800276756287\n",
      "validation loss: 0.6928324699401855\n",
      "training epoch nr 13\n",
      "training loss: 0.6924408078193665\n",
      "validation loss: 0.6928050518035889\n",
      "training epoch nr 14\n",
      "training loss: 0.692391574382782\n",
      "validation loss: 0.6928566694259644\n",
      "training epoch nr 15\n",
      "training loss: 0.6923061609268188\n",
      "validation loss: 0.6930568218231201\n",
      "training epoch nr 16\n",
      "training loss: 0.6922509074211121\n",
      "validation loss: 0.6929789781570435\n",
      "training epoch nr 17\n",
      "training loss: 0.6922470331192017\n",
      "validation loss: 0.6929014921188354\n",
      "training epoch nr 18\n",
      "training loss: 0.6922191977500916\n",
      "validation loss: 0.6931651830673218\n",
      "training epoch nr 19\n",
      "training loss: 0.6921563744544983\n",
      "validation loss: 0.6933786273002625\n",
      "training epoch nr 20\n",
      "training loss: 0.6921671628952026\n",
      "validation loss: 0.6930789947509766\n",
      "training epoch nr 21\n",
      "training loss: 0.6920995116233826\n",
      "validation loss: 0.693034291267395\n",
      "training epoch nr 22\n",
      "training loss: 0.6920133233070374\n",
      "validation loss: 0.6931782364845276\n",
      "training epoch nr 23\n",
      "training loss: 0.6920028328895569\n",
      "validation loss: 0.6932282447814941\n",
      "training epoch nr 24\n",
      "training loss: 0.6919726133346558\n",
      "validation loss: 0.6932387351989746\n",
      "training epoch nr 25\n",
      "training loss: 0.691939115524292\n",
      "validation loss: 0.6935797333717346\n",
      "training epoch nr 26\n",
      "training loss: 0.6919182538986206\n",
      "validation loss: 0.6931360960006714\n",
      "training epoch nr 27\n",
      "training loss: 0.6918541193008423\n",
      "validation loss: 0.6932355165481567\n",
      "training epoch nr 28\n",
      "training loss: 0.6917822957038879\n",
      "validation loss: 0.693284273147583\n",
      "training epoch nr 29\n",
      "training loss: 0.6917523145675659\n",
      "validation loss: 0.6933279633522034\n",
      "training epoch nr 30\n",
      "training loss: 0.6917246580123901\n",
      "validation loss: 0.6932359337806702\n",
      "training epoch nr 31\n",
      "training loss: 0.6917544603347778\n",
      "validation loss: 0.693179726600647\n",
      "training epoch nr 32\n",
      "training loss: 0.6917392015457153\n",
      "validation loss: 0.6933141350746155\n",
      "training epoch nr 33\n",
      "training loss: 0.6916003227233887\n",
      "validation loss: 0.6935555934906006\n",
      "training epoch nr 34\n",
      "training loss: 0.6915949583053589\n",
      "validation loss: 0.6937792301177979\n",
      "training epoch nr 35\n",
      "training loss: 0.6915856003761292\n",
      "validation loss: 0.6934959888458252\n",
      "training epoch nr 36\n",
      "training loss: 0.6915634870529175\n",
      "validation loss: 0.6934120655059814\n",
      "training epoch nr 37\n",
      "training loss: 0.6914761662483215\n",
      "validation loss: 0.6935433149337769\n",
      "training epoch nr 38\n",
      "training loss: 0.6914185285568237\n",
      "validation loss: 0.6936798691749573\n",
      "training epoch nr 39\n",
      "training loss: 0.6913478970527649\n",
      "validation loss: 0.693681001663208\n",
      "training epoch nr 40\n",
      "training loss: 0.6913353204727173\n",
      "validation loss: 0.6937727332115173\n",
      "training epoch nr 41\n",
      "training loss: 0.6912987232208252\n",
      "validation loss: 0.6936519145965576\n",
      "training epoch nr 42\n",
      "training loss: 0.6913114190101624\n",
      "validation loss: 0.6939384341239929\n",
      "training epoch nr 43\n",
      "training loss: 0.6912552118301392\n",
      "validation loss: 0.6938701868057251\n",
      "training epoch nr 44\n",
      "training loss: 0.6912404894828796\n",
      "validation loss: 0.6937680244445801\n",
      "training epoch nr 45\n",
      "training loss: 0.6911758184432983\n",
      "validation loss: 0.6936293244361877\n",
      "training epoch nr 46\n",
      "training loss: 0.6912019848823547\n",
      "validation loss: 0.6939055919647217\n",
      "training epoch nr 47\n",
      "training loss: 0.6910337805747986\n",
      "validation loss: 0.6938842535018921\n",
      "training epoch nr 48\n",
      "training loss: 0.6911056637763977\n",
      "validation loss: 0.6940174102783203\n",
      "training epoch nr 49\n",
      "training loss: 0.6910172700881958\n",
      "validation loss: 0.6941039562225342\n",
      "training epoch nr 50\n",
      "training loss: 0.6909950971603394\n",
      "validation loss: 0.694034993648529\n",
      "training epoch nr 51\n",
      "training loss: 0.6909312009811401\n",
      "validation loss: 0.6940825581550598\n",
      "training epoch nr 52\n",
      "training loss: 0.6909486651420593\n",
      "validation loss: 0.6943522095680237\n",
      "training epoch nr 53\n",
      "training loss: 0.6908860206604004\n",
      "validation loss: 0.6939279437065125\n",
      "training epoch nr 54\n",
      "training loss: 0.6907329559326172\n",
      "validation loss: 0.6944897174835205\n",
      "training epoch nr 55\n",
      "training loss: 0.6907544732093811\n",
      "validation loss: 0.694218099117279\n",
      "training epoch nr 56\n",
      "training loss: 0.6907727718353271\n",
      "validation loss: 0.6946801543235779\n",
      "training epoch nr 57\n",
      "training loss: 0.6907245516777039\n",
      "validation loss: 0.6943520903587341\n",
      "training epoch nr 58\n",
      "training loss: 0.6906517744064331\n",
      "validation loss: 0.6945109367370605\n",
      "training epoch nr 59\n",
      "training loss: 0.6906806826591492\n",
      "validation loss: 0.6944063305854797\n",
      "training epoch nr 60\n",
      "training loss: 0.6906240582466125\n",
      "validation loss: 0.6945112943649292\n",
      "training epoch nr 61\n",
      "training loss: 0.6906387209892273\n",
      "validation loss: 0.6947485208511353\n",
      "training epoch nr 62\n",
      "training loss: 0.690545916557312\n",
      "validation loss: 0.6951419711112976\n",
      "training epoch nr 63\n",
      "training loss: 0.6904645562171936\n",
      "validation loss: 0.6942776441574097\n",
      "training epoch nr 64\n",
      "training loss: 0.6904295086860657\n",
      "validation loss: 0.6943821907043457\n",
      "training epoch nr 65\n",
      "training loss: 0.6903167366981506\n",
      "validation loss: 0.694573163986206\n",
      "training epoch nr 66\n",
      "training loss: 0.6902949810028076\n",
      "validation loss: 0.6949298977851868\n",
      "training epoch nr 67\n",
      "training loss: 0.6903776526451111\n",
      "validation loss: 0.6948789954185486\n",
      "training epoch nr 68\n",
      "training loss: 0.6902483105659485\n",
      "validation loss: 0.6952558755874634\n",
      "training epoch nr 69\n",
      "training loss: 0.6901888251304626\n",
      "validation loss: 0.6950821280479431\n",
      "training epoch nr 70\n",
      "training loss: 0.6901954412460327\n",
      "validation loss: 0.6949940919876099\n",
      "training epoch nr 71\n",
      "training loss: 0.6901534795761108\n",
      "validation loss: 0.6949636936187744\n",
      "training epoch nr 72\n",
      "training loss: 0.6901706457138062\n",
      "validation loss: 0.6949267387390137\n",
      "training epoch nr 73\n",
      "training loss: 0.690062940120697\n",
      "validation loss: 0.6950350403785706\n",
      "training epoch nr 74\n",
      "training loss: 0.6900923848152161\n",
      "validation loss: 0.6952226758003235\n",
      "training epoch nr 75\n",
      "training loss: 0.6899819374084473\n",
      "validation loss: 0.6948098540306091\n",
      "training epoch nr 76\n",
      "training loss: 0.6899272203445435\n",
      "validation loss: 0.6951155066490173\n",
      "training epoch nr 77\n",
      "training loss: 0.6899588108062744\n",
      "validation loss: 0.6947864890098572\n",
      "training epoch nr 78\n",
      "training loss: 0.6898196339607239\n",
      "validation loss: 0.6951279044151306\n",
      "training epoch nr 79\n",
      "training loss: 0.6898249387741089\n",
      "validation loss: 0.6952147483825684\n",
      "training epoch nr 80\n",
      "training loss: 0.6897743940353394\n",
      "validation loss: 0.6954986453056335\n",
      "training epoch nr 81\n",
      "training loss: 0.689605176448822\n",
      "validation loss: 0.6951904296875\n",
      "training epoch nr 82\n",
      "training loss: 0.68978351354599\n",
      "validation loss: 0.6954212188720703\n",
      "training epoch nr 83\n",
      "training loss: 0.6896147727966309\n",
      "validation loss: 0.6961910724639893\n",
      "training epoch nr 84\n",
      "training loss: 0.6896949410438538\n",
      "validation loss: 0.6955174207687378\n",
      "training epoch nr 85\n",
      "training loss: 0.6895343661308289\n",
      "validation loss: 0.6956786513328552\n",
      "training epoch nr 86\n",
      "training loss: 0.689545750617981\n",
      "validation loss: 0.6964818239212036\n",
      "training epoch nr 87\n",
      "training loss: 0.6895334124565125\n",
      "validation loss: 0.6962130665779114\n",
      "training epoch nr 88\n",
      "training loss: 0.6894312500953674\n",
      "validation loss: 0.6958830952644348\n",
      "training epoch nr 89\n",
      "training loss: 0.6895453333854675\n",
      "validation loss: 0.6957942843437195\n",
      "training epoch nr 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6895214319229126\n",
      "validation loss: 0.6958550810813904\n",
      "training epoch nr 91\n",
      "training loss: 0.6894177198410034\n",
      "validation loss: 0.6964117288589478\n",
      "training epoch nr 92\n",
      "training loss: 0.6894461512565613\n",
      "validation loss: 0.6960422396659851\n",
      "training epoch nr 93\n",
      "training loss: 0.6894630193710327\n",
      "validation loss: 0.695569634437561\n",
      "training epoch nr 94\n",
      "training loss: 0.6892589926719666\n",
      "validation loss: 0.6960283517837524\n",
      "training epoch nr 95\n",
      "training loss: 0.6893377304077148\n",
      "validation loss: 0.6966918110847473\n",
      "training epoch nr 96\n",
      "training loss: 0.6891807317733765\n",
      "validation loss: 0.6965463757514954\n",
      "training epoch nr 97\n",
      "training loss: 0.6892654895782471\n",
      "validation loss: 0.6964305639266968\n",
      "training epoch nr 98\n",
      "training loss: 0.6891840696334839\n",
      "validation loss: 0.6961252093315125\n",
      "training epoch nr 99\n",
      "training loss: 0.689077615737915\n",
      "validation loss: 0.6962637305259705\n",
      "minimum validation loss epochs: [10 11 13 12  9  7 14 17  8  5]\n"
     ]
    }
   ],
   "source": [
    "train_classifier(**classifier_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e5b28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model nr 0...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932389140129089\n",
      "validation loss: 0.6931337714195251\n",
      "training epoch nr 1\n",
      "training loss: 0.6931347846984863\n",
      "validation loss: 0.6931878328323364\n",
      "training epoch nr 2\n",
      "training loss: 0.6931366324424744\n",
      "validation loss: 0.6931458115577698\n",
      "training epoch nr 3\n",
      "training loss: 0.6930834054946899\n",
      "validation loss: 0.6932743787765503\n",
      "training epoch nr 4\n",
      "training loss: 0.6930044889450073\n",
      "validation loss: 0.6930981874465942\n",
      "training epoch nr 5\n",
      "training loss: 0.6929227709770203\n",
      "validation loss: 0.6932023763656616\n",
      "training epoch nr 6\n",
      "training loss: 0.6928586959838867\n",
      "validation loss: 0.6930224895477295\n",
      "training epoch nr 7\n",
      "training loss: 0.6927919983863831\n",
      "validation loss: 0.6929300427436829\n",
      "training epoch nr 8\n",
      "training loss: 0.6926838755607605\n",
      "validation loss: 0.6929951906204224\n",
      "training epoch nr 9\n",
      "training loss: 0.6925961971282959\n",
      "validation loss: 0.6929051280021667\n",
      "training epoch nr 10\n",
      "training loss: 0.6925469040870667\n",
      "validation loss: 0.6931626796722412\n",
      "training epoch nr 11\n",
      "training loss: 0.6925235390663147\n",
      "validation loss: 0.6931189894676208\n",
      "training epoch nr 12\n",
      "training loss: 0.692447304725647\n",
      "validation loss: 0.693106472492218\n",
      "training epoch nr 13\n",
      "training loss: 0.6924400925636292\n",
      "validation loss: 0.692833423614502\n",
      "training epoch nr 14\n",
      "training loss: 0.6924218535423279\n",
      "validation loss: 0.6930063366889954\n",
      "training epoch nr 15\n",
      "training loss: 0.6923136711120605\n",
      "validation loss: 0.6932245492935181\n",
      "training epoch nr 16\n",
      "training loss: 0.6922779083251953\n",
      "validation loss: 0.6932544112205505\n",
      "training epoch nr 17\n",
      "training loss: 0.6922110319137573\n",
      "validation loss: 0.6931621432304382\n",
      "training epoch nr 18\n",
      "training loss: 0.692229688167572\n",
      "validation loss: 0.6929594278335571\n",
      "training epoch nr 19\n",
      "training loss: 0.6921602487564087\n",
      "validation loss: 0.6931480169296265\n",
      "training epoch nr 20\n",
      "training loss: 0.6921298503875732\n",
      "validation loss: 0.6934012174606323\n",
      "training epoch nr 21\n",
      "training loss: 0.692108154296875\n",
      "validation loss: 0.6933366656303406\n",
      "training epoch nr 22\n",
      "training loss: 0.6920942068099976\n",
      "validation loss: 0.6930211186408997\n",
      "training epoch nr 23\n",
      "training loss: 0.692073404788971\n",
      "validation loss: 0.6931843757629395\n",
      "training epoch nr 24\n",
      "training loss: 0.6919469833374023\n",
      "validation loss: 0.6929384469985962\n",
      "training epoch nr 25\n",
      "training loss: 0.6919637322425842\n",
      "validation loss: 0.6934133768081665\n",
      "training epoch nr 26\n",
      "training loss: 0.6918620467185974\n",
      "validation loss: 0.693301260471344\n",
      "training epoch nr 27\n",
      "training loss: 0.6919103264808655\n",
      "validation loss: 0.6931301355361938\n",
      "training epoch nr 28\n",
      "training loss: 0.6918700933456421\n",
      "validation loss: 0.6931036710739136\n",
      "training epoch nr 29\n",
      "training loss: 0.6918368935585022\n",
      "validation loss: 0.6932342052459717\n",
      "training epoch nr 30\n",
      "training loss: 0.6917279362678528\n",
      "validation loss: 0.6933755278587341\n",
      "training epoch nr 31\n",
      "training loss: 0.6917208433151245\n",
      "validation loss: 0.6933035850524902\n",
      "training epoch nr 32\n",
      "training loss: 0.6916695237159729\n",
      "validation loss: 0.6938700079917908\n",
      "training epoch nr 33\n",
      "training loss: 0.6916189789772034\n",
      "validation loss: 0.693493127822876\n",
      "training epoch nr 34\n",
      "training loss: 0.6915537118911743\n",
      "validation loss: 0.693662166595459\n",
      "training epoch nr 35\n",
      "training loss: 0.691516101360321\n",
      "validation loss: 0.6932592988014221\n",
      "training epoch nr 36\n",
      "training loss: 0.6915388107299805\n",
      "validation loss: 0.6935492753982544\n",
      "training epoch nr 37\n",
      "training loss: 0.6914921402931213\n",
      "validation loss: 0.6934320330619812\n",
      "training epoch nr 38\n",
      "training loss: 0.6914553642272949\n",
      "validation loss: 0.6935305595397949\n",
      "training epoch nr 39\n",
      "training loss: 0.6913904547691345\n",
      "validation loss: 0.6934864521026611\n",
      "training epoch nr 40\n",
      "training loss: 0.691342294216156\n",
      "validation loss: 0.694200873374939\n",
      "training epoch nr 41\n",
      "training loss: 0.6913902163505554\n",
      "validation loss: 0.693501889705658\n",
      "training epoch nr 42\n",
      "training loss: 0.6911768317222595\n",
      "validation loss: 0.694270133972168\n",
      "training epoch nr 43\n",
      "training loss: 0.6912286877632141\n",
      "validation loss: 0.6935303807258606\n",
      "training epoch nr 44\n",
      "training loss: 0.6912309527397156\n",
      "validation loss: 0.6936792731285095\n",
      "training epoch nr 45\n",
      "training loss: 0.6911406517028809\n",
      "validation loss: 0.6938393115997314\n",
      "training epoch nr 46\n",
      "training loss: 0.6911642551422119\n",
      "validation loss: 0.6936413645744324\n",
      "training epoch nr 47\n",
      "training loss: 0.691148579120636\n",
      "validation loss: 0.6935448050498962\n",
      "training epoch nr 48\n",
      "training loss: 0.6910443305969238\n",
      "validation loss: 0.6938864588737488\n",
      "training epoch nr 49\n",
      "training loss: 0.6910319924354553\n",
      "validation loss: 0.6940879821777344\n",
      "training epoch nr 50\n",
      "training loss: 0.6909893751144409\n",
      "validation loss: 0.6937679052352905\n",
      "training epoch nr 51\n",
      "training loss: 0.6909767985343933\n",
      "validation loss: 0.6940405964851379\n",
      "training epoch nr 52\n",
      "training loss: 0.6908683180809021\n",
      "validation loss: 0.6940967440605164\n",
      "training epoch nr 53\n",
      "training loss: 0.690768301486969\n",
      "validation loss: 0.694039523601532\n",
      "training epoch nr 54\n",
      "training loss: 0.6907730102539062\n",
      "validation loss: 0.6939289569854736\n",
      "training epoch nr 55\n",
      "training loss: 0.6907418370246887\n",
      "validation loss: 0.6941817402839661\n",
      "training epoch nr 56\n",
      "training loss: 0.6907236576080322\n",
      "validation loss: 0.6941143274307251\n",
      "training epoch nr 57\n",
      "training loss: 0.6907081604003906\n",
      "validation loss: 0.6940909624099731\n",
      "training epoch nr 58\n",
      "training loss: 0.6906337738037109\n",
      "validation loss: 0.6943745017051697\n",
      "training epoch nr 59\n",
      "training loss: 0.6905972957611084\n",
      "validation loss: 0.6942635774612427\n",
      "training epoch nr 60\n",
      "training loss: 0.6905825734138489\n",
      "validation loss: 0.694139838218689\n",
      "training epoch nr 61\n",
      "training loss: 0.6906090974807739\n",
      "validation loss: 0.6943339705467224\n",
      "training epoch nr 62\n",
      "training loss: 0.6903966069221497\n",
      "validation loss: 0.6947929859161377\n",
      "training epoch nr 63\n",
      "training loss: 0.6903874278068542\n",
      "validation loss: 0.6943861246109009\n",
      "training epoch nr 64\n",
      "training loss: 0.6904429197311401\n",
      "validation loss: 0.6945593953132629\n",
      "training epoch nr 65\n",
      "training loss: 0.6903741955757141\n",
      "validation loss: 0.69438236951828\n",
      "training epoch nr 66\n",
      "training loss: 0.6902700066566467\n",
      "validation loss: 0.6945236921310425\n",
      "training epoch nr 67\n",
      "training loss: 0.6902836561203003\n",
      "validation loss: 0.6946962475776672\n",
      "training epoch nr 68\n",
      "training loss: 0.6902088522911072\n",
      "validation loss: 0.6947597861289978\n",
      "training epoch nr 69\n",
      "training loss: 0.6902327537536621\n",
      "validation loss: 0.6952066421508789\n",
      "training epoch nr 70\n",
      "training loss: 0.6902049779891968\n",
      "validation loss: 0.6947042346000671\n",
      "training epoch nr 71\n",
      "training loss: 0.6901662349700928\n",
      "validation loss: 0.6953296661376953\n",
      "training epoch nr 72\n",
      "training loss: 0.6901233792304993\n",
      "validation loss: 0.6943209171295166\n",
      "training epoch nr 73\n",
      "training loss: 0.6900528073310852\n",
      "validation loss: 0.6952924728393555\n",
      "training epoch nr 74\n",
      "training loss: 0.6899974346160889\n",
      "validation loss: 0.6943570375442505\n",
      "training epoch nr 75\n",
      "training loss: 0.6899733543395996\n",
      "validation loss: 0.6949316263198853\n",
      "training epoch nr 76\n",
      "training loss: 0.6898796558380127\n",
      "validation loss: 0.6951319575309753\n",
      "training epoch nr 77\n",
      "training loss: 0.6898857951164246\n",
      "validation loss: 0.6949586868286133\n",
      "training epoch nr 78\n",
      "training loss: 0.6897757053375244\n",
      "validation loss: 0.6946488618850708\n",
      "training epoch nr 79\n",
      "training loss: 0.6898792386054993\n",
      "validation loss: 0.694823145866394\n",
      "training epoch nr 80\n",
      "training loss: 0.6897404789924622\n",
      "validation loss: 0.6953446865081787\n",
      "training epoch nr 81\n",
      "training loss: 0.6897479295730591\n",
      "validation loss: 0.6949630379676819\n",
      "training epoch nr 82\n",
      "training loss: 0.6897904276847839\n",
      "validation loss: 0.6949238777160645\n",
      "training epoch nr 83\n",
      "training loss: 0.6897961497306824\n",
      "validation loss: 0.695158064365387\n",
      "training epoch nr 84\n",
      "training loss: 0.6897106766700745\n",
      "validation loss: 0.6955477595329285\n",
      "training epoch nr 85\n",
      "training loss: 0.6896362900733948\n",
      "validation loss: 0.6952000260353088\n",
      "training epoch nr 86\n",
      "training loss: 0.6896371245384216\n",
      "validation loss: 0.695224404335022\n",
      "training epoch nr 87\n",
      "training loss: 0.6895303726196289\n",
      "validation loss: 0.6956959962844849\n",
      "training epoch nr 88\n",
      "training loss: 0.6896112561225891\n",
      "validation loss: 0.6952507495880127\n",
      "training epoch nr 89\n",
      "training loss: 0.6896346211433411\n",
      "validation loss: 0.6950770616531372\n",
      "training epoch nr 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6895325779914856\n",
      "validation loss: 0.6955487728118896\n",
      "training epoch nr 91\n",
      "training loss: 0.6894334554672241\n",
      "validation loss: 0.6952421069145203\n",
      "training epoch nr 92\n",
      "training loss: 0.6892732381820679\n",
      "validation loss: 0.6959866881370544\n",
      "training epoch nr 93\n",
      "training loss: 0.689393162727356\n",
      "validation loss: 0.6960118412971497\n",
      "training epoch nr 94\n",
      "training loss: 0.6893923878669739\n",
      "validation loss: 0.6955026388168335\n",
      "training epoch nr 95\n",
      "training loss: 0.689291775226593\n",
      "validation loss: 0.6954663991928101\n",
      "training epoch nr 96\n",
      "training loss: 0.6892255544662476\n",
      "validation loss: 0.6952071785926819\n",
      "training epoch nr 97\n",
      "training loss: 0.6891579031944275\n",
      "validation loss: 0.6958538293838501\n",
      "training epoch nr 98\n",
      "training loss: 0.6892915964126587\n",
      "validation loss: 0.6953274011611938\n",
      "training epoch nr 99\n",
      "training loss: 0.6892670392990112\n",
      "validation loss: 0.6953089833259583\n",
      "minimum validation loss epochs: [24 13  9  7 18 22  8 14  6  4]\n"
     ]
    }
   ],
   "source": [
    "train_classifier(**classifier_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb65bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = full_single_evaluation(save_dir, save_dir, n_ensemble_epochs=10,\n",
    "                           extra_signal=not no_extra_signal, sic_range=(0, 20),\n",
    "                           savefig=os.path.join(save_dir, 'result_SIC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545c99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
