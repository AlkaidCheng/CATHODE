{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc61272",
   "metadata": {},
   "source": [
    "# Notebook 02: Pipeline for CWoLa results\n",
    "\n",
    "This notebook goes through the pipeline for obtaining results using the idealized CWoLa (Classification Without Labels) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5909982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from run_ANODE_training import main as train_DE\n",
    "from run_classifier_data_creation import main as create_data\n",
    "from run_classifier_training import main as train_classifier\n",
    "from run_ANODE_evaluation import main as eval_ANODE\n",
    "from evaluation_utils import full_single_evaluation, classic_ANODE_eval, minimum_val_loss_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67c95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'CWoLa'\n",
    "data_dir = '../separated_data'\n",
    "save_dir = 'CWoLa_models'\n",
    "# Shift on jet mass variables to be applied.\n",
    "datashift = 0.\n",
    "# Shift is not correlated to the actual mjj but randomized.\n",
    "random_shift = False\n",
    "# Whether to apply an (ANODE paper) fiducial cut on the data (and samples).\n",
    "fiducial_cut = False\n",
    "# Suppress the processing of the extra signal sample.\n",
    "no_extra_signal = True\n",
    "verbose = False\n",
    "\n",
    "# ANODE model config file (.yml).\n",
    "DE_config_file = '../DE_MAF_model.yml'\n",
    "# 'Number of Density Estimation training epochs.'\n",
    "DE_epochs = 100\n",
    "# Batch size during density estimation training.\n",
    "DE_batch_size = 256\n",
    "# Skips the density estimation (loads existing files instead).\n",
    "DF_skip = False\n",
    "# Turns off the logit transform in the density estimator.\n",
    "DE_no_logit = False\n",
    "\n",
    "# File name for the density estimator.\n",
    "DE_file_name = 'my_ANODE_model'\n",
    "\n",
    "# Classifier model config file (.yml).\n",
    "cf_config_file = '../classifier.yml'\n",
    "\n",
    "# Number of classifier training epochs\n",
    "cf_epochs = 100\n",
    "# Number of samples to be generated. Currently the samples will be cut down to match data proportion.\n",
    "cf_n_samples = 130000\n",
    "\n",
    "# Sample the conditional from a KDE fit rather than a uniform distribution.\n",
    "cf_realistic_conditional = False\n",
    "# Bandwith of the KDE fit (used when realistic_conditional is selected)\n",
    "cf_KDE_bandwidth = 0.01\n",
    "# Add the full number of samples to the training set rather than mixing it in equal parts with data.\n",
    "cf_oversampling = True\n",
    "# Turns off logit tranform in the classifier.\n",
    "cf_no_logit = True\n",
    "# Space-separated list of pre-sampled npy files of physical variables if the sampling has been done externally. The format is \n",
    "# (mjj, mj1, dmj, tau21_1, tau21_2)\n",
    "cf_external_samples = \"\"\n",
    "# Lower boundary of signal region.\n",
    "cf_SR_min = 3.3\n",
    "# Upper boundary of signal region.\n",
    "cf_SR_max = 3.7\n",
    "# Number of independent classifier training runs.\n",
    "cf_n_runs = 10\n",
    "# Batch size during classifier training.\n",
    "cf_batch_size = 128\n",
    "# Use the conditional variable as classifier input during training.\n",
    "cf_use_mjj = False\n",
    "# Weight the classes according to their occurence in the training set. \n",
    "# Necessary if the training set was intentionally oversampled.'\n",
    "cf_use_class_weights = True\n",
    "# Central value of signal region. Must only be given for using CWoLa with weights.\n",
    "cf_SR_center = 3.5\n",
    "# Make use of extra background (for supervised and idealized AD).\n",
    "cf_extra_bkg = False\n",
    "# Define a separate validation set to pick the classifier epochs.\n",
    "cf_separate_val_set = True\n",
    "# Save the tensorflow model after each epoch instead of saving predictions.\n",
    "cf_save_model = True\n",
    "# Skips the creation of the classifier dataset (loads existing files instead).\n",
    "cf_skip_create = False\n",
    "# Skips the training of the classifier (loads existing files instead).\n",
    "cf_skip_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd0bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_creation_kwargs = {\n",
    "    'savedir': save_dir,\n",
    "    'datashift': datashift,\n",
    "    'data_dir': data_dir,\n",
    "    'random_shift': random_shift,\n",
    "    'config_file': DE_config_file,\n",
    "    'verbose': verbose,\n",
    "    'fiducial_cut': fiducial_cut,\n",
    "    'n_samples': cf_n_samples,\n",
    "    'realistic_conditional': cf_realistic_conditional,\n",
    "    'KDE_bandwidth': cf_KDE_bandwidth,\n",
    "    'oversampling': cf_oversampling,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'CWoLa': True,\n",
    "    'supervised': False,\n",
    "    'idealized_AD': False,\n",
    "    'no_logit': cf_no_logit,\n",
    "    'no_logit_trained': DE_no_logit,\n",
    "    'external_samples': cf_external_samples,\n",
    "    'SR_min': cf_SR_min,\n",
    "    'SR_max': cf_SR_max,\n",
    "    'extra_bkg': cf_extra_bkg,\n",
    "    'separate_val_set': cf_separate_val_set,\n",
    "    'ANODE_models': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9d27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data_handler import LHCORD_data_handler, sample_handler, mix_data_samples, plot_data_sample_comparison\n",
    "from density_estimator import DensityEstimator\n",
    "\n",
    "def create_data(**kwargs):\n",
    "\n",
    "    assert not ((not (kwargs['supervised'] or kwargs['idealized_AD'] or kwargs['CWoLa']) and\\\n",
    "                 kwargs['external_samples'] == \"\") and kwargs['ANODE_models'] == \"\"), (\n",
    "                     \"ANODE models need to be given unless CWoLa, supervised, idealized_AD or\"\n",
    "                     \" external sampling is used.\")\n",
    "\n",
    "    # selecting appropriate device\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    print(\"cuda available:\", CUDA)\n",
    "    device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "\n",
    "    # checking for data separation\n",
    "    data_files = os.listdir(kwargs['data_dir'])\n",
    "    if \"innerdata_val.npy\" in data_files:\n",
    "        finer_data_split = True\n",
    "    else:\n",
    "        finer_data_split = False\n",
    "\n",
    "    if finer_data_split:\n",
    "        innerdata_train_path = [os.path.join(kwargs['data_dir'], 'innerdata_train.npy')]\n",
    "        innerdata_val_path = [os.path.join(kwargs['data_dir'], 'innerdata_val.npy')]\n",
    "        innerdata_test_path = [os.path.join(kwargs['data_dir'], 'innerdata_test.npy')]\n",
    "        if \"innerdata_extrabkg_test.npy\" in data_files:\n",
    "            innerdata_test_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_test.npy'))\n",
    "        extrasig_path = None\n",
    "        if kwargs['supervised']:\n",
    "            innerdata_train_path = []\n",
    "            innerdata_val_path = []\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_val.npy'))\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy'))\n",
    "            extra_bkg = None\n",
    "        elif kwargs['idealized_AD']:\n",
    "            extra_bkg = [os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'),\n",
    "                         os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy')]\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "\n",
    "    else:\n",
    "        innerdata_train_path = os.path.join(kwargs['data_dir'], 'innerdata_train.npy')\n",
    "        extrasig_path = os.path.join(kwargs['data_dir'], 'innerdata_extrasig.npy')\n",
    "        if kwargs['extra_bkg']:\n",
    "            extra_bkg = os.path.join(kwargs['data_dir'], 'innerdata_extrabkg.npy')\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "        innerdata_val_path = None\n",
    "        innerdata_test_path = os.path.join(kwargs['data_dir'], 'innerdata_test.npy')\n",
    "\n",
    "    # data preprocessing\n",
    "    data = LHCORD_data_handler(innerdata_train_path,\n",
    "                               innerdata_test_path,\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_train.npy'),\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_test.npy'),\n",
    "                               extrasig_path,\n",
    "                               inner_extrabkg_path=extra_bkg,\n",
    "                               inner_val_path=innerdata_val_path,\n",
    "                               batch_size=256,\n",
    "                               device=device)\n",
    "    if kwargs['datashift'] != 0:\n",
    "        print(\"applying a datashift of\", kwargs['datashift'])\n",
    "        data.shift_data(kwargs['datashift'], constant_shift=False, random_shift=kwargs['random_shift'],\n",
    "                        shift_mj1=True, shift_dm=True, additional_shift=False)\n",
    "\n",
    "    if kwargs['CWoLa']:\n",
    "        # data preprocessing\n",
    "        samples = None\n",
    "        data.preprocess_CWoLa_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit'],\n",
    "                                   outer_range=(kwargs['SR_min']-0.2, kwargs['SR_max']+0.2))\n",
    "\n",
    "    else:\n",
    "        # data preprocessing\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'],\n",
    "                                   no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # model instantiation\n",
    "        if len(kwargs['external_samples']) > 0:\n",
    "            model_list = None\n",
    "            loaded_samples = [np.load(sample_path) for sample_path in kwargs['external_samples']]\n",
    "            external_sample = np.concatenate(loaded_samples)\n",
    "        else:\n",
    "            model_list = []\n",
    "            for model_path in kwargs['ANODE_models']:\n",
    "                anode = DensityEstimator(kwargs['config_file'],\n",
    "                                         eval_mode=True,\n",
    "                                         load_path=model_path,\n",
    "                                         device=device, verbose=kwargs['verbose'],\n",
    "                                         bound=kwargs['no_logit_trained'])\n",
    "                model_list.append(anode.model)\n",
    "            external_sample = None\n",
    "\n",
    "        # generate samples\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            uniform_cond = not kwargs['realistic_conditional']\n",
    "            samples = sample_handler(model_list, kwargs['n_samples'], data, cond_min=kwargs['SR_min'],\n",
    "                                     cond_max=kwargs['SR_max'], uniform_cond=uniform_cond,\n",
    "                                     external_sample=external_sample,\n",
    "                                     device=device, no_logit=kwargs['no_logit_trained'],\n",
    "                                     no_mean_shift=kwargs['no_logit_trained'],\n",
    "                                     KDE_bandwidth=kwargs['KDE_bandwidth'])\n",
    "        else:\n",
    "            samples = None\n",
    "\n",
    "        # redo data preprocessing if the classifier should not use logit but ANODE did\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # sample preprocessing\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            samples.preprocess_samples(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                       no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "\n",
    "    # sample mixing\n",
    "    X_train, y_train, X_test, y_test, X_extrasig, y_extrasig = mix_data_samples(\n",
    "        data, samples_handler=samples, oversampling=kwargs['oversampling'],\n",
    "        savedir=kwargs['savedir'], CWoLa=kwargs['CWoLa'], supervised=kwargs['supervised'],\n",
    "        idealized_AD=kwargs['idealized_AD'], separate_val_set=kwargs['separate_val_set'] or finer_data_split)\n",
    "\n",
    "    # sanity checks\n",
    "    if not kwargs['CWoLa'] and not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "        samples.sanity_check(savefig=os.path.join(kwargs['savedir'], \"sanity_check\"), suppress_show=True)\n",
    "        samples.sanity_check_after_cuts(savefig=os.path.join(kwargs['savedir'], \"sanity_check_cuts\"),\n",
    "                                        suppress_show=True)\n",
    "\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "        X_val = X_extrasig\n",
    "        if kwargs['supervised']:\n",
    "            y_train = X_train[:, -1]\n",
    "            y_test = X_test[:, -1]\n",
    "            y_val = X_val[:, -1]\n",
    "        else:\n",
    "            y_val = X_val[:, -2]\n",
    "        plot_data_sample_comparison(X_val, y_val, title=\"validation set\",\n",
    "                                    savefig=os.path.join(kwargs['savedir'],\n",
    "                                                         \"data_sample_comparison_val\"),\n",
    "                                    suppress_show=True)\n",
    "\n",
    "    plot_data_sample_comparison(X_train, y_train, title=\"training set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_train\"),\n",
    "                                suppress_show=True)\n",
    "    plot_data_sample_comparison(X_test, y_test, title=\"test set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_test\"),\n",
    "                                suppress_show=True)\n",
    "\n",
    "    print(\"number of training data =\", X_train.shape[0])\n",
    "    print(\"number of test data =\", X_test.shape[0])\n",
    "    if not kwargs['no_extra_signal']:\n",
    "        if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "            print(\"number of validation data =\", X_val.shape[0])\n",
    "        elif extrasig_path is not None:\n",
    "            print(\"number of extra signal data =\", X_extrasig.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22965cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cern.ch/work/c/chlcheng/local/miniconda/envs/root-latest/lib/python3.8/site-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "/afs/cern.ch/work/c/chlcheng/local/miniconda/envs/root-latest/lib/python3.8/site-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data = 125093\n",
      "number of test data = 359934\n"
     ]
    }
   ],
   "source": [
    "create_data(**data_creation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2556e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_kwargs = {\n",
    "    'config_file': cf_config_file,\n",
    "    'data_dir': save_dir,\n",
    "    'savedir': save_dir,\n",
    "    'verbose': verbose,\n",
    "    'epochs': cf_epochs,\n",
    "    'n_runs': cf_n_runs,\n",
    "    'batch_size': cf_batch_size,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'use_mjj': cf_use_mjj,\n",
    "    'supervised': False,\n",
    "    'use_class_weights': cf_oversampling or cf_use_class_weights,\n",
    "    'CWoLa': True,\n",
    "    'SR_center': cf_SR_center,\n",
    "    'save_model': cf_save_model,\n",
    "    'separate_val_set': cf_separate_val_set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e582c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier_training_utils import train_n_models, plot_classifier_losses\n",
    "from evaluation_utils import minimum_val_loss_model_evaluation\n",
    "import matplotlib as mpl\n",
    "\n",
    "def train_classifier(**kwargs):\n",
    "\n",
    "    # loading the data\n",
    "    # TODO get rid of the y's since the information is fully included in X\n",
    "    X_train = np.load(os.path.join(kwargs['data_dir'], 'X_train.npy'))\n",
    "    X_test = np.load(os.path.join(kwargs['data_dir'], 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(kwargs['data_dir'], 'y_train.npy'))\n",
    "    y_test = np.load(os.path.join(kwargs['data_dir'], 'y_test.npy'))\n",
    "    if kwargs['no_extra_signal'] or kwargs['supervised']:\n",
    "        X_extrasig = None\n",
    "    else:\n",
    "        X_extrasig = np.load(os.path.join(kwargs['data_dir'], 'X_extrasig.npy'))\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set']:\n",
    "        X_val = np.load(os.path.join(kwargs['data_dir'], 'X_validation.npy'))        \n",
    "    else:\n",
    "        X_val = None\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        if not os.path.exists(kwargs['savedir']):\n",
    "            os.makedirs(kwargs['savedir'])\n",
    "        save_model = os.path.join(kwargs['savedir'], \"model\")\n",
    "    else:\n",
    "        save_model = None\n",
    "\n",
    "    # actual training\n",
    "    loss_matris, val_loss_matris = train_n_models(\n",
    "        kwargs['n_runs'], kwargs['config_file'], kwargs['epochs'], X_train, y_train, X_test, y_test,\n",
    "        X_extrasig=X_extrasig, X_val=X_val, use_mjj=kwargs['use_mjj'], batch_size=kwargs['batch_size'],\n",
    "        supervised=kwargs['supervised'], use_class_weights=kwargs['use_class_weights'],\n",
    "        CWoLa=kwargs['CWoLa'], SR_center=kwargs['SR_center'], verbose=kwargs['verbose'],\n",
    "        savedir=kwargs['savedir'], save_model=save_model)\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        minimum_val_loss_model_evaluation(kwargs['data_dir'], kwargs['savedir'], n_epochs=10,\n",
    "                                use_mjj=kwargs['use_mjj'], extra_signal=not kwargs['no_extra_signal'])\n",
    "\n",
    "    for i in range(loss_matris.shape[0]):\n",
    "        plot_classifier_losses(\n",
    "            loss_matris[i], val_loss_matris[i],\n",
    "            savefig=save_model+\"_run\"+str(i)+\"_loss_plot\",\n",
    "            suppress_show=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bfbc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model nr 0...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932655572891235\n",
      "validation loss: 0.6912049651145935\n",
      "training epoch nr 1\n",
      "training loss: 0.6931723952293396\n",
      "validation loss: 0.6924153566360474\n",
      "training epoch nr 2\n",
      "training loss: 0.6931125521659851\n",
      "validation loss: 0.6926141381263733\n",
      "training epoch nr 3\n",
      "training loss: 0.6931195259094238\n",
      "validation loss: 0.6879608035087585\n",
      "training epoch nr 4\n",
      "training loss: 0.6931072473526001\n",
      "validation loss: 0.6912862658500671\n",
      "training epoch nr 5\n",
      "training loss: 0.6930382251739502\n",
      "validation loss: 0.6973282694816589\n",
      "training epoch nr 6\n",
      "training loss: 0.6930860280990601\n",
      "validation loss: 0.6895106434822083\n",
      "training epoch nr 7\n",
      "training loss: 0.6929660439491272\n",
      "validation loss: 0.6931555271148682\n",
      "training epoch nr 8\n",
      "training loss: 0.6929461359977722\n",
      "validation loss: 0.6855359077453613\n",
      "training epoch nr 9\n",
      "training loss: 0.6929247379302979\n",
      "validation loss: 0.6935411095619202\n",
      "training epoch nr 10\n",
      "training loss: 0.6928834915161133\n",
      "validation loss: 0.6975111961364746\n",
      "training epoch nr 11\n",
      "training loss: 0.6928983330726624\n",
      "validation loss: 0.6924293041229248\n",
      "training epoch nr 12\n",
      "training loss: 0.6928824782371521\n",
      "validation loss: 0.6935727000236511\n",
      "training epoch nr 13\n",
      "training loss: 0.6927497982978821\n",
      "validation loss: 0.6935864686965942\n",
      "training epoch nr 14\n",
      "training loss: 0.6927535533905029\n",
      "validation loss: 0.6923081278800964\n",
      "training epoch nr 15\n",
      "training loss: 0.6926624774932861\n",
      "validation loss: 0.6945774555206299\n",
      "training epoch nr 16\n",
      "training loss: 0.6926494240760803\n",
      "validation loss: 0.6940436959266663\n",
      "training epoch nr 17\n",
      "training loss: 0.692548394203186\n",
      "validation loss: 0.6934527158737183\n",
      "training epoch nr 18\n",
      "training loss: 0.692561686038971\n",
      "validation loss: 0.6927157640457153\n",
      "training epoch nr 19\n",
      "training loss: 0.6925116181373596\n",
      "validation loss: 0.693496823310852\n",
      "training epoch nr 20\n",
      "training loss: 0.6925575137138367\n",
      "validation loss: 0.6943880319595337\n",
      "training epoch nr 21\n",
      "training loss: 0.6925114393234253\n",
      "validation loss: 0.6966375112533569\n",
      "training epoch nr 22\n",
      "training loss: 0.6924416422843933\n",
      "validation loss: 0.6950659155845642\n",
      "training epoch nr 23\n",
      "training loss: 0.6923410892486572\n",
      "validation loss: 0.6936709880828857\n",
      "training epoch nr 24\n",
      "training loss: 0.6923232078552246\n",
      "validation loss: 0.6923791170120239\n",
      "training epoch nr 25\n",
      "training loss: 0.6923337578773499\n",
      "validation loss: 0.693721354007721\n",
      "training epoch nr 26\n",
      "training loss: 0.6922548413276672\n",
      "validation loss: 0.6940532922744751\n",
      "training epoch nr 27\n",
      "training loss: 0.6922115683555603\n",
      "validation loss: 0.6933764815330505\n",
      "training epoch nr 28\n",
      "training loss: 0.6921843886375427\n",
      "validation loss: 0.6911320686340332\n",
      "training epoch nr 29\n",
      "training loss: 0.6921834349632263\n",
      "validation loss: 0.6938351988792419\n",
      "training epoch nr 30\n",
      "training loss: 0.692131757736206\n",
      "validation loss: 0.6943092346191406\n",
      "training epoch nr 31\n",
      "training loss: 0.6920444369316101\n",
      "validation loss: 0.6949125528335571\n",
      "training epoch nr 32\n",
      "training loss: 0.6920531392097473\n",
      "validation loss: 0.6907482743263245\n",
      "training epoch nr 33\n",
      "training loss: 0.6919465661048889\n",
      "validation loss: 0.6951391100883484\n",
      "training epoch nr 34\n",
      "training loss: 0.691895604133606\n",
      "validation loss: 0.696459949016571\n",
      "training epoch nr 35\n",
      "training loss: 0.6918749213218689\n",
      "validation loss: 0.6939330101013184\n",
      "training epoch nr 36\n",
      "training loss: 0.6918675303459167\n",
      "validation loss: 0.6965432167053223\n",
      "training epoch nr 37\n",
      "training loss: 0.6919317841529846\n",
      "validation loss: 0.6935979723930359\n",
      "training epoch nr 38\n",
      "training loss: 0.6917564868927002\n",
      "validation loss: 0.692404568195343\n",
      "training epoch nr 39\n",
      "training loss: 0.6916877627372742\n",
      "validation loss: 0.701802134513855\n",
      "training epoch nr 40\n",
      "training loss: 0.6916630268096924\n",
      "validation loss: 0.6970441341400146\n",
      "training epoch nr 41\n",
      "training loss: 0.6916131973266602\n",
      "validation loss: 0.6952608823776245\n",
      "training epoch nr 42\n",
      "training loss: 0.6915947794914246\n",
      "validation loss: 0.6982840299606323\n",
      "training epoch nr 43\n",
      "training loss: 0.6916207671165466\n",
      "validation loss: 0.6954789161682129\n",
      "training epoch nr 44\n",
      "training loss: 0.6916234493255615\n",
      "validation loss: 0.6939695477485657\n",
      "training epoch nr 45\n",
      "training loss: 0.691413164138794\n",
      "validation loss: 0.6971837878227234\n",
      "training epoch nr 46\n",
      "training loss: 0.6914123892784119\n",
      "validation loss: 0.6960898041725159\n",
      "training epoch nr 47\n",
      "training loss: 0.6913667917251587\n",
      "validation loss: 0.6952778697013855\n",
      "training epoch nr 48\n",
      "training loss: 0.6914455890655518\n",
      "validation loss: 0.69428950548172\n",
      "training epoch nr 49\n",
      "training loss: 0.691333532333374\n",
      "validation loss: 0.6974937915802002\n",
      "training epoch nr 50\n",
      "training loss: 0.691253125667572\n",
      "validation loss: 0.6981229782104492\n",
      "training epoch nr 51\n",
      "training loss: 0.6912692189216614\n",
      "validation loss: 0.6936323642730713\n",
      "training epoch nr 52\n",
      "training loss: 0.6911437511444092\n",
      "validation loss: 0.6937969923019409\n",
      "training epoch nr 53\n",
      "training loss: 0.6911488771438599\n",
      "validation loss: 0.7041649222373962\n",
      "training epoch nr 54\n",
      "training loss: 0.6910709142684937\n",
      "validation loss: 0.7019017338752747\n",
      "training epoch nr 55\n",
      "training loss: 0.6910857558250427\n",
      "validation loss: 0.6972591876983643\n",
      "training epoch nr 56\n",
      "training loss: 0.6910373568534851\n",
      "validation loss: 0.6984204053878784\n",
      "training epoch nr 57\n",
      "training loss: 0.6909489631652832\n",
      "validation loss: 0.6977614164352417\n",
      "training epoch nr 58\n",
      "training loss: 0.6908968091011047\n",
      "validation loss: 0.6976309418678284\n",
      "training epoch nr 59\n",
      "training loss: 0.690967857837677\n",
      "validation loss: 0.6968225240707397\n",
      "training epoch nr 60\n",
      "training loss: 0.6908703446388245\n",
      "validation loss: 0.6950353980064392\n",
      "training epoch nr 61\n",
      "training loss: 0.6907997727394104\n",
      "validation loss: 0.6974124312400818\n",
      "training epoch nr 62\n",
      "training loss: 0.6908156871795654\n",
      "validation loss: 0.7038851380348206\n",
      "training epoch nr 63\n",
      "training loss: 0.6907387375831604\n",
      "validation loss: 0.6942693591117859\n",
      "training epoch nr 64\n",
      "training loss: 0.6906518340110779\n",
      "validation loss: 0.6985962390899658\n",
      "training epoch nr 65\n",
      "training loss: 0.6906647086143494\n",
      "validation loss: 0.7006338834762573\n",
      "training epoch nr 66\n",
      "training loss: 0.6906908750534058\n",
      "validation loss: 0.6977601051330566\n",
      "training epoch nr 67\n",
      "training loss: 0.69053715467453\n",
      "validation loss: 0.7014939785003662\n",
      "training epoch nr 68\n",
      "training loss: 0.6905514597892761\n",
      "validation loss: 0.7011635303497314\n",
      "training epoch nr 69\n",
      "training loss: 0.6905390620231628\n",
      "validation loss: 0.7028581500053406\n",
      "training epoch nr 70\n",
      "training loss: 0.6902820467948914\n",
      "validation loss: 0.7030423283576965\n",
      "training epoch nr 71\n",
      "training loss: 0.6904692053794861\n",
      "validation loss: 0.7030230760574341\n",
      "training epoch nr 72\n",
      "training loss: 0.6902825236320496\n",
      "validation loss: 0.7033112645149231\n",
      "training epoch nr 73\n",
      "training loss: 0.6902214884757996\n",
      "validation loss: 0.7028408050537109\n",
      "training epoch nr 74\n",
      "training loss: 0.6902438402175903\n",
      "validation loss: 0.6968236565589905\n",
      "training epoch nr 75\n",
      "training loss: 0.6902000904083252\n",
      "validation loss: 0.7011772394180298\n",
      "training epoch nr 76\n",
      "training loss: 0.6901401281356812\n",
      "validation loss: 0.6954301595687866\n",
      "training epoch nr 77\n",
      "training loss: 0.6901368498802185\n",
      "validation loss: 0.6996276378631592\n",
      "training epoch nr 78\n",
      "training loss: 0.6901261806488037\n",
      "validation loss: 0.6976381540298462\n",
      "training epoch nr 79\n",
      "training loss: 0.6900456547737122\n",
      "validation loss: 0.7129006385803223\n",
      "training epoch nr 80\n",
      "training loss: 0.6899827718734741\n",
      "validation loss: 0.6930531859397888\n",
      "training epoch nr 81\n",
      "training loss: 0.6899595856666565\n",
      "validation loss: 0.6972655653953552\n",
      "training epoch nr 82\n",
      "training loss: 0.68978351354599\n",
      "validation loss: 0.7088640928268433\n",
      "training epoch nr 83\n",
      "training loss: 0.6898604035377502\n",
      "validation loss: 0.6960559487342834\n",
      "training epoch nr 84\n",
      "training loss: 0.6898404955863953\n",
      "validation loss: 0.7000056505203247\n",
      "training epoch nr 85\n",
      "training loss: 0.6896584630012512\n",
      "validation loss: 0.7001399993896484\n",
      "training epoch nr 86\n",
      "training loss: 0.6898314952850342\n",
      "validation loss: 0.7006289958953857\n",
      "training epoch nr 87\n",
      "training loss: 0.6896100640296936\n",
      "validation loss: 0.6992430686950684\n",
      "training epoch nr 88\n",
      "training loss: 0.689542293548584\n",
      "validation loss: 0.7054223418235779\n",
      "training epoch nr 89\n",
      "training loss: 0.6895601153373718\n",
      "validation loss: 0.7079354524612427\n",
      "training epoch nr 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6895545125007629\n",
      "validation loss: 0.6979000568389893\n",
      "training epoch nr 91\n",
      "training loss: 0.6893881559371948\n",
      "validation loss: 0.7023988962173462\n",
      "training epoch nr 92\n",
      "training loss: 0.6894462704658508\n",
      "validation loss: 0.6972627639770508\n",
      "training epoch nr 93\n",
      "training loss: 0.6894827485084534\n",
      "validation loss: 0.7077352404594421\n",
      "training epoch nr 94\n",
      "training loss: 0.6892298460006714\n",
      "validation loss: 0.6919747591018677\n",
      "training epoch nr 95\n",
      "training loss: 0.6892366409301758\n",
      "validation loss: 0.6956527829170227\n",
      "training epoch nr 96\n",
      "training loss: 0.6891956925392151\n",
      "validation loss: 0.7000218629837036\n",
      "training epoch nr 97\n",
      "training loss: 0.689211368560791\n",
      "validation loss: 0.6929028630256653\n",
      "training epoch nr 98\n",
      "training loss: 0.6891415119171143\n",
      "validation loss: 0.6985769271850586\n",
      "training epoch nr 99\n",
      "training loss: 0.6891866326332092\n",
      "validation loss: 0.7058608531951904\n",
      "Training model nr 1...\n",
      "training epoch nr 0\n",
      "training loss: 0.693309485912323\n",
      "validation loss: 0.6934164762496948\n",
      "training epoch nr 1\n",
      "training loss: 0.6931909918785095\n",
      "validation loss: 0.6926865577697754\n",
      "training epoch nr 2\n",
      "training loss: 0.693131685256958\n",
      "validation loss: 0.6942669153213501\n",
      "training epoch nr 3\n",
      "training loss: 0.6930745244026184\n",
      "validation loss: 0.6943046450614929\n",
      "training epoch nr 4\n",
      "training loss: 0.69301837682724\n",
      "validation loss: 0.6927452087402344\n",
      "training epoch nr 5\n",
      "training loss: 0.6929897665977478\n",
      "validation loss: 0.6928256750106812\n",
      "training epoch nr 6\n",
      "training loss: 0.6930307149887085\n",
      "validation loss: 0.6901934146881104\n",
      "training epoch nr 7\n",
      "training loss: 0.6929821372032166\n",
      "validation loss: 0.6924045085906982\n",
      "training epoch nr 8\n",
      "training loss: 0.6929874420166016\n",
      "validation loss: 0.6962904334068298\n",
      "training epoch nr 9\n",
      "training loss: 0.6929365992546082\n",
      "validation loss: 0.6938349604606628\n",
      "training epoch nr 10\n",
      "training loss: 0.6929332613945007\n",
      "validation loss: 0.6915715336799622\n",
      "training epoch nr 11\n",
      "training loss: 0.69292151927948\n",
      "validation loss: 0.6935712695121765\n",
      "training epoch nr 12\n",
      "training loss: 0.6927750706672668\n",
      "validation loss: 0.6933076977729797\n",
      "training epoch nr 13\n",
      "training loss: 0.6928780674934387\n",
      "validation loss: 0.6938236355781555\n",
      "training epoch nr 14\n",
      "training loss: 0.6927474737167358\n",
      "validation loss: 0.6903677582740784\n",
      "training epoch nr 15\n",
      "training loss: 0.6927940845489502\n",
      "validation loss: 0.6913779973983765\n",
      "training epoch nr 16\n",
      "training loss: 0.6928433179855347\n",
      "validation loss: 0.691767692565918\n",
      "training epoch nr 17\n",
      "training loss: 0.6927053928375244\n",
      "validation loss: 0.6927475333213806\n",
      "training epoch nr 18\n",
      "training loss: 0.6927050352096558\n",
      "validation loss: 0.6939671635627747\n",
      "training epoch nr 19\n",
      "training loss: 0.6926527619361877\n",
      "validation loss: 0.6956940293312073\n",
      "training epoch nr 20\n",
      "training loss: 0.6927412748336792\n",
      "validation loss: 0.6955485939979553\n",
      "training epoch nr 21\n",
      "training loss: 0.6926403045654297\n",
      "validation loss: 0.6927721500396729\n",
      "training epoch nr 22\n",
      "training loss: 0.6926296949386597\n",
      "validation loss: 0.6908891201019287\n",
      "training epoch nr 23\n",
      "training loss: 0.6926184892654419\n",
      "validation loss: 0.6933936476707458\n",
      "training epoch nr 24\n",
      "training loss: 0.6925569176673889\n",
      "validation loss: 0.691501796245575\n",
      "training epoch nr 25\n",
      "training loss: 0.6926066279411316\n",
      "validation loss: 0.6941214203834534\n",
      "training epoch nr 26\n",
      "training loss: 0.6925073266029358\n",
      "validation loss: 0.694164514541626\n",
      "training epoch nr 27\n",
      "training loss: 0.6924553513526917\n",
      "validation loss: 0.692926824092865\n",
      "training epoch nr 28\n",
      "training loss: 0.6924305558204651\n",
      "validation loss: 0.6937931180000305\n",
      "training epoch nr 29\n",
      "training loss: 0.6924974918365479\n",
      "validation loss: 0.6937137246131897\n",
      "training epoch nr 30\n",
      "training loss: 0.6924547553062439\n",
      "validation loss: 0.6919111609458923\n",
      "training epoch nr 31\n",
      "training loss: 0.6923570036888123\n",
      "validation loss: 0.6898375749588013\n",
      "training epoch nr 32\n",
      "training loss: 0.6924523711204529\n",
      "validation loss: 0.693248987197876\n",
      "training epoch nr 33\n",
      "training loss: 0.6923323273658752\n",
      "validation loss: 0.6932103633880615\n",
      "training epoch nr 34\n",
      "training loss: 0.6923224925994873\n",
      "validation loss: 0.6927444338798523\n",
      "training epoch nr 35\n",
      "training loss: 0.692338764667511\n",
      "validation loss: 0.692995011806488\n",
      "training epoch nr 36\n",
      "training loss: 0.6922644972801208\n",
      "validation loss: 0.6932243704795837\n",
      "training epoch nr 37\n",
      "training loss: 0.69215989112854\n",
      "validation loss: 0.6934846043586731\n",
      "training epoch nr 38\n",
      "training loss: 0.6921908259391785\n",
      "validation loss: 0.6938645839691162\n",
      "training epoch nr 39\n",
      "training loss: 0.692190945148468\n",
      "validation loss: 0.6952939629554749\n",
      "training epoch nr 40\n",
      "training loss: 0.6921368837356567\n",
      "validation loss: 0.6916398406028748\n",
      "training epoch nr 41\n",
      "training loss: 0.692096471786499\n",
      "validation loss: 0.6924965977668762\n",
      "training epoch nr 42\n",
      "training loss: 0.6920352578163147\n",
      "validation loss: 0.691836953163147\n",
      "training epoch nr 43\n",
      "training loss: 0.6919995546340942\n",
      "validation loss: 0.6919435858726501\n",
      "training epoch nr 44\n",
      "training loss: 0.6919370293617249\n",
      "validation loss: 0.6889528036117554\n",
      "training epoch nr 45\n",
      "training loss: 0.6920143365859985\n",
      "validation loss: 0.6912454962730408\n",
      "training epoch nr 46\n",
      "training loss: 0.6919806599617004\n",
      "validation loss: 0.6945004463195801\n",
      "training epoch nr 47\n",
      "training loss: 0.6919392347335815\n",
      "validation loss: 0.6901125907897949\n",
      "training epoch nr 48\n",
      "training loss: 0.6918470859527588\n",
      "validation loss: 0.69197678565979\n",
      "training epoch nr 49\n",
      "training loss: 0.6919001340866089\n",
      "validation loss: 0.6924684643745422\n",
      "training epoch nr 50\n",
      "training loss: 0.6918325424194336\n",
      "validation loss: 0.695156455039978\n",
      "training epoch nr 51\n",
      "training loss: 0.6918402910232544\n",
      "validation loss: 0.693989634513855\n",
      "training epoch nr 52\n",
      "training loss: 0.6917117238044739\n",
      "validation loss: 0.6950396299362183\n",
      "training epoch nr 53\n",
      "training loss: 0.6917576789855957\n",
      "validation loss: 0.6939475536346436\n",
      "training epoch nr 54\n",
      "training loss: 0.6917271018028259\n",
      "validation loss: 0.6919820308685303\n",
      "training epoch nr 55\n",
      "training loss: 0.6916541457176208\n",
      "validation loss: 0.6938533782958984\n",
      "training epoch nr 56\n",
      "training loss: 0.6916210055351257\n",
      "validation loss: 0.6942122578620911\n",
      "training epoch nr 57\n",
      "training loss: 0.6917111277580261\n",
      "validation loss: 0.696218729019165\n",
      "training epoch nr 58\n",
      "training loss: 0.6916506886482239\n",
      "validation loss: 0.6942375898361206\n",
      "training epoch nr 59\n",
      "training loss: 0.6916086673736572\n",
      "validation loss: 0.6942346096038818\n",
      "training epoch nr 60\n",
      "training loss: 0.6916677951812744\n",
      "validation loss: 0.6968662142753601\n",
      "training epoch nr 61\n",
      "training loss: 0.6916235089302063\n",
      "validation loss: 0.6960864067077637\n",
      "training epoch nr 62\n",
      "training loss: 0.6915552616119385\n",
      "validation loss: 0.6960397958755493\n",
      "training epoch nr 63\n",
      "training loss: 0.6916244626045227\n",
      "validation loss: 0.6939795017242432\n",
      "training epoch nr 64\n",
      "training loss: 0.6914832592010498\n",
      "validation loss: 0.6973550319671631\n",
      "training epoch nr 65\n",
      "training loss: 0.6914488077163696\n",
      "validation loss: 0.6959388256072998\n",
      "training epoch nr 66\n",
      "training loss: 0.6913902163505554\n",
      "validation loss: 0.6922152042388916\n",
      "training epoch nr 67\n",
      "training loss: 0.6913414001464844\n",
      "validation loss: 0.6886181235313416\n",
      "training epoch nr 68\n",
      "training loss: 0.6913329362869263\n",
      "validation loss: 0.6980594992637634\n",
      "training epoch nr 69\n",
      "training loss: 0.6913709044456482\n",
      "validation loss: 0.6980018019676208\n",
      "training epoch nr 70\n",
      "training loss: 0.6912629008293152\n",
      "validation loss: 0.6957790851593018\n",
      "training epoch nr 71\n",
      "training loss: 0.6913122534751892\n",
      "validation loss: 0.6928889751434326\n",
      "training epoch nr 72\n",
      "training loss: 0.691233217716217\n",
      "validation loss: 0.695444643497467\n",
      "training epoch nr 73\n",
      "training loss: 0.6912553906440735\n",
      "validation loss: 0.6988691687583923\n",
      "training epoch nr 74\n",
      "training loss: 0.6911900639533997\n",
      "validation loss: 0.695065975189209\n",
      "training epoch nr 75\n",
      "training loss: 0.6911585330963135\n",
      "validation loss: 0.6974976062774658\n",
      "training epoch nr 76\n",
      "training loss: 0.6911329627037048\n",
      "validation loss: 0.6958991885185242\n",
      "training epoch nr 77\n",
      "training loss: 0.6911384463310242\n",
      "validation loss: 0.6957729458808899\n",
      "training epoch nr 78\n",
      "training loss: 0.6909896731376648\n",
      "validation loss: 0.6971830129623413\n",
      "training epoch nr 79\n",
      "training loss: 0.6910057067871094\n",
      "validation loss: 0.6957355737686157\n",
      "training epoch nr 80\n",
      "training loss: 0.6910321116447449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6949307918548584\n",
      "training epoch nr 81\n",
      "training loss: 0.6909645795822144\n",
      "validation loss: 0.6990415453910828\n",
      "training epoch nr 82\n",
      "training loss: 0.6910693645477295\n",
      "validation loss: 0.6967562437057495\n",
      "training epoch nr 83\n",
      "training loss: 0.690881609916687\n",
      "validation loss: 0.6942293643951416\n",
      "training epoch nr 84\n",
      "training loss: 0.6908169388771057\n",
      "validation loss: 0.697820246219635\n",
      "training epoch nr 85\n",
      "training loss: 0.6907696723937988\n",
      "validation loss: 0.6934583187103271\n",
      "training epoch nr 86\n",
      "training loss: 0.6908106803894043\n",
      "validation loss: 0.6947358846664429\n",
      "training epoch nr 87\n",
      "training loss: 0.6907904744148254\n",
      "validation loss: 0.6913406848907471\n",
      "training epoch nr 88\n",
      "training loss: 0.6907577514648438\n",
      "validation loss: 0.6962429285049438\n",
      "training epoch nr 89\n",
      "training loss: 0.690734326839447\n",
      "validation loss: 0.6979172825813293\n",
      "training epoch nr 90\n",
      "training loss: 0.6907042264938354\n",
      "validation loss: 0.6952803730964661\n",
      "training epoch nr 91\n",
      "training loss: 0.6907477974891663\n",
      "validation loss: 0.6941346526145935\n",
      "training epoch nr 92\n",
      "training loss: 0.6907374262809753\n",
      "validation loss: 0.6957037448883057\n",
      "training epoch nr 93\n",
      "training loss: 0.6906552910804749\n",
      "validation loss: 0.6968809962272644\n",
      "training epoch nr 94\n",
      "training loss: 0.6906318068504333\n",
      "validation loss: 0.6988043189048767\n",
      "training epoch nr 95\n",
      "training loss: 0.6905655264854431\n",
      "validation loss: 0.6955788731575012\n",
      "training epoch nr 96\n",
      "training loss: 0.690475583076477\n",
      "validation loss: 0.69654780626297\n",
      "training epoch nr 97\n",
      "training loss: 0.6905145049095154\n",
      "validation loss: 0.6973202228546143\n",
      "training epoch nr 98\n",
      "training loss: 0.6906420588493347\n",
      "validation loss: 0.6951387524604797\n",
      "training epoch nr 99\n",
      "training loss: 0.6904872059822083\n",
      "validation loss: 0.6979081630706787\n",
      "Training model nr 2...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933895945549011\n",
      "validation loss: 0.694826602935791\n",
      "training epoch nr 1\n",
      "training loss: 0.693226158618927\n",
      "validation loss: 0.6952396631240845\n",
      "training epoch nr 2\n",
      "training loss: 0.6931849718093872\n",
      "validation loss: 0.6907419562339783\n",
      "training epoch nr 3\n",
      "training loss: 0.6930534243583679\n",
      "validation loss: 0.6928643584251404\n",
      "training epoch nr 4\n",
      "training loss: 0.69303959608078\n",
      "validation loss: 0.6946988701820374\n",
      "training epoch nr 5\n",
      "training loss: 0.6929884552955627\n",
      "validation loss: 0.6924982666969299\n",
      "training epoch nr 6\n",
      "training loss: 0.6929633617401123\n",
      "validation loss: 0.6936447620391846\n",
      "training epoch nr 7\n",
      "training loss: 0.6929122805595398\n",
      "validation loss: 0.6969402432441711\n",
      "training epoch nr 8\n",
      "training loss: 0.6929013133049011\n",
      "validation loss: 0.6931226849555969\n",
      "training epoch nr 9\n",
      "training loss: 0.6928541660308838\n",
      "validation loss: 0.69300377368927\n",
      "training epoch nr 10\n",
      "training loss: 0.6928635835647583\n",
      "validation loss: 0.692545473575592\n",
      "training epoch nr 11\n",
      "training loss: 0.6928708553314209\n",
      "validation loss: 0.6936153173446655\n",
      "training epoch nr 12\n",
      "training loss: 0.6928284168243408\n",
      "validation loss: 0.6934579014778137\n",
      "training epoch nr 13\n",
      "training loss: 0.6928826570510864\n",
      "validation loss: 0.6941778063774109\n",
      "training epoch nr 14\n",
      "training loss: 0.6928293704986572\n",
      "validation loss: 0.691674530506134\n",
      "training epoch nr 15\n",
      "training loss: 0.6927754282951355\n",
      "validation loss: 0.700580894947052\n",
      "training epoch nr 16\n",
      "training loss: 0.6927505135536194\n",
      "validation loss: 0.6922682523727417\n",
      "training epoch nr 17\n",
      "training loss: 0.6926084756851196\n",
      "validation loss: 0.7094751596450806\n",
      "training epoch nr 18\n",
      "training loss: 0.6926226615905762\n",
      "validation loss: 0.6942353248596191\n",
      "training epoch nr 19\n",
      "training loss: 0.6927214860916138\n",
      "validation loss: 0.6976674795150757\n",
      "training epoch nr 20\n",
      "training loss: 0.6926579475402832\n",
      "validation loss: 0.6904140710830688\n",
      "training epoch nr 21\n",
      "training loss: 0.6926242113113403\n",
      "validation loss: 0.6931827664375305\n",
      "training epoch nr 22\n",
      "training loss: 0.6926124691963196\n",
      "validation loss: 0.6925279498100281\n",
      "training epoch nr 23\n",
      "training loss: 0.6925814151763916\n",
      "validation loss: 0.6946916580200195\n",
      "training epoch nr 24\n",
      "training loss: 0.6925866603851318\n",
      "validation loss: 0.6938698887825012\n",
      "training epoch nr 25\n",
      "training loss: 0.6925650238990784\n",
      "validation loss: 0.6941433548927307\n",
      "training epoch nr 26\n",
      "training loss: 0.6925854682922363\n",
      "validation loss: 0.6925972104072571\n",
      "training epoch nr 27\n",
      "training loss: 0.6924708485603333\n",
      "validation loss: 0.6958464980125427\n",
      "training epoch nr 28\n",
      "training loss: 0.6924459338188171\n",
      "validation loss: 0.6955220699310303\n",
      "training epoch nr 29\n",
      "training loss: 0.6925071477890015\n",
      "validation loss: 0.696369469165802\n",
      "training epoch nr 30\n",
      "training loss: 0.6924199461936951\n",
      "validation loss: 0.6954439878463745\n",
      "training epoch nr 31\n",
      "training loss: 0.6924746632575989\n",
      "validation loss: 0.6930004954338074\n",
      "training epoch nr 32\n",
      "training loss: 0.6923514008522034\n",
      "validation loss: 0.6921693682670593\n",
      "training epoch nr 33\n",
      "training loss: 0.692330539226532\n",
      "validation loss: 0.6934377551078796\n",
      "training epoch nr 34\n",
      "training loss: 0.6923447251319885\n",
      "validation loss: 0.6914737820625305\n",
      "training epoch nr 35\n",
      "training loss: 0.6923508644104004\n",
      "validation loss: 0.6948867440223694\n",
      "training epoch nr 36\n",
      "training loss: 0.6923592686653137\n",
      "validation loss: 0.6933495402336121\n",
      "training epoch nr 37\n",
      "training loss: 0.6923096776008606\n",
      "validation loss: 0.69533371925354\n",
      "training epoch nr 38\n",
      "training loss: 0.6922703385353088\n",
      "validation loss: 0.6933428645133972\n",
      "training epoch nr 39\n",
      "training loss: 0.6922270059585571\n",
      "validation loss: 0.6929639577865601\n",
      "training epoch nr 40\n",
      "training loss: 0.692272961139679\n",
      "validation loss: 0.6931250095367432\n",
      "training epoch nr 41\n",
      "training loss: 0.6922463178634644\n",
      "validation loss: 0.6931905150413513\n",
      "training epoch nr 42\n",
      "training loss: 0.692204475402832\n",
      "validation loss: 0.6900323629379272\n",
      "training epoch nr 43\n",
      "training loss: 0.6922374367713928\n",
      "validation loss: 0.6971116065979004\n",
      "training epoch nr 44\n",
      "training loss: 0.6921362280845642\n",
      "validation loss: 0.6943490505218506\n",
      "training epoch nr 45\n",
      "training loss: 0.6921259760856628\n",
      "validation loss: 0.6978538632392883\n",
      "training epoch nr 46\n",
      "training loss: 0.692098081111908\n",
      "validation loss: 0.6920453310012817\n",
      "training epoch nr 47\n",
      "training loss: 0.6920608878135681\n",
      "validation loss: 0.69291752576828\n",
      "training epoch nr 48\n",
      "training loss: 0.6919638514518738\n",
      "validation loss: 0.6887894868850708\n",
      "training epoch nr 49\n",
      "training loss: 0.6920222043991089\n",
      "validation loss: 0.6931740641593933\n",
      "training epoch nr 50\n",
      "training loss: 0.6919511556625366\n",
      "validation loss: 0.6937536597251892\n",
      "training epoch nr 51\n",
      "training loss: 0.6919938921928406\n",
      "validation loss: 0.6911828517913818\n",
      "training epoch nr 52\n",
      "training loss: 0.6919000148773193\n",
      "validation loss: 0.6910091042518616\n",
      "training epoch nr 53\n",
      "training loss: 0.6919410824775696\n",
      "validation loss: 0.6906774044036865\n",
      "training epoch nr 54\n",
      "training loss: 0.6918702125549316\n",
      "validation loss: 0.6925488114356995\n",
      "training epoch nr 55\n",
      "training loss: 0.6918121576309204\n",
      "validation loss: 0.6967705488204956\n",
      "training epoch nr 56\n",
      "training loss: 0.6917912364006042\n",
      "validation loss: 0.6913899779319763\n",
      "training epoch nr 57\n",
      "training loss: 0.6918109655380249\n",
      "validation loss: 0.6911209225654602\n",
      "training epoch nr 58\n",
      "training loss: 0.6916813850402832\n",
      "validation loss: 0.692316472530365\n",
      "training epoch nr 59\n",
      "training loss: 0.6916975975036621\n",
      "validation loss: 0.6935359835624695\n",
      "training epoch nr 60\n",
      "training loss: 0.6916742324829102\n",
      "validation loss: 0.6909522414207458\n",
      "training epoch nr 61\n",
      "training loss: 0.6916666030883789\n",
      "validation loss: 0.6883988976478577\n",
      "training epoch nr 62\n",
      "training loss: 0.6915633082389832\n",
      "validation loss: 0.6933266520500183\n",
      "training epoch nr 63\n",
      "training loss: 0.6915951371192932\n",
      "validation loss: 0.6915369033813477\n",
      "training epoch nr 64\n",
      "training loss: 0.6915054321289062\n",
      "validation loss: 0.6930105090141296\n",
      "training epoch nr 65\n",
      "training loss: 0.6914057731628418\n",
      "validation loss: 0.6973413825035095\n",
      "training epoch nr 66\n",
      "training loss: 0.6914007067680359\n",
      "validation loss: 0.6928690671920776\n",
      "training epoch nr 67\n",
      "training loss: 0.6913830637931824\n",
      "validation loss: 0.6981894969940186\n",
      "training epoch nr 68\n",
      "training loss: 0.6913341283798218\n",
      "validation loss: 0.6918001770973206\n",
      "training epoch nr 69\n",
      "training loss: 0.6913025975227356\n",
      "validation loss: 0.6954978108406067\n",
      "training epoch nr 70\n",
      "training loss: 0.6912721991539001\n",
      "validation loss: 0.6937742829322815\n",
      "training epoch nr 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6912575364112854\n",
      "validation loss: 0.6936166882514954\n",
      "training epoch nr 72\n",
      "training loss: 0.6911873817443848\n",
      "validation loss: 0.6935134530067444\n",
      "training epoch nr 73\n",
      "training loss: 0.6911927461624146\n",
      "validation loss: 0.6909866333007812\n",
      "training epoch nr 74\n",
      "training loss: 0.6911748647689819\n",
      "validation loss: 0.6931139230728149\n",
      "training epoch nr 75\n",
      "training loss: 0.6910248398780823\n",
      "validation loss: 0.6975204944610596\n",
      "training epoch nr 76\n",
      "training loss: 0.6909784078598022\n",
      "validation loss: 0.6927093863487244\n",
      "training epoch nr 77\n",
      "training loss: 0.691091001033783\n",
      "validation loss: 0.6930666565895081\n",
      "training epoch nr 78\n",
      "training loss: 0.690910816192627\n",
      "validation loss: 0.6960469484329224\n",
      "training epoch nr 79\n",
      "training loss: 0.6908534169197083\n",
      "validation loss: 0.6934594511985779\n",
      "training epoch nr 80\n",
      "training loss: 0.6908184289932251\n",
      "validation loss: 0.6966127157211304\n",
      "training epoch nr 81\n",
      "training loss: 0.6908301115036011\n",
      "validation loss: 0.6963210701942444\n",
      "training epoch nr 82\n",
      "training loss: 0.6908832788467407\n",
      "validation loss: 0.6934575438499451\n",
      "training epoch nr 83\n",
      "training loss: 0.6907669901847839\n",
      "validation loss: 0.6950653791427612\n",
      "training epoch nr 84\n",
      "training loss: 0.6907293200492859\n",
      "validation loss: 0.6962699890136719\n",
      "training epoch nr 85\n",
      "training loss: 0.6906669735908508\n",
      "validation loss: 0.6970853805541992\n",
      "training epoch nr 86\n",
      "training loss: 0.690674364566803\n",
      "validation loss: 0.6960127949714661\n",
      "training epoch nr 87\n",
      "training loss: 0.6906374096870422\n",
      "validation loss: 0.6939622759819031\n",
      "training epoch nr 88\n",
      "training loss: 0.6906088590621948\n",
      "validation loss: 0.6945809125900269\n",
      "training epoch nr 89\n",
      "training loss: 0.6906248927116394\n",
      "validation loss: 0.6988946199417114\n",
      "training epoch nr 90\n",
      "training loss: 0.6905597448348999\n",
      "validation loss: 0.6985152363777161\n",
      "training epoch nr 91\n",
      "training loss: 0.6905258297920227\n",
      "validation loss: 0.6938652992248535\n",
      "training epoch nr 92\n",
      "training loss: 0.6905871033668518\n",
      "validation loss: 0.6929782032966614\n",
      "training epoch nr 93\n",
      "training loss: 0.6902697682380676\n",
      "validation loss: 0.6945121884346008\n",
      "training epoch nr 94\n",
      "training loss: 0.690373957157135\n",
      "validation loss: 0.696140706539154\n",
      "training epoch nr 95\n",
      "training loss: 0.69024658203125\n",
      "validation loss: 0.6991783380508423\n",
      "training epoch nr 96\n",
      "training loss: 0.6902889609336853\n",
      "validation loss: 0.691875696182251\n",
      "training epoch nr 97\n",
      "training loss: 0.6901296973228455\n",
      "validation loss: 0.6951783299446106\n",
      "training epoch nr 98\n",
      "training loss: 0.6902281045913696\n",
      "validation loss: 0.7026013731956482\n",
      "training epoch nr 99\n",
      "training loss: 0.6902133822441101\n",
      "validation loss: 0.696882426738739\n",
      "Training model nr 3...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932904124259949\n",
      "validation loss: 0.6914568543434143\n",
      "training epoch nr 1\n",
      "training loss: 0.6931889653205872\n",
      "validation loss: 0.6908109188079834\n",
      "training epoch nr 2\n",
      "training loss: 0.6931660175323486\n",
      "validation loss: 0.6942242980003357\n",
      "training epoch nr 3\n",
      "training loss: 0.6930544972419739\n",
      "validation loss: 0.6919876337051392\n",
      "training epoch nr 4\n",
      "training loss: 0.693050742149353\n",
      "validation loss: 0.6927380561828613\n",
      "training epoch nr 5\n",
      "training loss: 0.6931031942367554\n",
      "validation loss: 0.7073073387145996\n",
      "training epoch nr 6\n",
      "training loss: 0.6930257678031921\n",
      "validation loss: 0.6933014988899231\n",
      "training epoch nr 7\n",
      "training loss: 0.6929778456687927\n",
      "validation loss: 0.6931138038635254\n",
      "training epoch nr 8\n",
      "training loss: 0.6929575204849243\n",
      "validation loss: 0.6925554275512695\n",
      "training epoch nr 9\n",
      "training loss: 0.6929219365119934\n",
      "validation loss: 0.6932975053787231\n",
      "training epoch nr 10\n",
      "training loss: 0.6929162740707397\n",
      "validation loss: 0.6937817931175232\n",
      "training epoch nr 11\n",
      "training loss: 0.6928697824478149\n",
      "validation loss: 0.69363933801651\n",
      "training epoch nr 12\n",
      "training loss: 0.6928935647010803\n",
      "validation loss: 0.692870020866394\n",
      "training epoch nr 13\n",
      "training loss: 0.692882776260376\n",
      "validation loss: 0.6926039457321167\n",
      "training epoch nr 14\n",
      "training loss: 0.6928730010986328\n",
      "validation loss: 0.6932609677314758\n",
      "training epoch nr 15\n",
      "training loss: 0.6928291916847229\n",
      "validation loss: 0.6926265358924866\n",
      "training epoch nr 16\n",
      "training loss: 0.6927663087844849\n",
      "validation loss: 0.6993322372436523\n",
      "training epoch nr 17\n",
      "training loss: 0.6927854418754578\n",
      "validation loss: 0.6947411298751831\n",
      "training epoch nr 18\n",
      "training loss: 0.6928130984306335\n",
      "validation loss: 0.7059159278869629\n",
      "training epoch nr 19\n",
      "training loss: 0.6927414536476135\n",
      "validation loss: 0.6951448917388916\n",
      "training epoch nr 20\n",
      "training loss: 0.6926973462104797\n",
      "validation loss: 0.6921971440315247\n",
      "training epoch nr 21\n",
      "training loss: 0.6927091479301453\n",
      "validation loss: 0.6928362250328064\n",
      "training epoch nr 22\n",
      "training loss: 0.6925941109657288\n",
      "validation loss: 0.6972548961639404\n",
      "training epoch nr 23\n",
      "training loss: 0.6926184296607971\n",
      "validation loss: 0.6908082962036133\n",
      "training epoch nr 24\n",
      "training loss: 0.6926361918449402\n",
      "validation loss: 0.6917498111724854\n",
      "training epoch nr 25\n",
      "training loss: 0.6925603151321411\n",
      "validation loss: 0.6900405883789062\n",
      "training epoch nr 26\n",
      "training loss: 0.692546010017395\n",
      "validation loss: 0.6913712620735168\n",
      "training epoch nr 27\n",
      "training loss: 0.6924754977226257\n",
      "validation loss: 0.6962135434150696\n",
      "training epoch nr 28\n",
      "training loss: 0.69251549243927\n",
      "validation loss: 0.6932364106178284\n",
      "training epoch nr 29\n",
      "training loss: 0.6924950480461121\n",
      "validation loss: 0.6914089918136597\n",
      "training epoch nr 30\n",
      "training loss: 0.6924408674240112\n",
      "validation loss: 0.6954992413520813\n",
      "training epoch nr 31\n",
      "training loss: 0.6923867464065552\n",
      "validation loss: 0.6942375302314758\n",
      "training epoch nr 32\n",
      "training loss: 0.6924092769622803\n",
      "validation loss: 0.6908379197120667\n",
      "training epoch nr 33\n",
      "training loss: 0.6923841238021851\n",
      "validation loss: 0.6913117170333862\n",
      "training epoch nr 34\n",
      "training loss: 0.6923843026161194\n",
      "validation loss: 0.6992939710617065\n",
      "training epoch nr 35\n",
      "training loss: 0.6923773884773254\n",
      "validation loss: 0.6953303217887878\n",
      "training epoch nr 36\n",
      "training loss: 0.6922832131385803\n",
      "validation loss: 0.6931606531143188\n",
      "training epoch nr 37\n",
      "training loss: 0.6922154426574707\n",
      "validation loss: 0.6948551535606384\n",
      "training epoch nr 38\n",
      "training loss: 0.6922483444213867\n",
      "validation loss: 0.6968650817871094\n",
      "training epoch nr 39\n",
      "training loss: 0.6922698616981506\n",
      "validation loss: 0.6952881813049316\n",
      "training epoch nr 40\n",
      "training loss: 0.6922051310539246\n",
      "validation loss: 0.7042784094810486\n",
      "training epoch nr 41\n",
      "training loss: 0.6921446919441223\n",
      "validation loss: 0.6965896487236023\n",
      "training epoch nr 42\n",
      "training loss: 0.6921553611755371\n",
      "validation loss: 0.6939922571182251\n",
      "training epoch nr 43\n",
      "training loss: 0.6922023296356201\n",
      "validation loss: 0.6914883255958557\n",
      "training epoch nr 44\n",
      "training loss: 0.6920285820960999\n",
      "validation loss: 0.6956156492233276\n",
      "training epoch nr 45\n",
      "training loss: 0.6920218467712402\n",
      "validation loss: 0.6944040656089783\n",
      "training epoch nr 46\n",
      "training loss: 0.6920394897460938\n",
      "validation loss: 0.6918939352035522\n",
      "training epoch nr 47\n",
      "training loss: 0.6919739246368408\n",
      "validation loss: 0.6947702169418335\n",
      "training epoch nr 48\n",
      "training loss: 0.691990315914154\n",
      "validation loss: 0.6947543025016785\n",
      "training epoch nr 49\n",
      "training loss: 0.6919735670089722\n",
      "validation loss: 0.694881021976471\n",
      "training epoch nr 50\n",
      "training loss: 0.6919717788696289\n",
      "validation loss: 0.7067375183105469\n",
      "training epoch nr 51\n",
      "training loss: 0.6919245719909668\n",
      "validation loss: 0.6965832710266113\n",
      "training epoch nr 52\n",
      "training loss: 0.691835343837738\n",
      "validation loss: 0.6932336091995239\n",
      "training epoch nr 53\n",
      "training loss: 0.6919040083885193\n",
      "validation loss: 0.6922193765640259\n",
      "training epoch nr 54\n",
      "training loss: 0.6917971968650818\n",
      "validation loss: 0.6873867511749268\n",
      "training epoch nr 55\n",
      "training loss: 0.6918314695358276\n",
      "validation loss: 0.6976547241210938\n",
      "training epoch nr 56\n",
      "training loss: 0.6918380260467529\n",
      "validation loss: 0.697208046913147\n",
      "training epoch nr 57\n",
      "training loss: 0.6917677521705627\n",
      "validation loss: 0.695793867111206\n",
      "training epoch nr 58\n",
      "training loss: 0.691723108291626\n",
      "validation loss: 0.6933716535568237\n",
      "training epoch nr 59\n",
      "training loss: 0.6916368007659912\n",
      "validation loss: 0.692352831363678\n",
      "training epoch nr 60\n",
      "training loss: 0.6916443705558777\n",
      "validation loss: 0.695343017578125\n",
      "training epoch nr 61\n",
      "training loss: 0.6916481256484985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.7033535838127136\n",
      "training epoch nr 62\n",
      "training loss: 0.691527247428894\n",
      "validation loss: 0.698296308517456\n",
      "training epoch nr 63\n",
      "training loss: 0.6915127635002136\n",
      "validation loss: 0.6950126886367798\n",
      "training epoch nr 64\n",
      "training loss: 0.6915889978408813\n",
      "validation loss: 0.6942485570907593\n",
      "training epoch nr 65\n",
      "training loss: 0.6915349960327148\n",
      "validation loss: 0.6884159445762634\n",
      "training epoch nr 66\n",
      "training loss: 0.6915715336799622\n",
      "validation loss: 0.6961824297904968\n",
      "training epoch nr 67\n",
      "training loss: 0.6915377378463745\n",
      "validation loss: 0.6922828555107117\n",
      "training epoch nr 68\n",
      "training loss: 0.6914750337600708\n",
      "validation loss: 0.6967470049858093\n",
      "training epoch nr 69\n",
      "training loss: 0.6914718151092529\n",
      "validation loss: 0.6999849677085876\n",
      "training epoch nr 70\n",
      "training loss: 0.691487729549408\n",
      "validation loss: 0.6903036236763\n",
      "training epoch nr 71\n",
      "training loss: 0.6914499998092651\n",
      "validation loss: 0.6938517093658447\n",
      "training epoch nr 72\n",
      "training loss: 0.691392183303833\n",
      "validation loss: 0.6990366578102112\n",
      "training epoch nr 73\n",
      "training loss: 0.6913298964500427\n",
      "validation loss: 0.6936301589012146\n",
      "training epoch nr 74\n",
      "training loss: 0.6913190484046936\n",
      "validation loss: 0.6925215721130371\n",
      "training epoch nr 75\n",
      "training loss: 0.691310703754425\n",
      "validation loss: 0.6955767869949341\n",
      "training epoch nr 76\n",
      "training loss: 0.6913250088691711\n",
      "validation loss: 0.6919442415237427\n",
      "training epoch nr 77\n",
      "training loss: 0.6912482976913452\n",
      "validation loss: 0.693556547164917\n",
      "training epoch nr 78\n",
      "training loss: 0.691369354724884\n",
      "validation loss: 0.693110466003418\n",
      "training epoch nr 79\n",
      "training loss: 0.6912509799003601\n",
      "validation loss: 0.6944577097892761\n",
      "training epoch nr 80\n",
      "training loss: 0.6911883354187012\n",
      "validation loss: 0.6939883232116699\n",
      "training epoch nr 81\n",
      "training loss: 0.6911921501159668\n",
      "validation loss: 0.697161853313446\n",
      "training epoch nr 82\n",
      "training loss: 0.6911404132843018\n",
      "validation loss: 0.69879549741745\n",
      "training epoch nr 83\n",
      "training loss: 0.6911515593528748\n",
      "validation loss: 0.6910821199417114\n",
      "training epoch nr 84\n",
      "training loss: 0.6911475658416748\n",
      "validation loss: 0.6989721059799194\n",
      "training epoch nr 85\n",
      "training loss: 0.6911730170249939\n",
      "validation loss: 0.6903820037841797\n",
      "training epoch nr 86\n",
      "training loss: 0.6911541819572449\n",
      "validation loss: 0.6954002976417542\n",
      "training epoch nr 87\n",
      "training loss: 0.691063404083252\n",
      "validation loss: 0.6907138228416443\n",
      "training epoch nr 88\n",
      "training loss: 0.6910398006439209\n",
      "validation loss: 0.6929891705513\n",
      "training epoch nr 89\n",
      "training loss: 0.6908619999885559\n",
      "validation loss: 0.6942301392555237\n",
      "training epoch nr 90\n",
      "training loss: 0.691087007522583\n",
      "validation loss: 0.693854033946991\n",
      "training epoch nr 91\n",
      "training loss: 0.6908898949623108\n",
      "validation loss: 0.6972370147705078\n",
      "training epoch nr 92\n",
      "training loss: 0.6909204125404358\n",
      "validation loss: 0.7000459432601929\n",
      "training epoch nr 93\n",
      "training loss: 0.6907531023025513\n",
      "validation loss: 0.6944412589073181\n",
      "training epoch nr 94\n",
      "training loss: 0.6908000707626343\n",
      "validation loss: 0.6929201483726501\n",
      "training epoch nr 95\n",
      "training loss: 0.6909152269363403\n",
      "validation loss: 0.6961417198181152\n",
      "training epoch nr 96\n",
      "training loss: 0.6907714605331421\n",
      "validation loss: 0.6918423771858215\n",
      "training epoch nr 97\n",
      "training loss: 0.6908015012741089\n",
      "validation loss: 0.6976974606513977\n",
      "training epoch nr 98\n",
      "training loss: 0.690822958946228\n",
      "validation loss: 0.693892240524292\n",
      "training epoch nr 99\n",
      "training loss: 0.6907070875167847\n",
      "validation loss: 0.6938219666481018\n",
      "Training model nr 4...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933218836784363\n",
      "validation loss: 0.6907381415367126\n",
      "training epoch nr 1\n",
      "training loss: 0.6932026147842407\n",
      "validation loss: 0.6862543821334839\n",
      "training epoch nr 2\n",
      "training loss: 0.6931250691413879\n",
      "validation loss: 0.6935257911682129\n",
      "training epoch nr 3\n",
      "training loss: 0.6931469440460205\n",
      "validation loss: 0.6929482221603394\n",
      "training epoch nr 4\n",
      "training loss: 0.6931837797164917\n",
      "validation loss: 0.6933519840240479\n",
      "training epoch nr 5\n",
      "training loss: 0.6930607557296753\n",
      "validation loss: 0.6950396299362183\n",
      "training epoch nr 6\n",
      "training loss: 0.693072497844696\n",
      "validation loss: 0.6920177936553955\n",
      "training epoch nr 7\n",
      "training loss: 0.6930532455444336\n",
      "validation loss: 0.6918707489967346\n",
      "training epoch nr 8\n",
      "training loss: 0.6930568814277649\n",
      "validation loss: 0.7055348753929138\n",
      "training epoch nr 9\n",
      "training loss: 0.693070113658905\n",
      "validation loss: 0.69466233253479\n",
      "training epoch nr 10\n",
      "training loss: 0.6929834485054016\n",
      "validation loss: 0.6912434101104736\n",
      "training epoch nr 11\n",
      "training loss: 0.6929519176483154\n",
      "validation loss: 0.6921257972717285\n",
      "training epoch nr 12\n",
      "training loss: 0.6929497718811035\n",
      "validation loss: 0.6899497509002686\n",
      "training epoch nr 13\n",
      "training loss: 0.6929072737693787\n",
      "validation loss: 0.6915615200996399\n",
      "training epoch nr 14\n",
      "training loss: 0.6928418278694153\n",
      "validation loss: 0.6893167495727539\n",
      "training epoch nr 15\n",
      "training loss: 0.6928244233131409\n",
      "validation loss: 0.6924003958702087\n",
      "training epoch nr 16\n",
      "training loss: 0.6928130984306335\n",
      "validation loss: 0.6947606205940247\n",
      "training epoch nr 17\n",
      "training loss: 0.6926887035369873\n",
      "validation loss: 0.6963856220245361\n",
      "training epoch nr 18\n",
      "training loss: 0.6927220225334167\n",
      "validation loss: 0.6939797401428223\n",
      "training epoch nr 19\n",
      "training loss: 0.6926974058151245\n",
      "validation loss: 0.6917330026626587\n",
      "training epoch nr 20\n",
      "training loss: 0.6926219463348389\n",
      "validation loss: 0.6926218271255493\n",
      "training epoch nr 21\n",
      "training loss: 0.6925980448722839\n",
      "validation loss: 0.6904653906822205\n",
      "training epoch nr 22\n",
      "training loss: 0.6925277709960938\n",
      "validation loss: 0.6914811134338379\n",
      "training epoch nr 23\n",
      "training loss: 0.6925157904624939\n",
      "validation loss: 0.6928690671920776\n",
      "training epoch nr 24\n",
      "training loss: 0.6924231052398682\n",
      "validation loss: 0.6922340393066406\n",
      "training epoch nr 25\n",
      "training loss: 0.6923214793205261\n",
      "validation loss: 0.6930368542671204\n",
      "training epoch nr 26\n",
      "training loss: 0.6923772096633911\n",
      "validation loss: 0.687105655670166\n",
      "training epoch nr 27\n",
      "training loss: 0.6923976540565491\n",
      "validation loss: 0.6956297755241394\n",
      "training epoch nr 28\n",
      "training loss: 0.6923550963401794\n",
      "validation loss: 0.6883149147033691\n",
      "training epoch nr 29\n",
      "training loss: 0.6922619342803955\n",
      "validation loss: 0.6890788674354553\n",
      "training epoch nr 30\n",
      "training loss: 0.6921735405921936\n",
      "validation loss: 0.6917811632156372\n",
      "training epoch nr 31\n",
      "training loss: 0.6922040581703186\n",
      "validation loss: 0.696733832359314\n",
      "training epoch nr 32\n",
      "training loss: 0.6921612620353699\n",
      "validation loss: 0.6914879083633423\n",
      "training epoch nr 33\n",
      "training loss: 0.6921201348304749\n",
      "validation loss: 0.6903294324874878\n",
      "training epoch nr 34\n",
      "training loss: 0.6920953989028931\n",
      "validation loss: 0.6906027793884277\n",
      "training epoch nr 35\n",
      "training loss: 0.6920217275619507\n",
      "validation loss: 0.6917673945426941\n",
      "training epoch nr 36\n",
      "training loss: 0.6920406818389893\n",
      "validation loss: 0.6891359090805054\n",
      "training epoch nr 37\n",
      "training loss: 0.6919834613800049\n",
      "validation loss: 0.6893972158432007\n",
      "training epoch nr 38\n",
      "training loss: 0.6920284032821655\n",
      "validation loss: 0.6925326585769653\n",
      "training epoch nr 39\n",
      "training loss: 0.6918888688087463\n",
      "validation loss: 0.6919431686401367\n",
      "training epoch nr 40\n",
      "training loss: 0.6918097734451294\n",
      "validation loss: 0.6962721943855286\n",
      "training epoch nr 41\n",
      "training loss: 0.6918790340423584\n",
      "validation loss: 0.693912148475647\n",
      "training epoch nr 42\n",
      "training loss: 0.6917471885681152\n",
      "validation loss: 0.6911689639091492\n",
      "training epoch nr 43\n",
      "training loss: 0.691728949546814\n",
      "validation loss: 0.689677357673645\n",
      "training epoch nr 44\n",
      "training loss: 0.6917765736579895\n",
      "validation loss: 0.6881698966026306\n",
      "training epoch nr 45\n",
      "training loss: 0.6916850209236145\n",
      "validation loss: 0.6977695822715759\n",
      "training epoch nr 46\n",
      "training loss: 0.6916446089744568\n",
      "validation loss: 0.6923494935035706\n",
      "training epoch nr 47\n",
      "training loss: 0.6915425658226013\n",
      "validation loss: 0.6898216605186462\n",
      "training epoch nr 48\n",
      "training loss: 0.6915215253829956\n",
      "validation loss: 0.6942216753959656\n",
      "training epoch nr 49\n",
      "training loss: 0.6913835406303406\n",
      "validation loss: 0.6962212920188904\n",
      "training epoch nr 50\n",
      "training loss: 0.6914151310920715\n",
      "validation loss: 0.6923553943634033\n",
      "training epoch nr 51\n",
      "training loss: 0.6913671493530273\n",
      "validation loss: 0.6936817169189453\n",
      "training epoch nr 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6913809180259705\n",
      "validation loss: 0.689378023147583\n",
      "training epoch nr 53\n",
      "training loss: 0.6913110613822937\n",
      "validation loss: 0.6956019401550293\n",
      "training epoch nr 54\n",
      "training loss: 0.6913222074508667\n",
      "validation loss: 0.6894998550415039\n",
      "training epoch nr 55\n",
      "training loss: 0.691218912601471\n",
      "validation loss: 0.693843424320221\n",
      "training epoch nr 56\n",
      "training loss: 0.6912829279899597\n",
      "validation loss: 0.6857420206069946\n",
      "training epoch nr 57\n",
      "training loss: 0.6911927461624146\n",
      "validation loss: 0.6889767050743103\n",
      "training epoch nr 58\n",
      "training loss: 0.6911465525627136\n",
      "validation loss: 0.6875969767570496\n",
      "training epoch nr 59\n",
      "training loss: 0.6910190582275391\n",
      "validation loss: 0.6955615878105164\n",
      "training epoch nr 60\n",
      "training loss: 0.6911520957946777\n",
      "validation loss: 0.6932112574577332\n",
      "training epoch nr 61\n",
      "training loss: 0.6911537647247314\n",
      "validation loss: 0.692004382610321\n",
      "training epoch nr 62\n",
      "training loss: 0.6910246014595032\n",
      "validation loss: 0.6983087658882141\n",
      "training epoch nr 63\n",
      "training loss: 0.6909527778625488\n",
      "validation loss: 0.6836228966712952\n",
      "training epoch nr 64\n",
      "training loss: 0.6908865571022034\n",
      "validation loss: 0.6922520399093628\n",
      "training epoch nr 65\n",
      "training loss: 0.6909511685371399\n",
      "validation loss: 0.7042791247367859\n",
      "training epoch nr 66\n",
      "training loss: 0.6907725930213928\n",
      "validation loss: 0.6855425238609314\n",
      "training epoch nr 67\n",
      "training loss: 0.6908090710639954\n",
      "validation loss: 0.695551872253418\n",
      "training epoch nr 68\n",
      "training loss: 0.6907333731651306\n",
      "validation loss: 0.6973986029624939\n",
      "training epoch nr 69\n",
      "training loss: 0.6907134652137756\n",
      "validation loss: 0.6927663683891296\n",
      "training epoch nr 70\n",
      "training loss: 0.6905899047851562\n",
      "validation loss: 0.6915828585624695\n",
      "training epoch nr 71\n",
      "training loss: 0.6906809210777283\n",
      "validation loss: 0.6981419324874878\n",
      "training epoch nr 72\n",
      "training loss: 0.6906464099884033\n",
      "validation loss: 0.7003961801528931\n",
      "training epoch nr 73\n",
      "training loss: 0.6905480027198792\n",
      "validation loss: 0.7034887075424194\n",
      "training epoch nr 74\n",
      "training loss: 0.6905575394630432\n",
      "validation loss: 0.6996908783912659\n",
      "training epoch nr 75\n",
      "training loss: 0.6905156970024109\n",
      "validation loss: 0.6908328533172607\n",
      "training epoch nr 76\n",
      "training loss: 0.6905392408370972\n",
      "validation loss: 0.6890213489532471\n",
      "training epoch nr 77\n",
      "training loss: 0.6904425024986267\n",
      "validation loss: 0.6840094923973083\n",
      "training epoch nr 78\n",
      "training loss: 0.6902798414230347\n",
      "validation loss: 0.6957005262374878\n",
      "training epoch nr 79\n",
      "training loss: 0.6903613209724426\n",
      "validation loss: 0.6903495788574219\n",
      "training epoch nr 80\n",
      "training loss: 0.6902914643287659\n",
      "validation loss: 0.6894552707672119\n",
      "training epoch nr 81\n",
      "training loss: 0.6903534531593323\n",
      "validation loss: 0.6849751472473145\n",
      "training epoch nr 82\n",
      "training loss: 0.6902405023574829\n",
      "validation loss: 0.7003824710845947\n",
      "training epoch nr 83\n",
      "training loss: 0.6902370452880859\n",
      "validation loss: 0.6869696378707886\n",
      "training epoch nr 84\n",
      "training loss: 0.6901737451553345\n",
      "validation loss: 0.6855481266975403\n",
      "training epoch nr 85\n",
      "training loss: 0.6900895833969116\n",
      "validation loss: 0.6920145153999329\n",
      "training epoch nr 86\n",
      "training loss: 0.6899615526199341\n",
      "validation loss: 0.6902386546134949\n",
      "training epoch nr 87\n",
      "training loss: 0.6898989677429199\n",
      "validation loss: 0.6985634565353394\n",
      "training epoch nr 88\n",
      "training loss: 0.6899685263633728\n",
      "validation loss: 0.6946207284927368\n",
      "training epoch nr 89\n",
      "training loss: 0.6900972723960876\n",
      "validation loss: 0.6850485801696777\n",
      "training epoch nr 90\n",
      "training loss: 0.6900379657745361\n",
      "validation loss: 0.6960473656654358\n",
      "training epoch nr 91\n",
      "training loss: 0.6897156238555908\n",
      "validation loss: 0.6959505081176758\n",
      "training epoch nr 92\n",
      "training loss: 0.6898419857025146\n",
      "validation loss: 0.6932564377784729\n",
      "training epoch nr 93\n",
      "training loss: 0.6897876858711243\n",
      "validation loss: 0.6893914937973022\n",
      "training epoch nr 94\n",
      "training loss: 0.6898265480995178\n",
      "validation loss: 0.6970874071121216\n",
      "training epoch nr 95\n",
      "training loss: 0.6896120309829712\n",
      "validation loss: 0.6909001469612122\n",
      "training epoch nr 96\n",
      "training loss: 0.6897768974304199\n",
      "validation loss: 0.6845892071723938\n",
      "training epoch nr 97\n",
      "training loss: 0.6895895004272461\n",
      "validation loss: 0.6932498216629028\n",
      "training epoch nr 98\n",
      "training loss: 0.689600944519043\n",
      "validation loss: 0.6964616775512695\n",
      "training epoch nr 99\n",
      "training loss: 0.6896141767501831\n",
      "validation loss: 0.691845178604126\n",
      "Training model nr 5...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933691501617432\n",
      "validation loss: 0.6852866411209106\n",
      "training epoch nr 1\n",
      "training loss: 0.693115770816803\n",
      "validation loss: 0.6911627054214478\n",
      "training epoch nr 2\n",
      "training loss: 0.6931424736976624\n",
      "validation loss: 0.6930769681930542\n",
      "training epoch nr 3\n",
      "training loss: 0.6931155920028687\n",
      "validation loss: 0.6939714550971985\n",
      "training epoch nr 4\n",
      "training loss: 0.6930967569351196\n",
      "validation loss: 0.6921029090881348\n",
      "training epoch nr 5\n",
      "training loss: 0.6930042505264282\n",
      "validation loss: 0.691815197467804\n",
      "training epoch nr 6\n",
      "training loss: 0.6930328011512756\n",
      "validation loss: 0.6920638680458069\n",
      "training epoch nr 7\n",
      "training loss: 0.6929767727851868\n",
      "validation loss: 0.6909705996513367\n",
      "training epoch nr 8\n",
      "training loss: 0.6929380297660828\n",
      "validation loss: 0.6932697296142578\n",
      "training epoch nr 9\n",
      "training loss: 0.6929046511650085\n",
      "validation loss: 0.6910839676856995\n",
      "training epoch nr 10\n",
      "training loss: 0.6928669214248657\n",
      "validation loss: 0.6903711557388306\n",
      "training epoch nr 11\n",
      "training loss: 0.6928871273994446\n",
      "validation loss: 0.691605269908905\n",
      "training epoch nr 12\n",
      "training loss: 0.6928615570068359\n",
      "validation loss: 0.697291910648346\n",
      "training epoch nr 13\n",
      "training loss: 0.6928722858428955\n",
      "validation loss: 0.6905304193496704\n",
      "training epoch nr 14\n",
      "training loss: 0.6927897930145264\n",
      "validation loss: 0.6927129030227661\n",
      "training epoch nr 15\n",
      "training loss: 0.6928130984306335\n",
      "validation loss: 0.6912712454795837\n",
      "training epoch nr 16\n",
      "training loss: 0.6927312612533569\n",
      "validation loss: 0.6914622187614441\n",
      "training epoch nr 17\n",
      "training loss: 0.6927137970924377\n",
      "validation loss: 0.6927617192268372\n",
      "training epoch nr 18\n",
      "training loss: 0.6926835775375366\n",
      "validation loss: 0.6893444657325745\n",
      "training epoch nr 19\n",
      "training loss: 0.6926274299621582\n",
      "validation loss: 0.6934259533882141\n",
      "training epoch nr 20\n",
      "training loss: 0.6925845742225647\n",
      "validation loss: 0.6940135955810547\n",
      "training epoch nr 21\n",
      "training loss: 0.6925089359283447\n",
      "validation loss: 0.6936842203140259\n",
      "training epoch nr 22\n",
      "training loss: 0.6925322413444519\n",
      "validation loss: 0.6954005360603333\n",
      "training epoch nr 23\n",
      "training loss: 0.6924972534179688\n",
      "validation loss: 0.6937049627304077\n",
      "training epoch nr 24\n",
      "training loss: 0.69252610206604\n",
      "validation loss: 0.692640483379364\n",
      "training epoch nr 25\n",
      "training loss: 0.6924686431884766\n",
      "validation loss: 0.6936414241790771\n",
      "training epoch nr 26\n",
      "training loss: 0.6924427151679993\n",
      "validation loss: 0.7184024453163147\n",
      "training epoch nr 27\n",
      "training loss: 0.6923803091049194\n",
      "validation loss: 0.695067822933197\n",
      "training epoch nr 28\n",
      "training loss: 0.6923208236694336\n",
      "validation loss: 0.6978638768196106\n",
      "training epoch nr 29\n",
      "training loss: 0.6923685073852539\n",
      "validation loss: 0.6938595175743103\n",
      "training epoch nr 30\n",
      "training loss: 0.6922630071640015\n",
      "validation loss: 0.6971743702888489\n",
      "training epoch nr 31\n",
      "training loss: 0.6922329664230347\n",
      "validation loss: 0.6941185593605042\n",
      "training epoch nr 32\n",
      "training loss: 0.6922615766525269\n",
      "validation loss: 0.695796549320221\n",
      "training epoch nr 33\n",
      "training loss: 0.6921883225440979\n",
      "validation loss: 0.6936529278755188\n",
      "training epoch nr 34\n",
      "training loss: 0.6921172142028809\n",
      "validation loss: 0.6944369077682495\n",
      "training epoch nr 35\n",
      "training loss: 0.6921420693397522\n",
      "validation loss: 0.6962954998016357\n",
      "training epoch nr 36\n",
      "training loss: 0.6921602487564087\n",
      "validation loss: 0.6994214653968811\n",
      "training epoch nr 37\n",
      "training loss: 0.6921550035476685\n",
      "validation loss: 0.6944383382797241\n",
      "training epoch nr 38\n",
      "training loss: 0.6920806169509888\n",
      "validation loss: 0.6960263848304749\n",
      "training epoch nr 39\n",
      "training loss: 0.6920135021209717\n",
      "validation loss: 0.703787088394165\n",
      "training epoch nr 40\n",
      "training loss: 0.6919436454772949\n",
      "validation loss: 0.6936877369880676\n",
      "training epoch nr 41\n",
      "training loss: 0.6918822526931763\n",
      "validation loss: 0.6929888129234314\n",
      "training epoch nr 42\n",
      "training loss: 0.6918679475784302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.697100043296814\n",
      "training epoch nr 43\n",
      "training loss: 0.6917247176170349\n",
      "validation loss: 0.6942140460014343\n",
      "training epoch nr 44\n",
      "training loss: 0.6917914748191833\n",
      "validation loss: 0.6973629593849182\n",
      "training epoch nr 45\n",
      "training loss: 0.6917471885681152\n",
      "validation loss: 0.7025663256645203\n",
      "training epoch nr 46\n",
      "training loss: 0.6917513012886047\n",
      "validation loss: 0.6977724432945251\n",
      "training epoch nr 47\n",
      "training loss: 0.6916401982307434\n",
      "validation loss: 0.696074366569519\n",
      "training epoch nr 48\n",
      "training loss: 0.6916109919548035\n",
      "validation loss: 0.6980382204055786\n",
      "training epoch nr 49\n",
      "training loss: 0.6915914416313171\n",
      "validation loss: 0.6949172616004944\n",
      "training epoch nr 50\n",
      "training loss: 0.6915772557258606\n",
      "validation loss: 0.6952934861183167\n",
      "training epoch nr 51\n",
      "training loss: 0.6915083527565002\n",
      "validation loss: 0.7010878324508667\n",
      "training epoch nr 52\n",
      "training loss: 0.6914380192756653\n",
      "validation loss: 0.7014341950416565\n",
      "training epoch nr 53\n",
      "training loss: 0.6913827657699585\n",
      "validation loss: 0.6934650540351868\n",
      "training epoch nr 54\n",
      "training loss: 0.6912415623664856\n",
      "validation loss: 0.6946265697479248\n",
      "training epoch nr 55\n",
      "training loss: 0.6912539601325989\n",
      "validation loss: 0.702591061592102\n",
      "training epoch nr 56\n",
      "training loss: 0.6912358403205872\n",
      "validation loss: 0.6998858451843262\n",
      "training epoch nr 57\n",
      "training loss: 0.6912362575531006\n",
      "validation loss: 0.6961967945098877\n",
      "training epoch nr 58\n",
      "training loss: 0.691127598285675\n",
      "validation loss: 0.6964067220687866\n",
      "training epoch nr 59\n",
      "training loss: 0.6911641359329224\n",
      "validation loss: 0.6927044987678528\n",
      "training epoch nr 60\n",
      "training loss: 0.6911173462867737\n",
      "validation loss: 0.6980635523796082\n",
      "training epoch nr 61\n",
      "training loss: 0.6910688877105713\n",
      "validation loss: 0.6973285675048828\n",
      "training epoch nr 62\n",
      "training loss: 0.6909708976745605\n",
      "validation loss: 0.696048378944397\n",
      "training epoch nr 63\n",
      "training loss: 0.690944492816925\n",
      "validation loss: 0.6960471272468567\n",
      "training epoch nr 64\n",
      "training loss: 0.6908789873123169\n",
      "validation loss: 0.6994448900222778\n",
      "training epoch nr 65\n",
      "training loss: 0.690883219242096\n",
      "validation loss: 0.6946674585342407\n",
      "training epoch nr 66\n",
      "training loss: 0.6908268332481384\n",
      "validation loss: 0.6970769762992859\n",
      "training epoch nr 67\n",
      "training loss: 0.6907255053520203\n",
      "validation loss: 0.6924847364425659\n",
      "training epoch nr 68\n",
      "training loss: 0.6906561851501465\n",
      "validation loss: 0.6935886144638062\n",
      "training epoch nr 69\n",
      "training loss: 0.6905442476272583\n",
      "validation loss: 0.7002004981040955\n",
      "training epoch nr 70\n",
      "training loss: 0.6906880140304565\n",
      "validation loss: 0.7008549571037292\n",
      "training epoch nr 71\n",
      "training loss: 0.6905295252799988\n",
      "validation loss: 0.6980520486831665\n",
      "training epoch nr 72\n",
      "training loss: 0.6904865503311157\n",
      "validation loss: 0.6992705464363098\n",
      "training epoch nr 73\n",
      "training loss: 0.6904990673065186\n",
      "validation loss: 0.7020906805992126\n",
      "training epoch nr 74\n",
      "training loss: 0.6904624104499817\n",
      "validation loss: 0.6915684342384338\n",
      "training epoch nr 75\n",
      "training loss: 0.6902946829795837\n",
      "validation loss: 0.6929752826690674\n",
      "training epoch nr 76\n",
      "training loss: 0.6903164386749268\n",
      "validation loss: 0.7015984058380127\n",
      "training epoch nr 77\n",
      "training loss: 0.690207839012146\n",
      "validation loss: 0.6939095258712769\n",
      "training epoch nr 78\n",
      "training loss: 0.6902937293052673\n",
      "validation loss: 0.7027140259742737\n",
      "training epoch nr 79\n",
      "training loss: 0.6902791857719421\n",
      "validation loss: 0.7005080580711365\n",
      "training epoch nr 80\n",
      "training loss: 0.6901127099990845\n",
      "validation loss: 0.695730447769165\n",
      "training epoch nr 81\n",
      "training loss: 0.6900566220283508\n",
      "validation loss: 0.7110066413879395\n",
      "training epoch nr 82\n",
      "training loss: 0.6899214386940002\n",
      "validation loss: 0.7031975984573364\n",
      "training epoch nr 83\n",
      "training loss: 0.6900372505187988\n",
      "validation loss: 0.708682119846344\n",
      "training epoch nr 84\n",
      "training loss: 0.6900076866149902\n",
      "validation loss: 0.6970097422599792\n",
      "training epoch nr 85\n",
      "training loss: 0.6898763179779053\n",
      "validation loss: 0.7048879265785217\n",
      "training epoch nr 86\n",
      "training loss: 0.6899577379226685\n",
      "validation loss: 0.6950176954269409\n",
      "training epoch nr 87\n",
      "training loss: 0.6898152232170105\n",
      "validation loss: 0.6981505751609802\n",
      "training epoch nr 88\n",
      "training loss: 0.6899324655532837\n",
      "validation loss: 0.696406900882721\n",
      "training epoch nr 89\n",
      "training loss: 0.6897993087768555\n",
      "validation loss: 0.7108652591705322\n",
      "training epoch nr 90\n",
      "training loss: 0.6896643042564392\n",
      "validation loss: 0.6987069845199585\n",
      "training epoch nr 91\n",
      "training loss: 0.6896602511405945\n",
      "validation loss: 0.6946466565132141\n",
      "training epoch nr 92\n",
      "training loss: 0.6895053386688232\n",
      "validation loss: 0.6991328597068787\n",
      "training epoch nr 93\n",
      "training loss: 0.6895936131477356\n",
      "validation loss: 0.6979542374610901\n",
      "training epoch nr 94\n",
      "training loss: 0.6895263195037842\n",
      "validation loss: 0.6987666487693787\n",
      "training epoch nr 95\n",
      "training loss: 0.6894717812538147\n",
      "validation loss: 0.7018911242485046\n",
      "training epoch nr 96\n",
      "training loss: 0.6893167495727539\n",
      "validation loss: 0.6925491690635681\n",
      "training epoch nr 97\n",
      "training loss: 0.6893643140792847\n",
      "validation loss: 0.6981028318405151\n",
      "training epoch nr 98\n",
      "training loss: 0.689240038394928\n",
      "validation loss: 0.6990976929664612\n",
      "training epoch nr 99\n",
      "training loss: 0.6892682313919067\n",
      "validation loss: 0.6947477459907532\n",
      "Training model nr 6...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933028697967529\n",
      "validation loss: 0.6932409405708313\n",
      "training epoch nr 1\n",
      "training loss: 0.6931774616241455\n",
      "validation loss: 0.6923108696937561\n",
      "training epoch nr 2\n",
      "training loss: 0.6931635141372681\n",
      "validation loss: 0.6912054419517517\n",
      "training epoch nr 3\n",
      "training loss: 0.6931220293045044\n",
      "validation loss: 0.6969001293182373\n",
      "training epoch nr 4\n",
      "training loss: 0.6930551528930664\n",
      "validation loss: 0.6941403150558472\n",
      "training epoch nr 5\n",
      "training loss: 0.6930637359619141\n",
      "validation loss: 0.6940015554428101\n",
      "training epoch nr 6\n",
      "training loss: 0.6930543184280396\n",
      "validation loss: 0.6940206289291382\n",
      "training epoch nr 7\n",
      "training loss: 0.6929568648338318\n",
      "validation loss: 0.6941041350364685\n",
      "training epoch nr 8\n",
      "training loss: 0.6929460167884827\n",
      "validation loss: 0.6920235753059387\n",
      "training epoch nr 9\n",
      "training loss: 0.6928686499595642\n",
      "validation loss: 0.6975261569023132\n",
      "training epoch nr 10\n",
      "training loss: 0.6929295063018799\n",
      "validation loss: 0.6905434131622314\n",
      "training epoch nr 11\n",
      "training loss: 0.6928077340126038\n",
      "validation loss: 0.6929484009742737\n",
      "training epoch nr 12\n",
      "training loss: 0.6928008794784546\n",
      "validation loss: 0.6932880878448486\n",
      "training epoch nr 13\n",
      "training loss: 0.6927610039710999\n",
      "validation loss: 0.6935094594955444\n",
      "training epoch nr 14\n",
      "training loss: 0.6927836537361145\n",
      "validation loss: 0.690548837184906\n",
      "training epoch nr 15\n",
      "training loss: 0.6926414966583252\n",
      "validation loss: 0.6915718913078308\n",
      "training epoch nr 16\n",
      "training loss: 0.6926358938217163\n",
      "validation loss: 0.6895172595977783\n",
      "training epoch nr 17\n",
      "training loss: 0.6925691366195679\n",
      "validation loss: 0.6910686492919922\n",
      "training epoch nr 18\n",
      "training loss: 0.6925092935562134\n",
      "validation loss: 0.6875672936439514\n",
      "training epoch nr 19\n",
      "training loss: 0.6925073862075806\n",
      "validation loss: 0.6857531666755676\n",
      "training epoch nr 20\n",
      "training loss: 0.6925747394561768\n",
      "validation loss: 0.6886348128318787\n",
      "training epoch nr 21\n",
      "training loss: 0.6925007700920105\n",
      "validation loss: 0.6934061050415039\n",
      "training epoch nr 22\n",
      "training loss: 0.6924324631690979\n",
      "validation loss: 0.6870898604393005\n",
      "training epoch nr 23\n",
      "training loss: 0.6923661828041077\n",
      "validation loss: 0.6926620602607727\n",
      "training epoch nr 24\n",
      "training loss: 0.692237138748169\n",
      "validation loss: 0.7085439562797546\n",
      "training epoch nr 25\n",
      "training loss: 0.6923101544380188\n",
      "validation loss: 0.6932595372200012\n",
      "training epoch nr 26\n",
      "training loss: 0.6922230124473572\n",
      "validation loss: 0.6902691721916199\n",
      "training epoch nr 27\n",
      "training loss: 0.6922736763954163\n",
      "validation loss: 0.7035917043685913\n",
      "training epoch nr 28\n",
      "training loss: 0.6922795176506042\n",
      "validation loss: 0.6932638883590698\n",
      "training epoch nr 29\n",
      "training loss: 0.6921854019165039\n",
      "validation loss: 0.7005861401557922\n",
      "training epoch nr 30\n",
      "training loss: 0.6920611262321472\n",
      "validation loss: 0.6948769688606262\n",
      "training epoch nr 31\n",
      "training loss: 0.6920784711837769\n",
      "validation loss: 0.6904878616333008\n",
      "training epoch nr 32\n",
      "training loss: 0.6920368671417236\n",
      "validation loss: 0.6963425278663635\n",
      "training epoch nr 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6919915080070496\n",
      "validation loss: 0.6957857012748718\n",
      "training epoch nr 34\n",
      "training loss: 0.6919534206390381\n",
      "validation loss: 0.6960039734840393\n",
      "training epoch nr 35\n",
      "training loss: 0.6919654607772827\n",
      "validation loss: 0.6958020925521851\n",
      "training epoch nr 36\n",
      "training loss: 0.6918777227401733\n",
      "validation loss: 0.6965941786766052\n",
      "training epoch nr 37\n",
      "training loss: 0.6918999552726746\n",
      "validation loss: 0.6975207924842834\n",
      "training epoch nr 38\n",
      "training loss: 0.6918730139732361\n",
      "validation loss: 0.6922696232795715\n",
      "training epoch nr 39\n",
      "training loss: 0.691680908203125\n",
      "validation loss: 0.6889004111289978\n",
      "training epoch nr 40\n",
      "training loss: 0.6915870904922485\n",
      "validation loss: 0.7025080919265747\n",
      "training epoch nr 41\n",
      "training loss: 0.6915934085845947\n",
      "validation loss: 0.7026206254959106\n",
      "training epoch nr 42\n",
      "training loss: 0.6916223764419556\n",
      "validation loss: 0.6989195942878723\n",
      "training epoch nr 43\n",
      "training loss: 0.6915925741195679\n",
      "validation loss: 0.6934776306152344\n",
      "training epoch nr 44\n",
      "training loss: 0.6914536952972412\n",
      "validation loss: 0.7024024724960327\n",
      "training epoch nr 45\n",
      "training loss: 0.691438615322113\n",
      "validation loss: 0.6934425234794617\n",
      "training epoch nr 46\n",
      "training loss: 0.6914539933204651\n",
      "validation loss: 0.6991987824440002\n",
      "training epoch nr 47\n",
      "training loss: 0.691260576248169\n",
      "validation loss: 0.7010963559150696\n",
      "training epoch nr 48\n",
      "training loss: 0.6913046836853027\n",
      "validation loss: 0.7042584419250488\n",
      "training epoch nr 49\n",
      "training loss: 0.6912142038345337\n",
      "validation loss: 0.6924696564674377\n",
      "training epoch nr 50\n",
      "training loss: 0.691253662109375\n",
      "validation loss: 0.6914141178131104\n",
      "training epoch nr 51\n",
      "training loss: 0.6910585761070251\n",
      "validation loss: 0.695857048034668\n",
      "training epoch nr 52\n",
      "training loss: 0.6911740899085999\n",
      "validation loss: 0.7063876986503601\n",
      "training epoch nr 53\n",
      "training loss: 0.6911486983299255\n",
      "validation loss: 0.6941465139389038\n",
      "training epoch nr 54\n",
      "training loss: 0.6909275054931641\n",
      "validation loss: 0.6988909840583801\n",
      "training epoch nr 55\n",
      "training loss: 0.6909011006355286\n",
      "validation loss: 0.6972731351852417\n",
      "training epoch nr 56\n",
      "training loss: 0.6908608675003052\n",
      "validation loss: 0.7049480080604553\n",
      "training epoch nr 57\n",
      "training loss: 0.6909642815589905\n",
      "validation loss: 0.6954248547554016\n",
      "training epoch nr 58\n",
      "training loss: 0.6908093690872192\n",
      "validation loss: 0.6969890594482422\n",
      "training epoch nr 59\n",
      "training loss: 0.6907362937927246\n",
      "validation loss: 0.68929123878479\n",
      "training epoch nr 60\n",
      "training loss: 0.690821647644043\n",
      "validation loss: 0.6889306306838989\n",
      "training epoch nr 61\n",
      "training loss: 0.6907615661621094\n",
      "validation loss: 0.697852611541748\n",
      "training epoch nr 62\n",
      "training loss: 0.6906295418739319\n",
      "validation loss: 0.6948364973068237\n",
      "training epoch nr 63\n",
      "training loss: 0.6905597448348999\n",
      "validation loss: 0.6967452764511108\n",
      "training epoch nr 64\n",
      "training loss: 0.6905380487442017\n",
      "validation loss: 0.7013534307479858\n",
      "training epoch nr 65\n",
      "training loss: 0.6904878616333008\n",
      "validation loss: 0.6912326216697693\n",
      "training epoch nr 66\n",
      "training loss: 0.6904696226119995\n",
      "validation loss: 0.6950470805168152\n",
      "training epoch nr 67\n",
      "training loss: 0.6903722286224365\n",
      "validation loss: 0.7012232542037964\n",
      "training epoch nr 68\n",
      "training loss: 0.6902987360954285\n",
      "validation loss: 0.6902142763137817\n",
      "training epoch nr 69\n",
      "training loss: 0.6903629302978516\n",
      "validation loss: 0.69695645570755\n",
      "training epoch nr 70\n",
      "training loss: 0.6901705265045166\n",
      "validation loss: 0.6906428933143616\n",
      "training epoch nr 71\n",
      "training loss: 0.6902225613594055\n",
      "validation loss: 0.7034850120544434\n",
      "training epoch nr 72\n",
      "training loss: 0.6902185082435608\n",
      "validation loss: 0.6995474100112915\n",
      "training epoch nr 73\n",
      "training loss: 0.6901534795761108\n",
      "validation loss: 0.6848503947257996\n",
      "training epoch nr 74\n",
      "training loss: 0.6901878118515015\n",
      "validation loss: 0.6951195001602173\n",
      "training epoch nr 75\n",
      "training loss: 0.6900493502616882\n",
      "validation loss: 0.701529860496521\n",
      "training epoch nr 76\n",
      "training loss: 0.6900395154953003\n",
      "validation loss: 0.7070828080177307\n",
      "training epoch nr 77\n",
      "training loss: 0.6898724436759949\n",
      "validation loss: 0.7132868766784668\n",
      "training epoch nr 78\n",
      "training loss: 0.6899067163467407\n",
      "validation loss: 0.6997551918029785\n",
      "training epoch nr 79\n",
      "training loss: 0.6898788213729858\n",
      "validation loss: 0.7002940773963928\n",
      "training epoch nr 80\n",
      "training loss: 0.6897674202919006\n",
      "validation loss: 0.6955788731575012\n",
      "training epoch nr 81\n",
      "training loss: 0.6897463798522949\n",
      "validation loss: 0.6950109601020813\n",
      "training epoch nr 82\n",
      "training loss: 0.689868152141571\n",
      "validation loss: 0.6993470191955566\n",
      "training epoch nr 83\n",
      "training loss: 0.6897562742233276\n",
      "validation loss: 0.7002109885215759\n",
      "training epoch nr 84\n",
      "training loss: 0.6895007491111755\n",
      "validation loss: 0.6912975311279297\n",
      "training epoch nr 85\n",
      "training loss: 0.6896786689758301\n",
      "validation loss: 0.6919671297073364\n",
      "training epoch nr 86\n",
      "training loss: 0.6895126104354858\n",
      "validation loss: 0.7000104188919067\n",
      "training epoch nr 87\n",
      "training loss: 0.6895397901535034\n",
      "validation loss: 0.7035674452781677\n",
      "training epoch nr 88\n",
      "training loss: 0.689449667930603\n",
      "validation loss: 0.6935003399848938\n",
      "training epoch nr 89\n",
      "training loss: 0.689471960067749\n",
      "validation loss: 0.6903053522109985\n",
      "training epoch nr 90\n",
      "training loss: 0.6893674731254578\n",
      "validation loss: 0.6986905932426453\n",
      "training epoch nr 91\n",
      "training loss: 0.6893593668937683\n",
      "validation loss: 0.7041642069816589\n",
      "training epoch nr 92\n",
      "training loss: 0.6894047260284424\n",
      "validation loss: 0.699529767036438\n",
      "training epoch nr 93\n",
      "training loss: 0.6892451047897339\n",
      "validation loss: 0.6957798600196838\n",
      "training epoch nr 94\n",
      "training loss: 0.6892635226249695\n",
      "validation loss: 0.7014755010604858\n",
      "training epoch nr 95\n",
      "training loss: 0.6891059875488281\n",
      "validation loss: 0.6948044896125793\n",
      "training epoch nr 96\n",
      "training loss: 0.6892179250717163\n",
      "validation loss: 0.6963604688644409\n",
      "training epoch nr 97\n",
      "training loss: 0.6891270279884338\n",
      "validation loss: 0.6988747715950012\n",
      "training epoch nr 98\n",
      "training loss: 0.6890261173248291\n",
      "validation loss: 0.7038719058036804\n",
      "training epoch nr 99\n",
      "training loss: 0.6889564990997314\n",
      "validation loss: 0.7028747797012329\n",
      "Training model nr 7...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933709979057312\n",
      "validation loss: 0.6939375400543213\n",
      "training epoch nr 1\n",
      "training loss: 0.6931992173194885\n",
      "validation loss: 0.6936875581741333\n",
      "training epoch nr 2\n",
      "training loss: 0.6931179165840149\n",
      "validation loss: 0.6945216655731201\n",
      "training epoch nr 3\n",
      "training loss: 0.6930869817733765\n",
      "validation loss: 0.6910916566848755\n",
      "training epoch nr 4\n",
      "training loss: 0.6930156350135803\n",
      "validation loss: 0.6929964423179626\n",
      "training epoch nr 5\n",
      "training loss: 0.6929858326911926\n",
      "validation loss: 0.694983720779419\n",
      "training epoch nr 6\n",
      "training loss: 0.6930111646652222\n",
      "validation loss: 0.6931474208831787\n",
      "training epoch nr 7\n",
      "training loss: 0.692974865436554\n",
      "validation loss: 0.6937666535377502\n",
      "training epoch nr 8\n",
      "training loss: 0.6929837465286255\n",
      "validation loss: 0.6932539939880371\n",
      "training epoch nr 9\n",
      "training loss: 0.6928802728652954\n",
      "validation loss: 0.6922762989997864\n",
      "training epoch nr 10\n",
      "training loss: 0.6928360462188721\n",
      "validation loss: 0.6889382004737854\n",
      "training epoch nr 11\n",
      "training loss: 0.6928724050521851\n",
      "validation loss: 0.6919265985488892\n",
      "training epoch nr 12\n",
      "training loss: 0.6927903294563293\n",
      "validation loss: 0.6935500502586365\n",
      "training epoch nr 13\n",
      "training loss: 0.6928984522819519\n",
      "validation loss: 0.6914258003234863\n",
      "training epoch nr 14\n",
      "training loss: 0.6927652955055237\n",
      "validation loss: 0.6923650503158569\n",
      "training epoch nr 15\n",
      "training loss: 0.6928391456604004\n",
      "validation loss: 0.6937364935874939\n",
      "training epoch nr 16\n",
      "training loss: 0.6927866339683533\n",
      "validation loss: 0.6918360590934753\n",
      "training epoch nr 17\n",
      "training loss: 0.6927982568740845\n",
      "validation loss: 0.6904499530792236\n",
      "training epoch nr 18\n",
      "training loss: 0.6927415728569031\n",
      "validation loss: 0.6906932592391968\n",
      "training epoch nr 19\n",
      "training loss: 0.6926977038383484\n",
      "validation loss: 0.6930035352706909\n",
      "training epoch nr 20\n",
      "training loss: 0.692716658115387\n",
      "validation loss: 0.6934648156166077\n",
      "training epoch nr 21\n",
      "training loss: 0.6926345229148865\n",
      "validation loss: 0.6908898949623108\n",
      "training epoch nr 22\n",
      "training loss: 0.6926400661468506\n",
      "validation loss: 0.6924193501472473\n",
      "training epoch nr 23\n",
      "training loss: 0.6925283670425415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6919596195220947\n",
      "training epoch nr 24\n",
      "training loss: 0.6924770474433899\n",
      "validation loss: 0.6911084651947021\n",
      "training epoch nr 25\n",
      "training loss: 0.6924301981925964\n",
      "validation loss: 0.6928812861442566\n",
      "training epoch nr 26\n",
      "training loss: 0.6923457980155945\n",
      "validation loss: 0.6930832862854004\n",
      "training epoch nr 27\n",
      "training loss: 0.692326009273529\n",
      "validation loss: 0.6970106363296509\n",
      "training epoch nr 28\n",
      "training loss: 0.6922925114631653\n",
      "validation loss: 0.6960746049880981\n",
      "training epoch nr 29\n",
      "training loss: 0.6922073364257812\n",
      "validation loss: 0.6950013041496277\n",
      "training epoch nr 30\n",
      "training loss: 0.6922035813331604\n",
      "validation loss: 0.6988515257835388\n",
      "training epoch nr 31\n",
      "training loss: 0.6921547651290894\n",
      "validation loss: 0.6969897150993347\n",
      "training epoch nr 32\n",
      "training loss: 0.6920498609542847\n",
      "validation loss: 0.6969448924064636\n",
      "training epoch nr 33\n",
      "training loss: 0.6921267509460449\n",
      "validation loss: 0.6961122751235962\n",
      "training epoch nr 34\n",
      "training loss: 0.6921188235282898\n",
      "validation loss: 0.7006869912147522\n",
      "training epoch nr 35\n",
      "training loss: 0.6919876337051392\n",
      "validation loss: 0.7009056806564331\n",
      "training epoch nr 36\n",
      "training loss: 0.6919375061988831\n",
      "validation loss: 0.6990672945976257\n",
      "training epoch nr 37\n",
      "training loss: 0.6919736266136169\n",
      "validation loss: 0.6966254711151123\n",
      "training epoch nr 38\n",
      "training loss: 0.6919281482696533\n",
      "validation loss: 0.7021528482437134\n",
      "training epoch nr 39\n",
      "training loss: 0.6918786764144897\n",
      "validation loss: 0.6987297534942627\n",
      "training epoch nr 40\n",
      "training loss: 0.6918880343437195\n",
      "validation loss: 0.6952381730079651\n",
      "training epoch nr 41\n",
      "training loss: 0.6918907165527344\n",
      "validation loss: 0.6932864785194397\n",
      "training epoch nr 42\n",
      "training loss: 0.6919036507606506\n",
      "validation loss: 0.6971724033355713\n",
      "training epoch nr 43\n",
      "training loss: 0.6918389797210693\n",
      "validation loss: 0.6969424486160278\n",
      "training epoch nr 44\n",
      "training loss: 0.6918438076972961\n",
      "validation loss: 0.6904693841934204\n",
      "training epoch nr 45\n",
      "training loss: 0.6917770504951477\n",
      "validation loss: 0.6958879232406616\n",
      "training epoch nr 46\n",
      "training loss: 0.6916059851646423\n",
      "validation loss: 0.6998067498207092\n",
      "training epoch nr 47\n",
      "training loss: 0.6916295289993286\n",
      "validation loss: 0.6993893384933472\n",
      "training epoch nr 48\n",
      "training loss: 0.6917478442192078\n",
      "validation loss: 0.6945465207099915\n",
      "training epoch nr 49\n",
      "training loss: 0.6916893124580383\n",
      "validation loss: 0.6966931819915771\n",
      "training epoch nr 50\n",
      "training loss: 0.6916247010231018\n",
      "validation loss: 0.7006292939186096\n",
      "training epoch nr 51\n",
      "training loss: 0.6915676593780518\n",
      "validation loss: 0.6992107033729553\n",
      "training epoch nr 52\n",
      "training loss: 0.6915059685707092\n",
      "validation loss: 0.7013975977897644\n",
      "training epoch nr 53\n",
      "training loss: 0.6915241479873657\n",
      "validation loss: 0.7023501396179199\n",
      "training epoch nr 54\n",
      "training loss: 0.6914940476417542\n",
      "validation loss: 0.6981946229934692\n",
      "training epoch nr 55\n",
      "training loss: 0.6914005875587463\n",
      "validation loss: 0.7048052549362183\n",
      "training epoch nr 56\n",
      "training loss: 0.6914506554603577\n",
      "validation loss: 0.6954463720321655\n",
      "training epoch nr 57\n",
      "training loss: 0.691451907157898\n",
      "validation loss: 0.6970943808555603\n",
      "training epoch nr 58\n",
      "training loss: 0.691402018070221\n",
      "validation loss: 0.7036896347999573\n",
      "training epoch nr 59\n",
      "training loss: 0.6913867592811584\n",
      "validation loss: 0.706216037273407\n",
      "training epoch nr 60\n",
      "training loss: 0.6913371086120605\n",
      "validation loss: 0.7000230550765991\n",
      "training epoch nr 61\n",
      "training loss: 0.6912781596183777\n",
      "validation loss: 0.6938687562942505\n",
      "training epoch nr 62\n",
      "training loss: 0.6912957429885864\n",
      "validation loss: 0.6994817852973938\n",
      "training epoch nr 63\n",
      "training loss: 0.69135582447052\n",
      "validation loss: 0.7011293172836304\n",
      "training epoch nr 64\n",
      "training loss: 0.6912015676498413\n",
      "validation loss: 0.6948543787002563\n",
      "training epoch nr 65\n",
      "training loss: 0.69122713804245\n",
      "validation loss: 0.7065479755401611\n",
      "training epoch nr 66\n",
      "training loss: 0.6912227869033813\n",
      "validation loss: 0.7031940221786499\n",
      "training epoch nr 67\n",
      "training loss: 0.6912199854850769\n",
      "validation loss: 0.69870924949646\n",
      "training epoch nr 68\n",
      "training loss: 0.6910151243209839\n",
      "validation loss: 0.7030073404312134\n",
      "training epoch nr 69\n",
      "training loss: 0.691087543964386\n",
      "validation loss: 0.6966492533683777\n",
      "training epoch nr 70\n",
      "training loss: 0.6911553144454956\n",
      "validation loss: 0.6961763501167297\n",
      "training epoch nr 71\n",
      "training loss: 0.6910638809204102\n",
      "validation loss: 0.7086181640625\n",
      "training epoch nr 72\n",
      "training loss: 0.6910721659660339\n",
      "validation loss: 0.7004420161247253\n",
      "training epoch nr 73\n",
      "training loss: 0.691087543964386\n",
      "validation loss: 0.7195515632629395\n",
      "training epoch nr 74\n",
      "training loss: 0.6909692883491516\n",
      "validation loss: 0.6990952491760254\n",
      "training epoch nr 75\n",
      "training loss: 0.6909422278404236\n",
      "validation loss: 0.7039880752563477\n",
      "training epoch nr 76\n",
      "training loss: 0.6910560727119446\n",
      "validation loss: 0.6976575255393982\n",
      "training epoch nr 77\n",
      "training loss: 0.690804660320282\n",
      "validation loss: 0.7069166302680969\n",
      "training epoch nr 78\n",
      "training loss: 0.6908726096153259\n",
      "validation loss: 0.7090821266174316\n",
      "training epoch nr 79\n",
      "training loss: 0.6908845901489258\n",
      "validation loss: 0.6979776620864868\n",
      "training epoch nr 80\n",
      "training loss: 0.6906601190567017\n",
      "validation loss: 0.7011383771896362\n",
      "training epoch nr 81\n",
      "training loss: 0.6906495094299316\n",
      "validation loss: 0.6985112428665161\n",
      "training epoch nr 82\n",
      "training loss: 0.6908214092254639\n",
      "validation loss: 0.6980100274085999\n",
      "training epoch nr 83\n",
      "training loss: 0.6907684206962585\n",
      "validation loss: 0.6991156935691833\n",
      "training epoch nr 84\n",
      "training loss: 0.6907572150230408\n",
      "validation loss: 0.6958555579185486\n",
      "training epoch nr 85\n",
      "training loss: 0.6907475590705872\n",
      "validation loss: 0.6938971281051636\n",
      "training epoch nr 86\n",
      "training loss: 0.6906713247299194\n",
      "validation loss: 0.693655252456665\n",
      "training epoch nr 87\n",
      "training loss: 0.6906552314758301\n",
      "validation loss: 0.6922569274902344\n",
      "training epoch nr 88\n",
      "training loss: 0.6905553340911865\n",
      "validation loss: 0.6995084881782532\n",
      "training epoch nr 89\n",
      "training loss: 0.6906110048294067\n",
      "validation loss: 0.6989247798919678\n",
      "training epoch nr 90\n",
      "training loss: 0.6905568838119507\n",
      "validation loss: 0.6997836828231812\n",
      "training epoch nr 91\n",
      "training loss: 0.690515398979187\n",
      "validation loss: 0.6953702569007874\n",
      "training epoch nr 92\n",
      "training loss: 0.6904945373535156\n",
      "validation loss: 0.7075139880180359\n",
      "training epoch nr 93\n",
      "training loss: 0.6904451251029968\n",
      "validation loss: 0.6999518871307373\n",
      "training epoch nr 94\n",
      "training loss: 0.6903554201126099\n",
      "validation loss: 0.7015404105186462\n",
      "training epoch nr 95\n",
      "training loss: 0.6903631091117859\n",
      "validation loss: 0.7060136795043945\n",
      "training epoch nr 96\n",
      "training loss: 0.6904336214065552\n",
      "validation loss: 0.6955854296684265\n",
      "training epoch nr 97\n",
      "training loss: 0.6904009580612183\n",
      "validation loss: 0.6985576748847961\n",
      "training epoch nr 98\n",
      "training loss: 0.6903060674667358\n",
      "validation loss: 0.7006197571754456\n",
      "training epoch nr 99\n",
      "training loss: 0.6902337670326233\n",
      "validation loss: 0.7007215619087219\n",
      "Training model nr 8...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932434439659119\n",
      "validation loss: 0.6983736157417297\n",
      "training epoch nr 1\n",
      "training loss: 0.6930666565895081\n",
      "validation loss: 0.6931099891662598\n",
      "training epoch nr 2\n",
      "training loss: 0.6930683851242065\n",
      "validation loss: 0.6942349076271057\n",
      "training epoch nr 3\n",
      "training loss: 0.693057119846344\n",
      "validation loss: 0.6927070617675781\n",
      "training epoch nr 4\n",
      "training loss: 0.6930460929870605\n",
      "validation loss: 0.6917757391929626\n",
      "training epoch nr 5\n",
      "training loss: 0.6929954290390015\n",
      "validation loss: 0.6933798789978027\n",
      "training epoch nr 6\n",
      "training loss: 0.692898154258728\n",
      "validation loss: 0.6919263005256653\n",
      "training epoch nr 7\n",
      "training loss: 0.6929330825805664\n",
      "validation loss: 0.691935122013092\n",
      "training epoch nr 8\n",
      "training loss: 0.692886233329773\n",
      "validation loss: 0.6922773718833923\n",
      "training epoch nr 9\n",
      "training loss: 0.692947506904602\n",
      "validation loss: 0.6907069087028503\n",
      "training epoch nr 10\n",
      "training loss: 0.6928112506866455\n",
      "validation loss: 0.6914219260215759\n",
      "training epoch nr 11\n",
      "training loss: 0.692821741104126\n",
      "validation loss: 0.6905308961868286\n",
      "training epoch nr 12\n",
      "training loss: 0.6928365230560303\n",
      "validation loss: 0.6919560432434082\n",
      "training epoch nr 13\n",
      "training loss: 0.6928396821022034\n",
      "validation loss: 0.6926474571228027\n",
      "training epoch nr 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6927737593650818\n",
      "validation loss: 0.6909342408180237\n",
      "training epoch nr 15\n",
      "training loss: 0.6927642226219177\n",
      "validation loss: 0.6916611194610596\n",
      "training epoch nr 16\n",
      "training loss: 0.6926897764205933\n",
      "validation loss: 0.691599428653717\n",
      "training epoch nr 17\n",
      "training loss: 0.6926308274269104\n",
      "validation loss: 0.6921172142028809\n",
      "training epoch nr 18\n",
      "training loss: 0.6927464604377747\n",
      "validation loss: 0.6925827860832214\n",
      "training epoch nr 19\n",
      "training loss: 0.6926538944244385\n",
      "validation loss: 0.691990852355957\n",
      "training epoch nr 20\n",
      "training loss: 0.6925605535507202\n",
      "validation loss: 0.6917730569839478\n",
      "training epoch nr 21\n",
      "training loss: 0.6926665902137756\n",
      "validation loss: 0.6977949738502502\n",
      "training epoch nr 22\n",
      "training loss: 0.6925734877586365\n",
      "validation loss: 0.6946200728416443\n",
      "training epoch nr 23\n",
      "training loss: 0.6926218271255493\n",
      "validation loss: 0.6919918060302734\n",
      "training epoch nr 24\n",
      "training loss: 0.6925466060638428\n",
      "validation loss: 0.6945841908454895\n",
      "training epoch nr 25\n",
      "training loss: 0.692481279373169\n",
      "validation loss: 0.689681887626648\n",
      "training epoch nr 26\n",
      "training loss: 0.6924248933792114\n",
      "validation loss: 0.6872312426567078\n",
      "training epoch nr 27\n",
      "training loss: 0.6923806667327881\n",
      "validation loss: 0.6944115161895752\n",
      "training epoch nr 28\n",
      "training loss: 0.6922500133514404\n",
      "validation loss: 0.692999541759491\n",
      "training epoch nr 29\n",
      "training loss: 0.6923460960388184\n",
      "validation loss: 0.697913408279419\n",
      "training epoch nr 30\n",
      "training loss: 0.6922621130943298\n",
      "validation loss: 0.6957459449768066\n",
      "training epoch nr 31\n",
      "training loss: 0.6921553611755371\n",
      "validation loss: 0.6952924132347107\n",
      "training epoch nr 32\n",
      "training loss: 0.6922194361686707\n",
      "validation loss: 0.6922853589057922\n",
      "training epoch nr 33\n",
      "training loss: 0.692134439945221\n",
      "validation loss: 0.702438473701477\n",
      "training epoch nr 34\n",
      "training loss: 0.6920959949493408\n",
      "validation loss: 0.6969245076179504\n",
      "training epoch nr 35\n",
      "training loss: 0.6921452283859253\n",
      "validation loss: 0.6949167847633362\n",
      "training epoch nr 36\n",
      "training loss: 0.6919853687286377\n",
      "validation loss: 0.6910929083824158\n",
      "training epoch nr 37\n",
      "training loss: 0.6919742226600647\n",
      "validation loss: 0.695125162601471\n",
      "training epoch nr 38\n",
      "training loss: 0.6919091939926147\n",
      "validation loss: 0.6908886432647705\n",
      "training epoch nr 39\n",
      "training loss: 0.6919524669647217\n",
      "validation loss: 0.6919932961463928\n",
      "training epoch nr 40\n",
      "training loss: 0.6918408870697021\n",
      "validation loss: 0.6924914717674255\n",
      "training epoch nr 41\n",
      "training loss: 0.6918444633483887\n",
      "validation loss: 0.6922961473464966\n",
      "training epoch nr 42\n",
      "training loss: 0.6917920112609863\n",
      "validation loss: 0.6906322240829468\n",
      "training epoch nr 43\n",
      "training loss: 0.6917622089385986\n",
      "validation loss: 0.6917384266853333\n",
      "training epoch nr 44\n",
      "training loss: 0.691626250743866\n",
      "validation loss: 0.6904536485671997\n",
      "training epoch nr 45\n",
      "training loss: 0.6916847229003906\n",
      "validation loss: 0.6904760003089905\n",
      "training epoch nr 46\n",
      "training loss: 0.6915877461433411\n",
      "validation loss: 0.6894371509552002\n",
      "training epoch nr 47\n",
      "training loss: 0.691748321056366\n",
      "validation loss: 0.6910691857337952\n",
      "training epoch nr 48\n",
      "training loss: 0.6915198564529419\n",
      "validation loss: 0.6911504864692688\n",
      "training epoch nr 49\n",
      "training loss: 0.6915391087532043\n",
      "validation loss: 0.692457914352417\n",
      "training epoch nr 50\n",
      "training loss: 0.6914651989936829\n",
      "validation loss: 0.6944753527641296\n",
      "training epoch nr 51\n",
      "training loss: 0.6914151906967163\n",
      "validation loss: 0.6971165537834167\n",
      "training epoch nr 52\n",
      "training loss: 0.6914311647415161\n",
      "validation loss: 0.6978212594985962\n",
      "training epoch nr 53\n",
      "training loss: 0.691265344619751\n",
      "validation loss: 0.6865750551223755\n",
      "training epoch nr 54\n",
      "training loss: 0.6912577748298645\n",
      "validation loss: 0.7016107439994812\n",
      "training epoch nr 55\n",
      "training loss: 0.6912062764167786\n",
      "validation loss: 0.6964699029922485\n",
      "training epoch nr 56\n",
      "training loss: 0.6910668611526489\n",
      "validation loss: 0.6993707418441772\n",
      "training epoch nr 57\n",
      "training loss: 0.6911025643348694\n",
      "validation loss: 0.6976486444473267\n",
      "training epoch nr 58\n",
      "training loss: 0.6910741329193115\n",
      "validation loss: 0.7051747441291809\n",
      "training epoch nr 59\n",
      "training loss: 0.6910597085952759\n",
      "validation loss: 0.6998916268348694\n",
      "training epoch nr 60\n",
      "training loss: 0.6909567713737488\n",
      "validation loss: 0.6992573738098145\n",
      "training epoch nr 61\n",
      "training loss: 0.6910607814788818\n",
      "validation loss: 0.7048466801643372\n",
      "training epoch nr 62\n",
      "training loss: 0.6909189224243164\n",
      "validation loss: 0.693212628364563\n",
      "training epoch nr 63\n",
      "training loss: 0.690956175327301\n",
      "validation loss: 0.6937311887741089\n",
      "training epoch nr 64\n",
      "training loss: 0.6907845735549927\n",
      "validation loss: 0.696650505065918\n",
      "training epoch nr 65\n",
      "training loss: 0.6908256411552429\n",
      "validation loss: 0.6853680610656738\n",
      "training epoch nr 66\n",
      "training loss: 0.6907484531402588\n",
      "validation loss: 0.693527340888977\n",
      "training epoch nr 67\n",
      "training loss: 0.6907367706298828\n",
      "validation loss: 0.6979089975357056\n",
      "training epoch nr 68\n",
      "training loss: 0.6906446218490601\n",
      "validation loss: 0.7015539407730103\n",
      "training epoch nr 69\n",
      "training loss: 0.6905714869499207\n",
      "validation loss: 0.6950899362564087\n",
      "training epoch nr 70\n",
      "training loss: 0.6904573440551758\n",
      "validation loss: 0.6970192193984985\n",
      "training epoch nr 71\n",
      "training loss: 0.6902821063995361\n",
      "validation loss: 0.7057097554206848\n",
      "training epoch nr 72\n",
      "training loss: 0.6904435157775879\n",
      "validation loss: 0.6975806951522827\n",
      "training epoch nr 73\n",
      "training loss: 0.6904347538948059\n",
      "validation loss: 0.6977564096450806\n",
      "training epoch nr 74\n",
      "training loss: 0.6903437972068787\n",
      "validation loss: 0.7021188139915466\n",
      "training epoch nr 75\n",
      "training loss: 0.6902313828468323\n",
      "validation loss: 0.6903865337371826\n",
      "training epoch nr 76\n",
      "training loss: 0.6903380751609802\n",
      "validation loss: 0.6973896026611328\n",
      "training epoch nr 77\n",
      "training loss: 0.6903209686279297\n",
      "validation loss: 0.7068955898284912\n",
      "training epoch nr 78\n",
      "training loss: 0.69008469581604\n",
      "validation loss: 0.6982227563858032\n",
      "training epoch nr 79\n",
      "training loss: 0.690140962600708\n",
      "validation loss: 0.69820237159729\n",
      "training epoch nr 80\n",
      "training loss: 0.690010130405426\n",
      "validation loss: 0.7061271071434021\n",
      "training epoch nr 81\n",
      "training loss: 0.6900614500045776\n",
      "validation loss: 0.6978544592857361\n",
      "training epoch nr 82\n",
      "training loss: 0.689906895160675\n",
      "validation loss: 0.6969698667526245\n",
      "training epoch nr 83\n",
      "training loss: 0.6899826526641846\n",
      "validation loss: 0.6964429616928101\n",
      "training epoch nr 84\n",
      "training loss: 0.689769446849823\n",
      "validation loss: 0.6936002969741821\n",
      "training epoch nr 85\n",
      "training loss: 0.6897712349891663\n",
      "validation loss: 0.700699508190155\n",
      "training epoch nr 86\n",
      "training loss: 0.6897574663162231\n",
      "validation loss: 0.6990064382553101\n",
      "training epoch nr 87\n",
      "training loss: 0.6894587874412537\n",
      "validation loss: 0.6977425217628479\n",
      "training epoch nr 88\n",
      "training loss: 0.6895434260368347\n",
      "validation loss: 0.6967592239379883\n",
      "training epoch nr 89\n",
      "training loss: 0.6897374987602234\n",
      "validation loss: 0.7018780708312988\n",
      "training epoch nr 90\n",
      "training loss: 0.6895570755004883\n",
      "validation loss: 0.7026540040969849\n",
      "training epoch nr 91\n",
      "training loss: 0.6896162033081055\n",
      "validation loss: 0.7052443623542786\n",
      "training epoch nr 92\n",
      "training loss: 0.6893208026885986\n",
      "validation loss: 0.6909763813018799\n",
      "training epoch nr 93\n",
      "training loss: 0.6892830729484558\n",
      "validation loss: 0.7011326551437378\n",
      "training epoch nr 94\n",
      "training loss: 0.689307689666748\n",
      "validation loss: 0.696094274520874\n",
      "training epoch nr 95\n",
      "training loss: 0.6891998648643494\n",
      "validation loss: 0.7127166986465454\n",
      "training epoch nr 96\n",
      "training loss: 0.6891905665397644\n",
      "validation loss: 0.6960077285766602\n",
      "training epoch nr 97\n",
      "training loss: 0.6892528533935547\n",
      "validation loss: 0.7073139548301697\n",
      "training epoch nr 98\n",
      "training loss: 0.6891540884971619\n",
      "validation loss: 0.6916080117225647\n",
      "training epoch nr 99\n",
      "training loss: 0.689074695110321\n",
      "validation loss: 0.6910520792007446\n",
      "Training model nr 9...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933354139328003\n",
      "validation loss: 0.6969327330589294\n",
      "training epoch nr 1\n",
      "training loss: 0.6932471394538879\n",
      "validation loss: 0.6940509080886841\n",
      "training epoch nr 2\n",
      "training loss: 0.693159282207489\n",
      "validation loss: 0.6948143243789673\n",
      "training epoch nr 3\n",
      "training loss: 0.6931100487709045\n",
      "validation loss: 0.6943783760070801\n",
      "training epoch nr 4\n",
      "training loss: 0.693167507648468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6922244429588318\n",
      "training epoch nr 5\n",
      "training loss: 0.6931259632110596\n",
      "validation loss: 0.6934130191802979\n",
      "training epoch nr 6\n",
      "training loss: 0.6930693984031677\n",
      "validation loss: 0.6935009360313416\n",
      "training epoch nr 7\n",
      "training loss: 0.6930195689201355\n",
      "validation loss: 0.6952805519104004\n",
      "training epoch nr 8\n",
      "training loss: 0.6930373907089233\n",
      "validation loss: 0.691798210144043\n",
      "training epoch nr 9\n",
      "training loss: 0.6929882764816284\n",
      "validation loss: 0.6939482688903809\n",
      "training epoch nr 10\n",
      "training loss: 0.6929425597190857\n",
      "validation loss: 0.6925357580184937\n",
      "training epoch nr 11\n",
      "training loss: 0.69284987449646\n",
      "validation loss: 0.6930391192436218\n",
      "training epoch nr 12\n",
      "training loss: 0.6929166913032532\n",
      "validation loss: 0.6943768858909607\n",
      "training epoch nr 13\n",
      "training loss: 0.6928553581237793\n",
      "validation loss: 0.6920607686042786\n",
      "training epoch nr 14\n",
      "training loss: 0.6928407549858093\n",
      "validation loss: 0.6925715804100037\n",
      "training epoch nr 15\n",
      "training loss: 0.6928205490112305\n",
      "validation loss: 0.6939974427223206\n",
      "training epoch nr 16\n",
      "training loss: 0.6928015351295471\n",
      "validation loss: 0.6964062452316284\n",
      "training epoch nr 17\n",
      "training loss: 0.6927480101585388\n",
      "validation loss: 0.6947299242019653\n",
      "training epoch nr 18\n",
      "training loss: 0.6927685737609863\n",
      "validation loss: 0.6939313411712646\n",
      "training epoch nr 19\n",
      "training loss: 0.6926984190940857\n",
      "validation loss: 0.6947280764579773\n",
      "training epoch nr 20\n",
      "training loss: 0.6926748752593994\n",
      "validation loss: 0.6944270730018616\n",
      "training epoch nr 21\n",
      "training loss: 0.6926076412200928\n",
      "validation loss: 0.6944365501403809\n",
      "training epoch nr 22\n",
      "training loss: 0.6925357580184937\n",
      "validation loss: 0.699023962020874\n",
      "training epoch nr 23\n",
      "training loss: 0.6924997568130493\n",
      "validation loss: 0.6923794150352478\n",
      "training epoch nr 24\n",
      "training loss: 0.6925450563430786\n",
      "validation loss: 0.6941081285476685\n",
      "training epoch nr 25\n",
      "training loss: 0.6925036311149597\n",
      "validation loss: 0.6936102509498596\n",
      "training epoch nr 26\n",
      "training loss: 0.6925016641616821\n",
      "validation loss: 0.6893437504768372\n",
      "training epoch nr 27\n",
      "training loss: 0.6924634575843811\n",
      "validation loss: 0.696333110332489\n",
      "training epoch nr 28\n",
      "training loss: 0.6924383044242859\n",
      "validation loss: 0.6940558552742004\n",
      "training epoch nr 29\n",
      "training loss: 0.6923828721046448\n",
      "validation loss: 0.6966457962989807\n",
      "training epoch nr 30\n",
      "training loss: 0.6923602819442749\n",
      "validation loss: 0.6938396692276001\n",
      "training epoch nr 31\n",
      "training loss: 0.6922991871833801\n",
      "validation loss: 0.6923292875289917\n",
      "training epoch nr 32\n",
      "training loss: 0.6923273801803589\n",
      "validation loss: 0.6978669166564941\n",
      "training epoch nr 33\n",
      "training loss: 0.6922204494476318\n",
      "validation loss: 0.6881566047668457\n",
      "training epoch nr 34\n",
      "training loss: 0.6922450661659241\n",
      "validation loss: 0.7001221776008606\n",
      "training epoch nr 35\n",
      "training loss: 0.6921753287315369\n",
      "validation loss: 0.6982671022415161\n",
      "training epoch nr 36\n",
      "training loss: 0.6921050548553467\n",
      "validation loss: 0.6900932192802429\n",
      "training epoch nr 37\n",
      "training loss: 0.6920199990272522\n",
      "validation loss: 0.6939321160316467\n",
      "training epoch nr 38\n",
      "training loss: 0.6920993328094482\n",
      "validation loss: 0.6951971650123596\n",
      "training epoch nr 39\n",
      "training loss: 0.6919474601745605\n",
      "validation loss: 0.693425714969635\n",
      "training epoch nr 40\n",
      "training loss: 0.6918939352035522\n",
      "validation loss: 0.6940153241157532\n",
      "training epoch nr 41\n",
      "training loss: 0.6918383836746216\n",
      "validation loss: 0.6954560875892639\n",
      "training epoch nr 42\n",
      "training loss: 0.6918986439704895\n",
      "validation loss: 0.697171688079834\n",
      "training epoch nr 43\n",
      "training loss: 0.6917647123336792\n",
      "validation loss: 0.7042505741119385\n",
      "training epoch nr 44\n",
      "training loss: 0.6917176246643066\n",
      "validation loss: 0.6960827708244324\n",
      "training epoch nr 45\n",
      "training loss: 0.6916885375976562\n",
      "validation loss: 0.6998262405395508\n",
      "training epoch nr 46\n",
      "training loss: 0.6916441917419434\n",
      "validation loss: 0.695785403251648\n",
      "training epoch nr 47\n",
      "training loss: 0.6915950775146484\n",
      "validation loss: 0.6940454244613647\n",
      "training epoch nr 48\n",
      "training loss: 0.6915104389190674\n",
      "validation loss: 0.6920409798622131\n",
      "training epoch nr 49\n",
      "training loss: 0.6914165616035461\n",
      "validation loss: 0.6938698291778564\n",
      "training epoch nr 50\n",
      "training loss: 0.691508948802948\n",
      "validation loss: 0.693216860294342\n",
      "training epoch nr 51\n",
      "training loss: 0.6913648843765259\n",
      "validation loss: 0.701211154460907\n",
      "training epoch nr 52\n",
      "training loss: 0.691331148147583\n",
      "validation loss: 0.6948215365409851\n",
      "training epoch nr 53\n",
      "training loss: 0.6912323832511902\n",
      "validation loss: 0.6884414553642273\n",
      "training epoch nr 54\n",
      "training loss: 0.6912528872489929\n",
      "validation loss: 0.6952287554740906\n",
      "training epoch nr 55\n",
      "training loss: 0.6912174820899963\n",
      "validation loss: 0.7045251131057739\n",
      "training epoch nr 56\n",
      "training loss: 0.6911470293998718\n",
      "validation loss: 0.7033014893531799\n",
      "training epoch nr 57\n",
      "training loss: 0.6911699175834656\n",
      "validation loss: 0.70308917760849\n",
      "training epoch nr 58\n",
      "training loss: 0.6912150382995605\n",
      "validation loss: 0.6944729685783386\n",
      "training epoch nr 59\n",
      "training loss: 0.691148579120636\n",
      "validation loss: 0.6877756118774414\n",
      "training epoch nr 60\n",
      "training loss: 0.6909475922584534\n",
      "validation loss: 0.7080355286598206\n",
      "training epoch nr 61\n",
      "training loss: 0.6910821199417114\n",
      "validation loss: 0.7073460817337036\n",
      "training epoch nr 62\n",
      "training loss: 0.6909850239753723\n",
      "validation loss: 0.7148685455322266\n",
      "training epoch nr 63\n",
      "training loss: 0.6908435821533203\n",
      "validation loss: 0.6932217478752136\n",
      "training epoch nr 64\n",
      "training loss: 0.6908281445503235\n",
      "validation loss: 0.6978042721748352\n",
      "training epoch nr 65\n",
      "training loss: 0.6908122897148132\n",
      "validation loss: 0.6943556666374207\n",
      "training epoch nr 66\n",
      "training loss: 0.6907768845558167\n",
      "validation loss: 0.6978655457496643\n",
      "training epoch nr 67\n",
      "training loss: 0.6906287670135498\n",
      "validation loss: 0.704103410243988\n",
      "training epoch nr 68\n",
      "training loss: 0.6907802820205688\n",
      "validation loss: 0.6952935457229614\n",
      "training epoch nr 69\n",
      "training loss: 0.690719485282898\n",
      "validation loss: 0.6992321014404297\n",
      "training epoch nr 70\n",
      "training loss: 0.690683126449585\n",
      "validation loss: 0.6890525817871094\n",
      "training epoch nr 71\n",
      "training loss: 0.6906353235244751\n",
      "validation loss: 0.698493242263794\n",
      "training epoch nr 72\n",
      "training loss: 0.6905906796455383\n",
      "validation loss: 0.6985213756561279\n",
      "training epoch nr 73\n",
      "training loss: 0.6904804110527039\n",
      "validation loss: 0.6876185536384583\n",
      "training epoch nr 74\n",
      "training loss: 0.6905667185783386\n",
      "validation loss: 0.6976507306098938\n",
      "training epoch nr 75\n",
      "training loss: 0.6904383897781372\n",
      "validation loss: 0.6977452039718628\n",
      "training epoch nr 76\n",
      "training loss: 0.6904995441436768\n",
      "validation loss: 0.6993076205253601\n",
      "training epoch nr 77\n",
      "training loss: 0.6903938055038452\n",
      "validation loss: 0.6971133947372437\n",
      "training epoch nr 78\n",
      "training loss: 0.6902897357940674\n",
      "validation loss: 0.6933261156082153\n",
      "training epoch nr 79\n",
      "training loss: 0.6903045773506165\n",
      "validation loss: 0.6921612620353699\n",
      "training epoch nr 80\n",
      "training loss: 0.6902580857276917\n",
      "validation loss: 0.6984494924545288\n",
      "training epoch nr 81\n",
      "training loss: 0.6901258826255798\n",
      "validation loss: 0.7112549543380737\n",
      "training epoch nr 82\n",
      "training loss: 0.6901717185974121\n",
      "validation loss: 0.6972073316574097\n",
      "training epoch nr 83\n",
      "training loss: 0.6902772784233093\n",
      "validation loss: 0.6950081586837769\n",
      "training epoch nr 84\n",
      "training loss: 0.6900280714035034\n",
      "validation loss: 0.7019219994544983\n",
      "training epoch nr 85\n",
      "training loss: 0.6900931000709534\n",
      "validation loss: 0.7010513544082642\n",
      "training epoch nr 86\n",
      "training loss: 0.6900255680084229\n",
      "validation loss: 0.6987996697425842\n",
      "training epoch nr 87\n",
      "training loss: 0.6900090575218201\n",
      "validation loss: 0.6971792578697205\n",
      "training epoch nr 88\n",
      "training loss: 0.6900298595428467\n",
      "validation loss: 0.7117221355438232\n",
      "training epoch nr 89\n",
      "training loss: 0.690012514591217\n",
      "validation loss: 0.7014502882957458\n",
      "training epoch nr 90\n",
      "training loss: 0.6899227499961853\n",
      "validation loss: 0.7030127644538879\n",
      "training epoch nr 91\n",
      "training loss: 0.6897962689399719\n",
      "validation loss: 0.7072772979736328\n",
      "training epoch nr 92\n",
      "training loss: 0.6897421479225159\n",
      "validation loss: 0.7013950347900391\n",
      "training epoch nr 93\n",
      "training loss: 0.6898277401924133\n",
      "validation loss: 0.7008281350135803\n",
      "training epoch nr 94\n",
      "training loss: 0.689849317073822\n",
      "validation loss: 0.6956797242164612\n",
      "training epoch nr 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6896779537200928\n",
      "validation loss: 0.7006023526191711\n",
      "training epoch nr 96\n",
      "training loss: 0.6896700263023376\n",
      "validation loss: 0.7070900201797485\n",
      "training epoch nr 97\n",
      "training loss: 0.6896584033966064\n",
      "validation loss: 0.6924435496330261\n",
      "training epoch nr 98\n",
      "training loss: 0.6897239685058594\n",
      "validation loss: 0.7102804780006409\n",
      "training epoch nr 99\n",
      "training loss: 0.6895762085914612\n",
      "validation loss: 0.7107054591178894\n",
      "minimum validation loss epochs: [94 14 28  3  4 24  6  0  8 32]\n",
      "minimum validation loss epochs: [31 67 44 47 14 22  6 45 87 15]\n",
      "minimum validation loss epochs: [20 60  2 42 48 73 61 53 52 57]\n",
      "minimum validation loss epochs: [65 85 25 70 54 87 32  1 23 83]\n",
      "minimum validation loss epochs: [89 63 96 77 81 66 84 56  1 83]\n",
      "minimum validation loss epochs: [ 0 18 10 13  7 15  1  9 16 74]\n",
      "minimum validation loss epochs: [20 73 39 18 19 22 60 59 16 68]\n",
      "minimum validation loss epochs: [17 10 44 18 21  3 24 13 16 11]\n",
      "minimum validation loss epochs: [65 53 26 25 46 75 44 45 11 42]\n",
      "minimum validation loss epochs: [ 8 33 36 53 26 73 70 59 48 13]\n"
     ]
    }
   ],
   "source": [
    "train_classifier(**classifier_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c549809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "from evaluation_utils import (load_predictions, tprs_fprs_sics,\n",
    "                              minumum_validation_loss_ensemble, compare_on_various_runs)\n",
    "# evalautes the ROCs and SICs of one given training with minimum validation loss epoch picking\n",
    "def full_single_evaluation(data_dir, prediction_dir, n_ensemble_epochs=10, extra_signal=True,\n",
    "                           sic_range=(0,20), savefig=None, suppress_show=False, return_all=False):\n",
    "    X_test, y_test, predictions, val_losses = load_predictions(\n",
    "        data_dir, prediction_dir, extra_signal=extra_signal)\n",
    "    if predictions.shape[1]==1: ## check if ensembling done already\n",
    "        min_val_loss_predictions = predictions\n",
    "    else:\n",
    "        min_val_loss_predictions = minumum_validation_loss_ensemble(\n",
    "            predictions, val_losses, n_epochs=n_ensemble_epochs)\n",
    "    tprs, fprs, sics = tprs_fprs_sics(min_val_loss_predictions, y_test, X_test)\n",
    "\n",
    "    return compare_on_various_runs(\n",
    "        [tprs], [fprs], [np.zeros(min_val_loss_predictions.shape[0])], [\"\"],\n",
    "        sic_lim=sic_range, savefig=savefig, only_median=False, continuous_colors=False,\n",
    "        reduced_legend=False, suppress_show=suppress_show, return_all=return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5979dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = full_single_evaluation(save_dir, save_dir, n_ensemble_epochs=10,\n",
    "                           extra_signal=not no_extra_signal, sic_range=(0, 20),\n",
    "                           savefig=os.path.join(save_dir, 'result_SIC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade5382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
