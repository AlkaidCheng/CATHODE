{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207af6f7",
   "metadata": {},
   "source": [
    "# Notebook 02: Pipeline for idealized AD results\n",
    "\n",
    "This notebook goes through the pipeline for obtaining results using the idealized AD (idealized anomaly detection) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab3251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from run_ANODE_training import main as train_DE\n",
    "from run_classifier_data_creation import main as create_data\n",
    "from run_classifier_training import main as train_classifier\n",
    "from run_ANODE_evaluation import main as eval_ANODE\n",
    "from evaluation_utils import full_single_evaluation, classic_ANODE_eval, minimum_val_loss_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334e4076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "mode = 'idealized_AD'\n",
    "data_dir = '../separated_data'\n",
    "save_dir = 'idealized_AD_models'\n",
    "# Shift on jet mass variables to be applied.\n",
    "datashift = 0.\n",
    "# Shift is not correlated to the actual mjj but randomized.\n",
    "random_shift = False\n",
    "# Whether to apply an (ANODE paper) fiducial cut on the data (and samples).\n",
    "fiducial_cut = False\n",
    "# Suppress the processing of the extra signal sample.\n",
    "no_extra_signal = True\n",
    "verbose = False\n",
    "\n",
    "# ANODE model config file (.yml).\n",
    "DE_config_file = '../DE_MAF_model.yml'\n",
    "# 'Number of Density Estimation training epochs.'\n",
    "DE_epochs = 100\n",
    "# Batch size during density estimation training.\n",
    "DE_batch_size = 256\n",
    "# Skips the density estimation (loads existing files instead).\n",
    "DF_skip = False\n",
    "# Turns off the logit transform in the density estimator.\n",
    "DE_no_logit = False\n",
    "\n",
    "# File name for the density estimator.\n",
    "DE_file_name = 'my_ANODE_model'\n",
    "\n",
    "# Classifier model config file (.yml).\n",
    "cf_config_file = '../classifier.yml'\n",
    "\n",
    "# Number of classifier training epochs\n",
    "cf_epochs = 100\n",
    "# Number of samples to be generated. Currently the samples will be cut down to match data proportion.\n",
    "cf_n_samples = 130000\n",
    "\n",
    "# Sample the conditional from a KDE fit rather than a uniform distribution.\n",
    "cf_realistic_conditional = False\n",
    "# Bandwith of the KDE fit (used when realistic_conditional is selected)\n",
    "cf_KDE_bandwidth = 0.01\n",
    "# Add the full number of samples to the training set rather than mixing it in equal parts with data.\n",
    "cf_oversampling = True\n",
    "# Turns off logit tranform in the classifier.\n",
    "cf_no_logit = True\n",
    "# Space-separated list of pre-sampled npy files of physical variables if the sampling has been done externally. The format is \n",
    "# (mjj, mj1, dmj, tau21_1, tau21_2)\n",
    "cf_external_samples = \"\"\n",
    "# Lower boundary of signal region.\n",
    "cf_SR_min = 3.3\n",
    "# Upper boundary of signal region.\n",
    "cf_SR_max = 3.7\n",
    "# Number of independent classifier training runs.\n",
    "cf_n_runs = 10\n",
    "# Batch size during classifier training.\n",
    "cf_batch_size = 128\n",
    "# Use the conditional variable as classifier input during training.\n",
    "cf_use_mjj = False\n",
    "# Weight the classes according to their occurence in the training set. \n",
    "# Necessary if the training set was intentionally oversampled.'\n",
    "cf_use_class_weights = True\n",
    "# Central value of signal region. Must only be given for using CWoLa with weights.\n",
    "cf_SR_center = 3.5\n",
    "# Make use of extra background (for supervised and idealized AD).\n",
    "cf_extra_bkg = True\n",
    "# Define a separate validation set to pick the classifier epochs.\n",
    "cf_separate_val_set = True\n",
    "# Save the tensorflow model after each epoch instead of saving predictions.\n",
    "cf_save_model = True\n",
    "# Skips the creation of the classifier dataset (loads existing files instead).\n",
    "cf_skip_create = False\n",
    "# Skips the training of the classifier (loads existing files instead).\n",
    "cf_skip_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56559910",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_creation_kwargs = {\n",
    "    'savedir': save_dir,\n",
    "    'datashift': datashift,\n",
    "    'data_dir': data_dir,\n",
    "    'random_shift': random_shift,\n",
    "    'config_file': DE_config_file,\n",
    "    'verbose': verbose,\n",
    "    'fiducial_cut': fiducial_cut,\n",
    "    'n_samples': cf_n_samples,\n",
    "    'realistic_conditional': cf_realistic_conditional,\n",
    "    'KDE_bandwidth': cf_KDE_bandwidth,\n",
    "    'oversampling': cf_oversampling,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'CWoLa': False,\n",
    "    'supervised': False,\n",
    "    'idealized_AD': True,\n",
    "    'no_logit': cf_no_logit,\n",
    "    'no_logit_trained': DE_no_logit,\n",
    "    'external_samples': cf_external_samples,\n",
    "    'SR_min': cf_SR_min,\n",
    "    'SR_max': cf_SR_max,\n",
    "    'extra_bkg': cf_extra_bkg,\n",
    "    'separate_val_set': cf_separate_val_set,\n",
    "    'ANODE_models': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f68e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data_handler import LHCORD_data_handler, sample_handler, mix_data_samples, plot_data_sample_comparison\n",
    "from density_estimator import DensityEstimator\n",
    "\n",
    "def create_data(**kwargs):\n",
    "\n",
    "    assert not ((not (kwargs['supervised'] or kwargs['idealized_AD'] or kwargs['CWoLa']) and\\\n",
    "                 kwargs['external_samples'] == \"\") and kwargs['ANODE_models'] == \"\"), (\n",
    "                     \"ANODE models need to be given unless CWoLa, supervised, idealized_AD or\"\n",
    "                     \" external sampling is used.\")\n",
    "\n",
    "    # selecting appropriate device\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    print(\"cuda available:\", CUDA)\n",
    "    device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "\n",
    "    # checking for data separation\n",
    "    data_files = os.listdir(kwargs['data_dir'])\n",
    "    if \"innerdata_val.npy\" in data_files:\n",
    "        finer_data_split = True\n",
    "    else:\n",
    "        finer_data_split = False\n",
    "\n",
    "    if finer_data_split:\n",
    "        innerdata_train_path = [os.path.join(kwargs['data_dir'], 'innerdata_train.npy')]\n",
    "        innerdata_val_path = [os.path.join(kwargs['data_dir'], 'innerdata_val.npy')]\n",
    "        innerdata_test_path = [os.path.join(kwargs['data_dir'], 'innerdata_test.npy')]\n",
    "        if \"innerdata_extrabkg_test.npy\" in data_files:\n",
    "            innerdata_test_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_test.npy'))\n",
    "        extrasig_path = None\n",
    "        if kwargs['supervised']:\n",
    "            innerdata_train_path = []\n",
    "            innerdata_val_path = []\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_val.npy'))\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy'))\n",
    "            extra_bkg = None\n",
    "        elif kwargs['idealized_AD']:\n",
    "            extra_bkg = [os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'),\n",
    "                         os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy')]\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "\n",
    "    else:\n",
    "        innerdata_train_path = os.path.join(kwargs['data_dir'], 'innerdata_train.npy')\n",
    "        extrasig_path = os.path.join(kwargs['data_dir'], 'innerdata_extrasig.npy')\n",
    "        if kwargs['extra_bkg']:\n",
    "            extra_bkg = os.path.join(kwargs['data_dir'], 'innerdata_extrabkg.npy')\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "        innerdata_val_path = None\n",
    "        innerdata_test_path = os.path.join(kwargs['data_dir'], 'innerdata_test.npy')\n",
    "\n",
    "    # data preprocessing\n",
    "    data = LHCORD_data_handler(innerdata_train_path,\n",
    "                               innerdata_test_path,\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_train.npy'),\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_test.npy'),\n",
    "                               extrasig_path,\n",
    "                               inner_extrabkg_path=extra_bkg,\n",
    "                               inner_val_path=innerdata_val_path,\n",
    "                               batch_size=256,\n",
    "                               device=device)\n",
    "    if kwargs['datashift'] != 0:\n",
    "        print(\"applying a datashift of\", kwargs['datashift'])\n",
    "        data.shift_data(kwargs['datashift'], constant_shift=False, random_shift=kwargs['random_shift'],\n",
    "                        shift_mj1=True, shift_dm=True, additional_shift=False)\n",
    "\n",
    "    if kwargs['CWoLa']:\n",
    "        # data preprocessing\n",
    "        samples = None\n",
    "        data.preprocess_CWoLa_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit'],\n",
    "                                   outer_range=(kwargs['SR_min']-0.2, kwargs['SR_max']+0.2))\n",
    "\n",
    "    else:\n",
    "        # data preprocessing\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'],\n",
    "                                   no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # model instantiation\n",
    "        if len(kwargs['external_samples']) > 0:\n",
    "            model_list = None\n",
    "            loaded_samples = [np.load(sample_path) for sample_path in kwargs['external_samples']]\n",
    "            external_sample = np.concatenate(loaded_samples)\n",
    "        else:\n",
    "            model_list = []\n",
    "            for model_path in kwargs['ANODE_models']:\n",
    "                anode = DensityEstimator(kwargs['config_file'],\n",
    "                                         eval_mode=True,\n",
    "                                         load_path=model_path,\n",
    "                                         device=device, verbose=kwargs['verbose'],\n",
    "                                         bound=kwargs['no_logit_trained'])\n",
    "                model_list.append(anode.model)\n",
    "            external_sample = None\n",
    "\n",
    "        # generate samples\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            uniform_cond = not kwargs['realistic_conditional']\n",
    "            samples = sample_handler(model_list, kwargs['n_samples'], data, cond_min=kwargs['SR_min'],\n",
    "                                     cond_max=kwargs['SR_max'], uniform_cond=uniform_cond,\n",
    "                                     external_sample=external_sample,\n",
    "                                     device=device, no_logit=kwargs['no_logit_trained'],\n",
    "                                     no_mean_shift=kwargs['no_logit_trained'],\n",
    "                                     KDE_bandwidth=kwargs['KDE_bandwidth'])\n",
    "        else:\n",
    "            samples = None\n",
    "\n",
    "        # redo data preprocessing if the classifier should not use logit but ANODE did\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # sample preprocessing\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            samples.preprocess_samples(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                       no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "\n",
    "    # sample mixing\n",
    "    X_train, y_train, X_test, y_test, X_extrasig, y_extrasig = mix_data_samples(\n",
    "        data, samples_handler=samples, oversampling=kwargs['oversampling'],\n",
    "        savedir=kwargs['savedir'], CWoLa=kwargs['CWoLa'], supervised=kwargs['supervised'],\n",
    "        idealized_AD=kwargs['idealized_AD'], separate_val_set=kwargs['separate_val_set'] or finer_data_split)\n",
    "\n",
    "    # sanity checks\n",
    "    if not kwargs['CWoLa'] and not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "        samples.sanity_check(savefig=os.path.join(kwargs['savedir'], \"sanity_check\"), suppress_show=True)\n",
    "        samples.sanity_check_after_cuts(savefig=os.path.join(kwargs['savedir'], \"sanity_check_cuts\"),\n",
    "                                        suppress_show=True)\n",
    "\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "        X_val = X_extrasig\n",
    "        if kwargs['supervised']:\n",
    "            y_train = X_train[:, -1]\n",
    "            y_test = X_test[:, -1]\n",
    "            y_val = X_val[:, -1]\n",
    "        else:\n",
    "            y_val = X_val[:, -2]\n",
    "        plot_data_sample_comparison(X_val, y_val, title=\"validation set\",\n",
    "                                    savefig=os.path.join(kwargs['savedir'],\n",
    "                                                         \"data_sample_comparison_val\"),\n",
    "                                    suppress_show=True)\n",
    "\n",
    "    plot_data_sample_comparison(X_train, y_train, title=\"training set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_train\"),\n",
    "                                suppress_show=True)\n",
    "    plot_data_sample_comparison(X_test, y_test, title=\"test set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_test\"),\n",
    "                                suppress_show=True)\n",
    "\n",
    "    print(\"number of training data =\", X_train.shape[0])\n",
    "    print(\"number of test data =\", X_test.shape[0])\n",
    "    if not kwargs['no_extra_signal']:\n",
    "        if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "            print(\"number of validation data =\", X_val.shape[0])\n",
    "        elif extrasig_path is not None:\n",
    "            print(\"number of extra signal data =\", X_extrasig.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e406cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: False\n",
      "Using extra background...\n",
      "number of training data = 196372\n",
      "number of test data = 359883\n"
     ]
    }
   ],
   "source": [
    "create_data(**data_creation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9f808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_kwargs = {\n",
    "    'config_file': cf_config_file,\n",
    "    'data_dir': save_dir,\n",
    "    'savedir': save_dir,\n",
    "    'verbose': verbose,\n",
    "    'epochs': cf_epochs,\n",
    "    'n_runs': cf_n_runs,\n",
    "    'batch_size': cf_batch_size,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'use_mjj': cf_use_mjj,\n",
    "    'supervised': False,\n",
    "    'use_class_weights': cf_oversampling or cf_use_class_weights,\n",
    "    'CWoLa': False,\n",
    "    'SR_center': cf_SR_center,\n",
    "    'save_model': cf_save_model,\n",
    "    'separate_val_set': cf_separate_val_set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599cdd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier_training_utils import train_n_models, plot_classifier_losses\n",
    "from evaluation_utils import minimum_val_loss_model_evaluation\n",
    "import matplotlib as mpl\n",
    "\n",
    "def train_classifier(**kwargs):\n",
    "\n",
    "    # loading the data\n",
    "    # TODO get rid of the y's since the information is fully included in X\n",
    "    X_train = np.load(os.path.join(kwargs['data_dir'], 'X_train.npy'))\n",
    "    X_test = np.load(os.path.join(kwargs['data_dir'], 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(kwargs['data_dir'], 'y_train.npy'))\n",
    "    y_test = np.load(os.path.join(kwargs['data_dir'], 'y_test.npy'))\n",
    "    if kwargs['no_extra_signal'] or kwargs['supervised']:\n",
    "        X_extrasig = None\n",
    "    else:\n",
    "        X_extrasig = np.load(os.path.join(kwargs['data_dir'], 'X_extrasig.npy'))\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set']:\n",
    "        X_val = np.load(os.path.join(kwargs['data_dir'], 'X_validation.npy'))        \n",
    "    else:\n",
    "        X_val = None\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        if not os.path.exists(kwargs['savedir']):\n",
    "            os.makedirs(kwargs['savedir'])\n",
    "        save_model = os.path.join(kwargs['savedir'], \"model\")\n",
    "    else:\n",
    "        save_model = None\n",
    "\n",
    "    # actual training\n",
    "    loss_matris, val_loss_matris = train_n_models(\n",
    "        kwargs['n_runs'], kwargs['config_file'], kwargs['epochs'], X_train, y_train, X_test, y_test,\n",
    "        X_extrasig=X_extrasig, X_val=X_val, use_mjj=kwargs['use_mjj'], batch_size=kwargs['batch_size'],\n",
    "        supervised=kwargs['supervised'], use_class_weights=kwargs['use_class_weights'],\n",
    "        CWoLa=kwargs['CWoLa'], SR_center=kwargs['SR_center'], verbose=kwargs['verbose'],\n",
    "        savedir=kwargs['savedir'], save_model=save_model)\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        minimum_val_loss_model_evaluation(kwargs['data_dir'], kwargs['savedir'], n_epochs=10,\n",
    "                                use_mjj=kwargs['use_mjj'], extra_signal=not kwargs['no_extra_signal'])\n",
    "\n",
    "    for i in range(loss_matris.shape[0]):\n",
    "        plot_classifier_losses(\n",
    "            loss_matris[i], val_loss_matris[i],\n",
    "            savefig=save_model+\"_run\"+str(i)+\"_loss_plot\",\n",
    "            suppress_show=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17164dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model nr 0...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932780742645264\n",
      "validation loss: 0.6931359171867371\n",
      "training epoch nr 1\n",
      "training loss: 0.6931953430175781\n",
      "validation loss: 0.6930888891220093\n",
      "training epoch nr 2\n",
      "training loss: 0.6930838227272034\n",
      "validation loss: 0.6933363676071167\n",
      "training epoch nr 3\n",
      "training loss: 0.6931409239768982\n",
      "validation loss: 0.6930994987487793\n",
      "training epoch nr 4\n",
      "training loss: 0.6930557489395142\n",
      "validation loss: 0.6930897235870361\n",
      "training epoch nr 5\n",
      "training loss: 0.6930726170539856\n",
      "validation loss: 0.6930766105651855\n",
      "training epoch nr 6\n",
      "training loss: 0.6930475831031799\n",
      "validation loss: 0.6930747628211975\n",
      "training epoch nr 7\n",
      "training loss: 0.6930184364318848\n",
      "validation loss: 0.6932281851768494\n",
      "training epoch nr 8\n",
      "training loss: 0.6930239200592041\n",
      "validation loss: 0.6930232048034668\n",
      "training epoch nr 9\n",
      "training loss: 0.6929574608802795\n",
      "validation loss: 0.6930456757545471\n",
      "training epoch nr 10\n",
      "training loss: 0.6928954124450684\n",
      "validation loss: 0.6930840015411377\n",
      "training epoch nr 11\n",
      "training loss: 0.6929289698600769\n",
      "validation loss: 0.693065345287323\n",
      "training epoch nr 12\n",
      "training loss: 0.6928400993347168\n",
      "validation loss: 0.6929420828819275\n",
      "training epoch nr 13\n",
      "training loss: 0.6927357316017151\n",
      "validation loss: 0.6930056214332581\n",
      "training epoch nr 14\n",
      "training loss: 0.6927334070205688\n",
      "validation loss: 0.692963182926178\n",
      "training epoch nr 15\n",
      "training loss: 0.6927501559257507\n",
      "validation loss: 0.6931688189506531\n",
      "training epoch nr 16\n",
      "training loss: 0.6927213668823242\n",
      "validation loss: 0.6930726170539856\n",
      "training epoch nr 17\n",
      "training loss: 0.692782998085022\n",
      "validation loss: 0.6930704116821289\n",
      "training epoch nr 18\n",
      "training loss: 0.6927317380905151\n",
      "validation loss: 0.6932149529457092\n",
      "training epoch nr 19\n",
      "training loss: 0.6926253437995911\n",
      "validation loss: 0.6931098103523254\n",
      "training epoch nr 20\n",
      "training loss: 0.69260573387146\n",
      "validation loss: 0.6930014491081238\n",
      "training epoch nr 21\n",
      "training loss: 0.6925898790359497\n",
      "validation loss: 0.6930559277534485\n",
      "training epoch nr 22\n",
      "training loss: 0.6925716996192932\n",
      "validation loss: 0.69314044713974\n",
      "training epoch nr 23\n",
      "training loss: 0.6925663352012634\n",
      "validation loss: 0.6930240392684937\n",
      "training epoch nr 24\n",
      "training loss: 0.6925382614135742\n",
      "validation loss: 0.6932108998298645\n",
      "training epoch nr 25\n",
      "training loss: 0.6925137042999268\n",
      "validation loss: 0.6935555338859558\n",
      "training epoch nr 26\n",
      "training loss: 0.692534863948822\n",
      "validation loss: 0.6932705640792847\n",
      "training epoch nr 27\n",
      "training loss: 0.6924037337303162\n",
      "validation loss: 0.6931629776954651\n",
      "training epoch nr 28\n",
      "training loss: 0.6924851536750793\n",
      "validation loss: 0.6931768655776978\n",
      "training epoch nr 29\n",
      "training loss: 0.692421555519104\n",
      "validation loss: 0.6932557821273804\n",
      "training epoch nr 30\n",
      "training loss: 0.692343533039093\n",
      "validation loss: 0.6933472752571106\n",
      "training epoch nr 31\n",
      "training loss: 0.6923057436943054\n",
      "validation loss: 0.6933486461639404\n",
      "training epoch nr 32\n",
      "training loss: 0.6922038197517395\n",
      "validation loss: 0.6932998299598694\n",
      "training epoch nr 33\n",
      "training loss: 0.6921996474266052\n",
      "validation loss: 0.6932648420333862\n",
      "training epoch nr 34\n",
      "training loss: 0.6921311020851135\n",
      "validation loss: 0.6933386921882629\n",
      "training epoch nr 35\n",
      "training loss: 0.6921288967132568\n",
      "validation loss: 0.693428635597229\n",
      "training epoch nr 36\n",
      "training loss: 0.6920639276504517\n",
      "validation loss: 0.6934183835983276\n",
      "training epoch nr 37\n",
      "training loss: 0.6920394897460938\n",
      "validation loss: 0.6934378147125244\n",
      "training epoch nr 38\n",
      "training loss: 0.6919758915901184\n",
      "validation loss: 0.6932372450828552\n",
      "training epoch nr 39\n",
      "training loss: 0.6919461488723755\n",
      "validation loss: 0.6937034130096436\n",
      "training epoch nr 40\n",
      "training loss: 0.6919708847999573\n",
      "validation loss: 0.6937472820281982\n",
      "training epoch nr 41\n",
      "training loss: 0.691943347454071\n",
      "validation loss: 0.6938627362251282\n",
      "training epoch nr 42\n",
      "training loss: 0.6918520927429199\n",
      "validation loss: 0.6938919425010681\n",
      "training epoch nr 43\n",
      "training loss: 0.6919381618499756\n",
      "validation loss: 0.6935954093933105\n",
      "training epoch nr 44\n",
      "training loss: 0.6917445659637451\n",
      "validation loss: 0.693565845489502\n",
      "training epoch nr 45\n",
      "training loss: 0.6917225122451782\n",
      "validation loss: 0.6936836838722229\n",
      "training epoch nr 46\n",
      "training loss: 0.6917905211448669\n",
      "validation loss: 0.6937206387519836\n",
      "training epoch nr 47\n",
      "training loss: 0.691768229007721\n",
      "validation loss: 0.6937781572341919\n",
      "training epoch nr 48\n",
      "training loss: 0.6915971636772156\n",
      "validation loss: 0.6937652826309204\n",
      "training epoch nr 49\n",
      "training loss: 0.6916863918304443\n",
      "validation loss: 0.6940928101539612\n",
      "training epoch nr 50\n",
      "training loss: 0.6915358901023865\n",
      "validation loss: 0.6940836906433105\n",
      "training epoch nr 51\n",
      "training loss: 0.6915848851203918\n",
      "validation loss: 0.6935878992080688\n",
      "training epoch nr 52\n",
      "training loss: 0.6916438937187195\n",
      "validation loss: 0.6938840746879578\n",
      "training epoch nr 53\n",
      "training loss: 0.691591203212738\n",
      "validation loss: 0.6938508152961731\n",
      "training epoch nr 54\n",
      "training loss: 0.6915105581283569\n",
      "validation loss: 0.6942110657691956\n",
      "training epoch nr 55\n",
      "training loss: 0.6914450526237488\n",
      "validation loss: 0.6937719583511353\n",
      "training epoch nr 56\n",
      "training loss: 0.6913459897041321\n",
      "validation loss: 0.6941375136375427\n",
      "training epoch nr 57\n",
      "training loss: 0.6914586424827576\n",
      "validation loss: 0.6939423084259033\n",
      "training epoch nr 58\n",
      "training loss: 0.6914181709289551\n",
      "validation loss: 0.69384765625\n",
      "training epoch nr 59\n",
      "training loss: 0.6914002895355225\n",
      "validation loss: 0.6939200162887573\n",
      "training epoch nr 60\n",
      "training loss: 0.6913424730300903\n",
      "validation loss: 0.6939857006072998\n",
      "training epoch nr 61\n",
      "training loss: 0.6913529634475708\n",
      "validation loss: 0.6944627165794373\n",
      "training epoch nr 62\n",
      "training loss: 0.6913931965827942\n",
      "validation loss: 0.6941097974777222\n",
      "training epoch nr 63\n",
      "training loss: 0.6912797093391418\n",
      "validation loss: 0.6943249702453613\n",
      "training epoch nr 64\n",
      "training loss: 0.6912087798118591\n",
      "validation loss: 0.6942881345748901\n",
      "training epoch nr 65\n",
      "training loss: 0.691233217716217\n",
      "validation loss: 0.6942909955978394\n",
      "training epoch nr 66\n",
      "training loss: 0.6910054683685303\n",
      "validation loss: 0.6945748329162598\n",
      "training epoch nr 67\n",
      "training loss: 0.6911916732788086\n",
      "validation loss: 0.6943684816360474\n",
      "training epoch nr 68\n",
      "training loss: 0.6910771727561951\n",
      "validation loss: 0.6946215033531189\n",
      "training epoch nr 69\n",
      "training loss: 0.6910187005996704\n",
      "validation loss: 0.6944912672042847\n",
      "training epoch nr 70\n",
      "training loss: 0.6910169720649719\n",
      "validation loss: 0.6945130228996277\n",
      "training epoch nr 71\n",
      "training loss: 0.6910017132759094\n",
      "validation loss: 0.6944743990898132\n",
      "training epoch nr 72\n",
      "training loss: 0.6908722519874573\n",
      "validation loss: 0.6943618059158325\n",
      "training epoch nr 73\n",
      "training loss: 0.6909322738647461\n",
      "validation loss: 0.6945215463638306\n",
      "training epoch nr 74\n",
      "training loss: 0.6908358335494995\n",
      "validation loss: 0.6946773529052734\n",
      "training epoch nr 75\n",
      "training loss: 0.6909143328666687\n",
      "validation loss: 0.6943945288658142\n",
      "training epoch nr 76\n",
      "training loss: 0.6907778978347778\n",
      "validation loss: 0.6944568753242493\n",
      "training epoch nr 77\n",
      "training loss: 0.6907578706741333\n",
      "validation loss: 0.6943164467811584\n",
      "training epoch nr 78\n",
      "training loss: 0.6908677220344543\n",
      "validation loss: 0.6952765583992004\n",
      "training epoch nr 79\n",
      "training loss: 0.6907342672348022\n",
      "validation loss: 0.6942785382270813\n",
      "training epoch nr 80\n",
      "training loss: 0.6906643509864807\n",
      "validation loss: 0.6946578025817871\n",
      "training epoch nr 81\n",
      "training loss: 0.6908008456230164\n",
      "validation loss: 0.6951399445533752\n",
      "training epoch nr 82\n",
      "training loss: 0.6905966997146606\n",
      "validation loss: 0.6943795084953308\n",
      "training epoch nr 83\n",
      "training loss: 0.6907318234443665\n",
      "validation loss: 0.6948040723800659\n",
      "training epoch nr 84\n",
      "training loss: 0.6905972957611084\n",
      "validation loss: 0.6945735812187195\n",
      "training epoch nr 85\n",
      "training loss: 0.6904294490814209\n",
      "validation loss: 0.6949001550674438\n",
      "training epoch nr 86\n",
      "training loss: 0.6904996633529663\n",
      "validation loss: 0.6945664882659912\n",
      "training epoch nr 87\n",
      "training loss: 0.6904981732368469\n",
      "validation loss: 0.694685697555542\n",
      "training epoch nr 88\n",
      "training loss: 0.6905038952827454\n",
      "validation loss: 0.6950058341026306\n",
      "training epoch nr 89\n",
      "training loss: 0.6904025077819824\n",
      "validation loss: 0.6950005292892456\n",
      "training epoch nr 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.69047611951828\n",
      "validation loss: 0.6955103278160095\n",
      "training epoch nr 91\n",
      "training loss: 0.6904341578483582\n",
      "validation loss: 0.6953031420707703\n",
      "training epoch nr 92\n",
      "training loss: 0.6903659105300903\n",
      "validation loss: 0.69511878490448\n",
      "training epoch nr 93\n",
      "training loss: 0.6902026534080505\n",
      "validation loss: 0.6952337622642517\n",
      "training epoch nr 94\n",
      "training loss: 0.6904141902923584\n",
      "validation loss: 0.6950365304946899\n",
      "training epoch nr 95\n",
      "training loss: 0.6902703046798706\n",
      "validation loss: 0.6953434944152832\n",
      "training epoch nr 96\n",
      "training loss: 0.6901812553405762\n",
      "validation loss: 0.6951842308044434\n",
      "training epoch nr 97\n",
      "training loss: 0.6902047991752625\n",
      "validation loss: 0.6950226426124573\n",
      "training epoch nr 98\n",
      "training loss: 0.6900937557220459\n",
      "validation loss: 0.6955899000167847\n",
      "training epoch nr 99\n",
      "training loss: 0.6901094317436218\n",
      "validation loss: 0.6949779987335205\n",
      "Training model nr 1...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932125687599182\n",
      "validation loss: 0.693444550037384\n",
      "training epoch nr 1\n",
      "training loss: 0.6931875348091125\n",
      "validation loss: 0.6931291222572327\n",
      "training epoch nr 2\n",
      "training loss: 0.6931651830673218\n",
      "validation loss: 0.6931252479553223\n",
      "training epoch nr 3\n",
      "training loss: 0.6930209994316101\n",
      "validation loss: 0.6931668519973755\n",
      "training epoch nr 4\n",
      "training loss: 0.6931354999542236\n",
      "validation loss: 0.6931376457214355\n",
      "training epoch nr 5\n",
      "training loss: 0.6930983066558838\n",
      "validation loss: 0.693078339099884\n",
      "training epoch nr 6\n",
      "training loss: 0.6930416226387024\n",
      "validation loss: 0.6930856704711914\n",
      "training epoch nr 7\n",
      "training loss: 0.6929989457130432\n",
      "validation loss: 0.6930010318756104\n",
      "training epoch nr 8\n",
      "training loss: 0.6929305195808411\n",
      "validation loss: 0.6929983496665955\n",
      "training epoch nr 9\n",
      "training loss: 0.6930252313613892\n",
      "validation loss: 0.6929614543914795\n",
      "training epoch nr 10\n",
      "training loss: 0.6928937435150146\n",
      "validation loss: 0.6930178999900818\n",
      "training epoch nr 11\n",
      "training loss: 0.6929656267166138\n",
      "validation loss: 0.6929463744163513\n",
      "training epoch nr 12\n",
      "training loss: 0.692866861820221\n",
      "validation loss: 0.6929857730865479\n",
      "training epoch nr 13\n",
      "training loss: 0.6929429173469543\n",
      "validation loss: 0.6929103136062622\n",
      "training epoch nr 14\n",
      "training loss: 0.6928002238273621\n",
      "validation loss: 0.6929744482040405\n",
      "training epoch nr 15\n",
      "training loss: 0.6927750110626221\n",
      "validation loss: 0.692966103553772\n",
      "training epoch nr 16\n",
      "training loss: 0.6927470564842224\n",
      "validation loss: 0.6929495930671692\n",
      "training epoch nr 17\n",
      "training loss: 0.6928083300590515\n",
      "validation loss: 0.6929576396942139\n",
      "training epoch nr 18\n",
      "training loss: 0.6927521824836731\n",
      "validation loss: 0.6928503513336182\n",
      "training epoch nr 19\n",
      "training loss: 0.6927427649497986\n",
      "validation loss: 0.6930050849914551\n",
      "training epoch nr 20\n",
      "training loss: 0.6927663087844849\n",
      "validation loss: 0.6929939389228821\n",
      "training epoch nr 21\n",
      "training loss: 0.692776620388031\n",
      "validation loss: 0.692959189414978\n",
      "training epoch nr 22\n",
      "training loss: 0.6926886439323425\n",
      "validation loss: 0.692919135093689\n",
      "training epoch nr 23\n",
      "training loss: 0.6925865411758423\n",
      "validation loss: 0.692887008190155\n",
      "training epoch nr 24\n",
      "training loss: 0.6926174163818359\n",
      "validation loss: 0.6928620934486389\n",
      "training epoch nr 25\n",
      "training loss: 0.692620575428009\n",
      "validation loss: 0.6931112408638\n",
      "training epoch nr 26\n",
      "training loss: 0.6925486922264099\n",
      "validation loss: 0.6929126977920532\n",
      "training epoch nr 27\n",
      "training loss: 0.6926239132881165\n",
      "validation loss: 0.6929003596305847\n",
      "training epoch nr 28\n",
      "training loss: 0.692573606967926\n",
      "validation loss: 0.6930529475212097\n",
      "training epoch nr 29\n",
      "training loss: 0.6925990581512451\n",
      "validation loss: 0.6929725408554077\n",
      "training epoch nr 30\n",
      "training loss: 0.6925234794616699\n",
      "validation loss: 0.692919135093689\n",
      "training epoch nr 31\n",
      "training loss: 0.6924404501914978\n",
      "validation loss: 0.6931007504463196\n",
      "training epoch nr 32\n",
      "training loss: 0.6923897862434387\n",
      "validation loss: 0.6931359171867371\n",
      "training epoch nr 33\n",
      "training loss: 0.6924583315849304\n",
      "validation loss: 0.693135142326355\n",
      "training epoch nr 34\n",
      "training loss: 0.692421555519104\n",
      "validation loss: 0.693080723285675\n",
      "training epoch nr 35\n",
      "training loss: 0.6924300789833069\n",
      "validation loss: 0.6930946111679077\n",
      "training epoch nr 36\n",
      "training loss: 0.692384660243988\n",
      "validation loss: 0.6932742595672607\n",
      "training epoch nr 37\n",
      "training loss: 0.6923481225967407\n",
      "validation loss: 0.6935678720474243\n",
      "training epoch nr 38\n",
      "training loss: 0.6923007965087891\n",
      "validation loss: 0.6931886076927185\n",
      "training epoch nr 39\n",
      "training loss: 0.6922643780708313\n",
      "validation loss: 0.693237841129303\n",
      "training epoch nr 40\n",
      "training loss: 0.6922892928123474\n",
      "validation loss: 0.6934638023376465\n",
      "training epoch nr 41\n",
      "training loss: 0.6921253204345703\n",
      "validation loss: 0.6932470798492432\n",
      "training epoch nr 42\n",
      "training loss: 0.6921411156654358\n",
      "validation loss: 0.693345844745636\n",
      "training epoch nr 43\n",
      "training loss: 0.6921619772911072\n",
      "validation loss: 0.693189263343811\n",
      "training epoch nr 44\n",
      "training loss: 0.6921274662017822\n",
      "validation loss: 0.6932094693183899\n",
      "training epoch nr 45\n",
      "training loss: 0.6922021508216858\n",
      "validation loss: 0.6935241222381592\n",
      "training epoch nr 46\n",
      "training loss: 0.6921719908714294\n",
      "validation loss: 0.6934933066368103\n",
      "training epoch nr 47\n",
      "training loss: 0.6920003890991211\n",
      "validation loss: 0.6935272812843323\n",
      "training epoch nr 48\n",
      "training loss: 0.6920979619026184\n",
      "validation loss: 0.6937742829322815\n",
      "training epoch nr 49\n",
      "training loss: 0.6920313239097595\n",
      "validation loss: 0.693458616733551\n",
      "training epoch nr 50\n",
      "training loss: 0.6918198466300964\n",
      "validation loss: 0.6936399936676025\n",
      "training epoch nr 51\n",
      "training loss: 0.6919309496879578\n",
      "validation loss: 0.6937917470932007\n",
      "training epoch nr 52\n",
      "training loss: 0.6917992830276489\n",
      "validation loss: 0.6935343146324158\n",
      "training epoch nr 53\n",
      "training loss: 0.6917702555656433\n",
      "validation loss: 0.6936730742454529\n",
      "training epoch nr 54\n",
      "training loss: 0.6917724013328552\n",
      "validation loss: 0.6935656070709229\n",
      "training epoch nr 55\n",
      "training loss: 0.6917420625686646\n",
      "validation loss: 0.6936582326889038\n",
      "training epoch nr 56\n",
      "training loss: 0.6917471885681152\n",
      "validation loss: 0.6938740015029907\n",
      "training epoch nr 57\n",
      "training loss: 0.6915850043296814\n",
      "validation loss: 0.6941231489181519\n",
      "training epoch nr 58\n",
      "training loss: 0.6915982961654663\n",
      "validation loss: 0.6938583850860596\n",
      "training epoch nr 59\n",
      "training loss: 0.6915332674980164\n",
      "validation loss: 0.6938374042510986\n",
      "training epoch nr 60\n",
      "training loss: 0.691514790058136\n",
      "validation loss: 0.6937127709388733\n",
      "training epoch nr 61\n",
      "training loss: 0.6914147734642029\n",
      "validation loss: 0.6941823959350586\n",
      "training epoch nr 62\n",
      "training loss: 0.6914329528808594\n",
      "validation loss: 0.6940523386001587\n",
      "training epoch nr 63\n",
      "training loss: 0.6914281249046326\n",
      "validation loss: 0.694139301776886\n",
      "training epoch nr 64\n",
      "training loss: 0.6912972331047058\n",
      "validation loss: 0.6939504146575928\n",
      "training epoch nr 65\n",
      "training loss: 0.6913051009178162\n",
      "validation loss: 0.694487988948822\n",
      "training epoch nr 66\n",
      "training loss: 0.6912384629249573\n",
      "validation loss: 0.6942726969718933\n",
      "training epoch nr 67\n",
      "training loss: 0.6912420988082886\n",
      "validation loss: 0.6940242648124695\n",
      "training epoch nr 68\n",
      "training loss: 0.6911719441413879\n",
      "validation loss: 0.6939277648925781\n",
      "training epoch nr 69\n",
      "training loss: 0.6911302208900452\n",
      "validation loss: 0.6941084861755371\n",
      "training epoch nr 70\n",
      "training loss: 0.6911376714706421\n",
      "validation loss: 0.6941220164299011\n",
      "training epoch nr 71\n",
      "training loss: 0.6911337971687317\n",
      "validation loss: 0.6946331262588501\n",
      "training epoch nr 72\n",
      "training loss: 0.6910619139671326\n",
      "validation loss: 0.6940216422080994\n",
      "training epoch nr 73\n",
      "training loss: 0.6909627914428711\n",
      "validation loss: 0.6943627595901489\n",
      "training epoch nr 74\n",
      "training loss: 0.690925121307373\n",
      "validation loss: 0.6945388317108154\n",
      "training epoch nr 75\n",
      "training loss: 0.6908857822418213\n",
      "validation loss: 0.6944282054901123\n",
      "training epoch nr 76\n",
      "training loss: 0.6909297108650208\n",
      "validation loss: 0.6943188905715942\n",
      "training epoch nr 77\n",
      "training loss: 0.6908531785011292\n",
      "validation loss: 0.6945381164550781\n",
      "training epoch nr 78\n",
      "training loss: 0.6907214522361755\n",
      "validation loss: 0.695044755935669\n",
      "training epoch nr 79\n",
      "training loss: 0.6908654570579529\n",
      "validation loss: 0.6947109699249268\n",
      "training epoch nr 80\n",
      "training loss: 0.6907927393913269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6946978569030762\n",
      "training epoch nr 81\n",
      "training loss: 0.6906936168670654\n",
      "validation loss: 0.6947641968727112\n",
      "training epoch nr 82\n",
      "training loss: 0.6906795501708984\n",
      "validation loss: 0.6949481964111328\n",
      "training epoch nr 83\n",
      "training loss: 0.6906285881996155\n",
      "validation loss: 0.6949411034584045\n",
      "training epoch nr 84\n",
      "training loss: 0.6905679106712341\n",
      "validation loss: 0.6949799060821533\n",
      "training epoch nr 85\n",
      "training loss: 0.6905386447906494\n",
      "validation loss: 0.6950817108154297\n",
      "training epoch nr 86\n",
      "training loss: 0.6906304955482483\n",
      "validation loss: 0.6955601572990417\n",
      "training epoch nr 87\n",
      "training loss: 0.6905013918876648\n",
      "validation loss: 0.6947298049926758\n",
      "training epoch nr 88\n",
      "training loss: 0.6904920339584351\n",
      "validation loss: 0.695368230342865\n",
      "training epoch nr 89\n",
      "training loss: 0.6903680562973022\n",
      "validation loss: 0.6955840587615967\n",
      "training epoch nr 90\n",
      "training loss: 0.6903074383735657\n",
      "validation loss: 0.6953136920928955\n",
      "training epoch nr 91\n",
      "training loss: 0.6904801726341248\n",
      "validation loss: 0.6950505971908569\n",
      "training epoch nr 92\n",
      "training loss: 0.6903901696205139\n",
      "validation loss: 0.6952573657035828\n",
      "training epoch nr 93\n",
      "training loss: 0.6903492212295532\n",
      "validation loss: 0.6955171227455139\n",
      "training epoch nr 94\n",
      "training loss: 0.6902105808258057\n",
      "validation loss: 0.6952953934669495\n",
      "training epoch nr 95\n",
      "training loss: 0.6902157664299011\n",
      "validation loss: 0.695022702217102\n",
      "training epoch nr 96\n",
      "training loss: 0.6902097463607788\n",
      "validation loss: 0.6954695582389832\n",
      "training epoch nr 97\n",
      "training loss: 0.6902753114700317\n",
      "validation loss: 0.6951109766960144\n",
      "training epoch nr 98\n",
      "training loss: 0.6901049017906189\n",
      "validation loss: 0.6958670020103455\n",
      "training epoch nr 99\n",
      "training loss: 0.6901605129241943\n",
      "validation loss: 0.6950005292892456\n",
      "Training model nr 2...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932675242424011\n",
      "validation loss: 0.6931188106536865\n",
      "training epoch nr 1\n",
      "training loss: 0.6931325793266296\n",
      "validation loss: 0.6931300759315491\n",
      "training epoch nr 2\n",
      "training loss: 0.693082332611084\n",
      "validation loss: 0.6931576132774353\n",
      "training epoch nr 3\n",
      "training loss: 0.6930729150772095\n",
      "validation loss: 0.6931694149971008\n",
      "training epoch nr 4\n",
      "training loss: 0.6930638551712036\n",
      "validation loss: 0.6931816339492798\n",
      "training epoch nr 5\n",
      "training loss: 0.6930634379386902\n",
      "validation loss: 0.693119466304779\n",
      "training epoch nr 6\n",
      "training loss: 0.6930093169212341\n",
      "validation loss: 0.6931343078613281\n",
      "training epoch nr 7\n",
      "training loss: 0.6930360198020935\n",
      "validation loss: 0.6930451989173889\n",
      "training epoch nr 8\n",
      "training loss: 0.6928980350494385\n",
      "validation loss: 0.6930312514305115\n",
      "training epoch nr 9\n",
      "training loss: 0.6929419636726379\n",
      "validation loss: 0.6930714249610901\n",
      "training epoch nr 10\n",
      "training loss: 0.6929351091384888\n",
      "validation loss: 0.6930535435676575\n",
      "training epoch nr 11\n",
      "training loss: 0.6929078102111816\n",
      "validation loss: 0.6930021643638611\n",
      "training epoch nr 12\n",
      "training loss: 0.6929548382759094\n",
      "validation loss: 0.692984402179718\n",
      "training epoch nr 13\n",
      "training loss: 0.6929208636283875\n",
      "validation loss: 0.6930367946624756\n",
      "training epoch nr 14\n",
      "training loss: 0.6928446888923645\n",
      "validation loss: 0.6929969787597656\n",
      "training epoch nr 15\n",
      "training loss: 0.6927927732467651\n",
      "validation loss: 0.6930369734764099\n",
      "training epoch nr 16\n",
      "training loss: 0.6928178071975708\n",
      "validation loss: 0.6929210424423218\n",
      "training epoch nr 17\n",
      "training loss: 0.6927125453948975\n",
      "validation loss: 0.6929702758789062\n",
      "training epoch nr 18\n",
      "training loss: 0.6926535964012146\n",
      "validation loss: 0.6929447054862976\n",
      "training epoch nr 19\n",
      "training loss: 0.6927344799041748\n",
      "validation loss: 0.6930281519889832\n",
      "training epoch nr 20\n",
      "training loss: 0.6927186250686646\n",
      "validation loss: 0.6929932236671448\n",
      "training epoch nr 21\n",
      "training loss: 0.6926052570343018\n",
      "validation loss: 0.6931985020637512\n",
      "training epoch nr 22\n",
      "training loss: 0.6925841569900513\n",
      "validation loss: 0.6930907368659973\n",
      "training epoch nr 23\n",
      "training loss: 0.6926317811012268\n",
      "validation loss: 0.6930362582206726\n",
      "training epoch nr 24\n",
      "training loss: 0.6925672292709351\n",
      "validation loss: 0.6928925514221191\n",
      "training epoch nr 25\n",
      "training loss: 0.6925727128982544\n",
      "validation loss: 0.6929441094398499\n",
      "training epoch nr 26\n",
      "training loss: 0.6925812363624573\n",
      "validation loss: 0.6931495666503906\n",
      "training epoch nr 27\n",
      "training loss: 0.6924973130226135\n",
      "validation loss: 0.6931361556053162\n",
      "training epoch nr 28\n",
      "training loss: 0.6925575137138367\n",
      "validation loss: 0.6930855512619019\n",
      "training epoch nr 29\n",
      "training loss: 0.6924574971199036\n",
      "validation loss: 0.6930812001228333\n",
      "training epoch nr 30\n",
      "training loss: 0.6924634575843811\n",
      "validation loss: 0.6933060884475708\n",
      "training epoch nr 31\n",
      "training loss: 0.6924625039100647\n",
      "validation loss: 0.693199634552002\n",
      "training epoch nr 32\n",
      "training loss: 0.6924341917037964\n",
      "validation loss: 0.6933159828186035\n",
      "training epoch nr 33\n",
      "training loss: 0.6923314332962036\n",
      "validation loss: 0.693179726600647\n",
      "training epoch nr 34\n",
      "training loss: 0.6924214363098145\n",
      "validation loss: 0.6932242512702942\n",
      "training epoch nr 35\n",
      "training loss: 0.6923555135726929\n",
      "validation loss: 0.6932613253593445\n",
      "training epoch nr 36\n",
      "training loss: 0.6923166513442993\n",
      "validation loss: 0.693135142326355\n",
      "training epoch nr 37\n",
      "training loss: 0.6922643184661865\n",
      "validation loss: 0.6934098601341248\n",
      "training epoch nr 38\n",
      "training loss: 0.6922740340232849\n",
      "validation loss: 0.6935100555419922\n",
      "training epoch nr 39\n",
      "training loss: 0.6922089457511902\n",
      "validation loss: 0.6933984756469727\n",
      "training epoch nr 40\n",
      "training loss: 0.6921947598457336\n",
      "validation loss: 0.6934455037117004\n",
      "training epoch nr 41\n",
      "training loss: 0.6921027302742004\n",
      "validation loss: 0.6937677264213562\n",
      "training epoch nr 42\n",
      "training loss: 0.6920456290245056\n",
      "validation loss: 0.6935021877288818\n",
      "training epoch nr 43\n",
      "training loss: 0.6920587420463562\n",
      "validation loss: 0.6935219764709473\n",
      "training epoch nr 44\n",
      "training loss: 0.6921070218086243\n",
      "validation loss: 0.6933910846710205\n",
      "training epoch nr 45\n",
      "training loss: 0.6920204162597656\n",
      "validation loss: 0.6934912204742432\n",
      "training epoch nr 46\n",
      "training loss: 0.6919244527816772\n",
      "validation loss: 0.6934296488761902\n",
      "training epoch nr 47\n",
      "training loss: 0.6919482946395874\n",
      "validation loss: 0.693492591381073\n",
      "training epoch nr 48\n",
      "training loss: 0.6919092535972595\n",
      "validation loss: 0.6936851739883423\n",
      "training epoch nr 49\n",
      "training loss: 0.6917626261711121\n",
      "validation loss: 0.6936019062995911\n",
      "training epoch nr 50\n",
      "training loss: 0.6918721795082092\n",
      "validation loss: 0.693488597869873\n",
      "training epoch nr 51\n",
      "training loss: 0.6917333602905273\n",
      "validation loss: 0.6938151121139526\n",
      "training epoch nr 52\n",
      "training loss: 0.6917851567268372\n",
      "validation loss: 0.6936661601066589\n",
      "training epoch nr 53\n",
      "training loss: 0.6917827129364014\n",
      "validation loss: 0.6935498118400574\n",
      "training epoch nr 54\n",
      "training loss: 0.6918190121650696\n",
      "validation loss: 0.693604052066803\n",
      "training epoch nr 55\n",
      "training loss: 0.6917453408241272\n",
      "validation loss: 0.6937025785446167\n",
      "training epoch nr 56\n",
      "training loss: 0.6916558146476746\n",
      "validation loss: 0.693659245967865\n",
      "training epoch nr 57\n",
      "training loss: 0.6916694641113281\n",
      "validation loss: 0.6936206817626953\n",
      "training epoch nr 58\n",
      "training loss: 0.6915990114212036\n",
      "validation loss: 0.6938608288764954\n",
      "training epoch nr 59\n",
      "training loss: 0.6915985345840454\n",
      "validation loss: 0.6943110227584839\n",
      "training epoch nr 60\n",
      "training loss: 0.6914111971855164\n",
      "validation loss: 0.6939070820808411\n",
      "training epoch nr 61\n",
      "training loss: 0.6914116740226746\n",
      "validation loss: 0.6940804123878479\n",
      "training epoch nr 62\n",
      "training loss: 0.6914185881614685\n",
      "validation loss: 0.6942949891090393\n",
      "training epoch nr 63\n",
      "training loss: 0.6913584470748901\n",
      "validation loss: 0.694407045841217\n",
      "training epoch nr 64\n",
      "training loss: 0.69142746925354\n",
      "validation loss: 0.6940386891365051\n",
      "training epoch nr 65\n",
      "training loss: 0.6912640333175659\n",
      "validation loss: 0.6942275166511536\n",
      "training epoch nr 66\n",
      "training loss: 0.6912729740142822\n",
      "validation loss: 0.6939709186553955\n",
      "training epoch nr 67\n",
      "training loss: 0.6913048028945923\n",
      "validation loss: 0.6940510272979736\n",
      "training epoch nr 68\n",
      "training loss: 0.6912494897842407\n",
      "validation loss: 0.6940758228302002\n",
      "training epoch nr 69\n",
      "training loss: 0.6912935376167297\n",
      "validation loss: 0.6942138075828552\n",
      "training epoch nr 70\n",
      "training loss: 0.6911759376525879\n",
      "validation loss: 0.6942185759544373\n",
      "training epoch nr 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6911621689796448\n",
      "validation loss: 0.6944054961204529\n",
      "training epoch nr 72\n",
      "training loss: 0.691191554069519\n",
      "validation loss: 0.6944600939750671\n",
      "training epoch nr 73\n",
      "training loss: 0.6912329792976379\n",
      "validation loss: 0.6939171552658081\n",
      "training epoch nr 74\n",
      "training loss: 0.6911283731460571\n",
      "validation loss: 0.694666862487793\n",
      "training epoch nr 75\n",
      "training loss: 0.6910624504089355\n",
      "validation loss: 0.6944803595542908\n",
      "training epoch nr 76\n",
      "training loss: 0.6911155581474304\n",
      "validation loss: 0.6941702961921692\n",
      "training epoch nr 77\n",
      "training loss: 0.6912094354629517\n",
      "validation loss: 0.6943863034248352\n",
      "training epoch nr 78\n",
      "training loss: 0.6910942196846008\n",
      "validation loss: 0.6946521997451782\n",
      "training epoch nr 79\n",
      "training loss: 0.6911026835441589\n",
      "validation loss: 0.6941227912902832\n",
      "training epoch nr 80\n",
      "training loss: 0.6909779906272888\n",
      "validation loss: 0.6943683624267578\n",
      "training epoch nr 81\n",
      "training loss: 0.6910178661346436\n",
      "validation loss: 0.6942960023880005\n",
      "training epoch nr 82\n",
      "training loss: 0.6909124851226807\n",
      "validation loss: 0.6945855617523193\n",
      "training epoch nr 83\n",
      "training loss: 0.6908968687057495\n",
      "validation loss: 0.6947115063667297\n",
      "training epoch nr 84\n",
      "training loss: 0.6909157633781433\n",
      "validation loss: 0.6947047114372253\n",
      "training epoch nr 85\n",
      "training loss: 0.6909455060958862\n",
      "validation loss: 0.6947685480117798\n",
      "training epoch nr 86\n",
      "training loss: 0.6909162402153015\n",
      "validation loss: 0.6942080855369568\n",
      "training epoch nr 87\n",
      "training loss: 0.6907161474227905\n",
      "validation loss: 0.6949717998504639\n",
      "training epoch nr 88\n",
      "training loss: 0.6907601356506348\n",
      "validation loss: 0.6946364641189575\n",
      "training epoch nr 89\n",
      "training loss: 0.6908427476882935\n",
      "validation loss: 0.6945385932922363\n",
      "training epoch nr 90\n",
      "training loss: 0.6907314658164978\n",
      "validation loss: 0.6944984793663025\n",
      "training epoch nr 91\n",
      "training loss: 0.6907923817634583\n",
      "validation loss: 0.6946300864219666\n",
      "training epoch nr 92\n",
      "training loss: 0.6906578540802002\n",
      "validation loss: 0.6950773596763611\n",
      "training epoch nr 93\n",
      "training loss: 0.6907023191452026\n",
      "validation loss: 0.6947721838951111\n",
      "training epoch nr 94\n",
      "training loss: 0.6906721591949463\n",
      "validation loss: 0.694786787033081\n",
      "training epoch nr 95\n",
      "training loss: 0.6907378435134888\n",
      "validation loss: 0.6949141621589661\n",
      "training epoch nr 96\n",
      "training loss: 0.6906045079231262\n",
      "validation loss: 0.6945769786834717\n",
      "training epoch nr 97\n",
      "training loss: 0.6905586123466492\n",
      "validation loss: 0.6952793598175049\n",
      "training epoch nr 98\n",
      "training loss: 0.6907220482826233\n",
      "validation loss: 0.6946990489959717\n",
      "training epoch nr 99\n",
      "training loss: 0.6904099583625793\n",
      "validation loss: 0.6953005194664001\n",
      "Training model nr 3...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932875514030457\n",
      "validation loss: 0.6931418776512146\n",
      "training epoch nr 1\n",
      "training loss: 0.6931396722793579\n",
      "validation loss: 0.6930733919143677\n",
      "training epoch nr 2\n",
      "training loss: 0.6930914521217346\n",
      "validation loss: 0.6930907964706421\n",
      "training epoch nr 3\n",
      "training loss: 0.6930498480796814\n",
      "validation loss: 0.6929996609687805\n",
      "training epoch nr 4\n",
      "training loss: 0.6929426789283752\n",
      "validation loss: 0.6929758787155151\n",
      "training epoch nr 5\n",
      "training loss: 0.6929450631141663\n",
      "validation loss: 0.6930353045463562\n",
      "training epoch nr 6\n",
      "training loss: 0.6929737329483032\n",
      "validation loss: 0.6928832530975342\n",
      "training epoch nr 7\n",
      "training loss: 0.6929479837417603\n",
      "validation loss: 0.6929298639297485\n",
      "training epoch nr 8\n",
      "training loss: 0.6929183602333069\n",
      "validation loss: 0.6929106116294861\n",
      "training epoch nr 9\n",
      "training loss: 0.6928785443305969\n",
      "validation loss: 0.6928686499595642\n",
      "training epoch nr 10\n",
      "training loss: 0.6928610801696777\n",
      "validation loss: 0.6927576661109924\n",
      "training epoch nr 11\n",
      "training loss: 0.6928241848945618\n",
      "validation loss: 0.69303959608078\n",
      "training epoch nr 12\n",
      "training loss: 0.692764401435852\n",
      "validation loss: 0.6929941177368164\n",
      "training epoch nr 13\n",
      "training loss: 0.6927206516265869\n",
      "validation loss: 0.6929425001144409\n",
      "training epoch nr 14\n",
      "training loss: 0.6927600502967834\n",
      "validation loss: 0.6928920149803162\n",
      "training epoch nr 15\n",
      "training loss: 0.6927659511566162\n",
      "validation loss: 0.692874014377594\n",
      "training epoch nr 16\n",
      "training loss: 0.6927196979522705\n",
      "validation loss: 0.6928384304046631\n",
      "training epoch nr 17\n",
      "training loss: 0.692755401134491\n",
      "validation loss: 0.6928213834762573\n",
      "training epoch nr 18\n",
      "training loss: 0.6926562190055847\n",
      "validation loss: 0.6928078532218933\n",
      "training epoch nr 19\n",
      "training loss: 0.6927202939987183\n",
      "validation loss: 0.6928955316543579\n",
      "training epoch nr 20\n",
      "training loss: 0.692645251750946\n",
      "validation loss: 0.6929558515548706\n",
      "training epoch nr 21\n",
      "training loss: 0.6926431655883789\n",
      "validation loss: 0.6929545998573303\n",
      "training epoch nr 22\n",
      "training loss: 0.6926079392433167\n",
      "validation loss: 0.6929201483726501\n",
      "training epoch nr 23\n",
      "training loss: 0.6925520300865173\n",
      "validation loss: 0.6929470896720886\n",
      "training epoch nr 24\n",
      "training loss: 0.6926105618476868\n",
      "validation loss: 0.6929138898849487\n",
      "training epoch nr 25\n",
      "training loss: 0.6925091743469238\n",
      "validation loss: 0.6930033564567566\n",
      "training epoch nr 26\n",
      "training loss: 0.6924866437911987\n",
      "validation loss: 0.6929688453674316\n",
      "training epoch nr 27\n",
      "training loss: 0.6925437450408936\n",
      "validation loss: 0.6933944225311279\n",
      "training epoch nr 28\n",
      "training loss: 0.6925300359725952\n",
      "validation loss: 0.6929630041122437\n",
      "training epoch nr 29\n",
      "training loss: 0.692460298538208\n",
      "validation loss: 0.6932188272476196\n",
      "training epoch nr 30\n",
      "training loss: 0.6925470232963562\n",
      "validation loss: 0.6931195259094238\n",
      "training epoch nr 31\n",
      "training loss: 0.6924116611480713\n",
      "validation loss: 0.6930426359176636\n",
      "training epoch nr 32\n",
      "training loss: 0.692390501499176\n",
      "validation loss: 0.6932840943336487\n",
      "training epoch nr 33\n",
      "training loss: 0.6923832893371582\n",
      "validation loss: 0.6933160424232483\n",
      "training epoch nr 34\n",
      "training loss: 0.6923356652259827\n",
      "validation loss: 0.6929829716682434\n",
      "training epoch nr 35\n",
      "training loss: 0.6924126148223877\n",
      "validation loss: 0.6931989192962646\n",
      "training epoch nr 36\n",
      "training loss: 0.6923139691352844\n",
      "validation loss: 0.6930404901504517\n",
      "training epoch nr 37\n",
      "training loss: 0.6923114657402039\n",
      "validation loss: 0.6931185126304626\n",
      "training epoch nr 38\n",
      "training loss: 0.6923606991767883\n",
      "validation loss: 0.693054735660553\n",
      "training epoch nr 39\n",
      "training loss: 0.6923530101776123\n",
      "validation loss: 0.6932839155197144\n",
      "training epoch nr 40\n",
      "training loss: 0.692233681678772\n",
      "validation loss: 0.6931878924369812\n",
      "training epoch nr 41\n",
      "training loss: 0.6922438740730286\n",
      "validation loss: 0.693241536617279\n",
      "training epoch nr 42\n",
      "training loss: 0.6921998262405396\n",
      "validation loss: 0.6932985186576843\n",
      "training epoch nr 43\n",
      "training loss: 0.6921617984771729\n",
      "validation loss: 0.6933082938194275\n",
      "training epoch nr 44\n",
      "training loss: 0.6921221017837524\n",
      "validation loss: 0.693282961845398\n",
      "training epoch nr 45\n",
      "training loss: 0.6920897364616394\n",
      "validation loss: 0.6932263374328613\n",
      "training epoch nr 46\n",
      "training loss: 0.6920715570449829\n",
      "validation loss: 0.6933432817459106\n",
      "training epoch nr 47\n",
      "training loss: 0.6920450925827026\n",
      "validation loss: 0.6934043169021606\n",
      "training epoch nr 48\n",
      "training loss: 0.6921147108078003\n",
      "validation loss: 0.6933760046958923\n",
      "training epoch nr 49\n",
      "training loss: 0.6919663548469543\n",
      "validation loss: 0.693115234375\n",
      "training epoch nr 50\n",
      "training loss: 0.6920598745346069\n",
      "validation loss: 0.6933834552764893\n",
      "training epoch nr 51\n",
      "training loss: 0.6919991970062256\n",
      "validation loss: 0.693364679813385\n",
      "training epoch nr 52\n",
      "training loss: 0.6919569373130798\n",
      "validation loss: 0.6934133172035217\n",
      "training epoch nr 53\n",
      "training loss: 0.6920194625854492\n",
      "validation loss: 0.6933982372283936\n",
      "training epoch nr 54\n",
      "training loss: 0.6918964982032776\n",
      "validation loss: 0.6935123205184937\n",
      "training epoch nr 55\n",
      "training loss: 0.6919586062431335\n",
      "validation loss: 0.6933995485305786\n",
      "training epoch nr 56\n",
      "training loss: 0.6918554306030273\n",
      "validation loss: 0.6935389637947083\n",
      "training epoch nr 57\n",
      "training loss: 0.6918739080429077\n",
      "validation loss: 0.6936612725257874\n",
      "training epoch nr 58\n",
      "training loss: 0.6918279528617859\n",
      "validation loss: 0.6937912702560425\n",
      "training epoch nr 59\n",
      "training loss: 0.6919062733650208\n",
      "validation loss: 0.6935068368911743\n",
      "training epoch nr 60\n",
      "training loss: 0.691744863986969\n",
      "validation loss: 0.6937141418457031\n",
      "training epoch nr 61\n",
      "training loss: 0.6916847229003906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6934456825256348\n",
      "training epoch nr 62\n",
      "training loss: 0.6916823387145996\n",
      "validation loss: 0.6935083270072937\n",
      "training epoch nr 63\n",
      "training loss: 0.6916695833206177\n",
      "validation loss: 0.6937739253044128\n",
      "training epoch nr 64\n",
      "training loss: 0.6917261481285095\n",
      "validation loss: 0.6936299800872803\n",
      "training epoch nr 65\n",
      "training loss: 0.6917118430137634\n",
      "validation loss: 0.6935216784477234\n",
      "training epoch nr 66\n",
      "training loss: 0.6916903853416443\n",
      "validation loss: 0.6936642527580261\n",
      "training epoch nr 67\n",
      "training loss: 0.6915190815925598\n",
      "validation loss: 0.6939173340797424\n",
      "training epoch nr 68\n",
      "training loss: 0.6916424632072449\n",
      "validation loss: 0.6939753890037537\n",
      "training epoch nr 69\n",
      "training loss: 0.6915319561958313\n",
      "validation loss: 0.6936342120170593\n",
      "training epoch nr 70\n",
      "training loss: 0.6915045380592346\n",
      "validation loss: 0.6943835020065308\n",
      "training epoch nr 71\n",
      "training loss: 0.6915168762207031\n",
      "validation loss: 0.6941301822662354\n",
      "training epoch nr 72\n",
      "training loss: 0.691563069820404\n",
      "validation loss: 0.6944631934165955\n",
      "training epoch nr 73\n",
      "training loss: 0.6914917230606079\n",
      "validation loss: 0.6938233375549316\n",
      "training epoch nr 74\n",
      "training loss: 0.6913695335388184\n",
      "validation loss: 0.694094717502594\n",
      "training epoch nr 75\n",
      "training loss: 0.6914004683494568\n",
      "validation loss: 0.6940110921859741\n",
      "training epoch nr 76\n",
      "training loss: 0.6914290189743042\n",
      "validation loss: 0.6942282319068909\n",
      "training epoch nr 77\n",
      "training loss: 0.6913348436355591\n",
      "validation loss: 0.6938954591751099\n",
      "training epoch nr 78\n",
      "training loss: 0.6912177205085754\n",
      "validation loss: 0.6939692497253418\n",
      "training epoch nr 79\n",
      "training loss: 0.6912703514099121\n",
      "validation loss: 0.6942277550697327\n",
      "training epoch nr 80\n",
      "training loss: 0.6912708878517151\n",
      "validation loss: 0.6938864588737488\n",
      "training epoch nr 81\n",
      "training loss: 0.6912618279457092\n",
      "validation loss: 0.6941909790039062\n",
      "training epoch nr 82\n",
      "training loss: 0.691124677658081\n",
      "validation loss: 0.694331705570221\n",
      "training epoch nr 83\n",
      "training loss: 0.6911919116973877\n",
      "validation loss: 0.6942173838615417\n",
      "training epoch nr 84\n",
      "training loss: 0.6910665035247803\n",
      "validation loss: 0.6945448517799377\n",
      "training epoch nr 85\n",
      "training loss: 0.69094318151474\n",
      "validation loss: 0.6944892406463623\n",
      "training epoch nr 86\n",
      "training loss: 0.6910012364387512\n",
      "validation loss: 0.6942944526672363\n",
      "training epoch nr 87\n",
      "training loss: 0.6910531520843506\n",
      "validation loss: 0.6943823099136353\n",
      "training epoch nr 88\n",
      "training loss: 0.690983235836029\n",
      "validation loss: 0.6943727731704712\n",
      "training epoch nr 89\n",
      "training loss: 0.690952718257904\n",
      "validation loss: 0.6945352554321289\n",
      "training epoch nr 90\n",
      "training loss: 0.6908743977546692\n",
      "validation loss: 0.6945879459381104\n",
      "training epoch nr 91\n",
      "training loss: 0.6908227801322937\n",
      "validation loss: 0.6942362785339355\n",
      "training epoch nr 92\n",
      "training loss: 0.6908619999885559\n",
      "validation loss: 0.6946195363998413\n",
      "training epoch nr 93\n",
      "training loss: 0.6908020377159119\n",
      "validation loss: 0.6946558356285095\n",
      "training epoch nr 94\n",
      "training loss: 0.69072026014328\n",
      "validation loss: 0.6943918466567993\n",
      "training epoch nr 95\n",
      "training loss: 0.6907350420951843\n",
      "validation loss: 0.6943995356559753\n",
      "training epoch nr 96\n",
      "training loss: 0.6907444596290588\n",
      "validation loss: 0.6945882439613342\n",
      "training epoch nr 97\n",
      "training loss: 0.6906479597091675\n",
      "validation loss: 0.6949117183685303\n",
      "training epoch nr 98\n",
      "training loss: 0.6906212568283081\n",
      "validation loss: 0.6949443817138672\n",
      "training epoch nr 99\n",
      "training loss: 0.6905099153518677\n",
      "validation loss: 0.6950643658638\n",
      "Training model nr 4...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932849287986755\n",
      "validation loss: 0.6930969953536987\n",
      "training epoch nr 1\n",
      "training loss: 0.6931259036064148\n",
      "validation loss: 0.693121612071991\n",
      "training epoch nr 2\n",
      "training loss: 0.6931716799736023\n",
      "validation loss: 0.6930764317512512\n",
      "training epoch nr 3\n",
      "training loss: 0.6930937767028809\n",
      "validation loss: 0.6931121349334717\n",
      "training epoch nr 4\n",
      "training loss: 0.6930015683174133\n",
      "validation loss: 0.6933796405792236\n",
      "training epoch nr 5\n",
      "training loss: 0.6929793357849121\n",
      "validation loss: 0.693052351474762\n",
      "training epoch nr 6\n",
      "training loss: 0.6930313110351562\n",
      "validation loss: 0.693048357963562\n",
      "training epoch nr 7\n",
      "training loss: 0.6929457783699036\n",
      "validation loss: 0.6929886937141418\n",
      "training epoch nr 8\n",
      "training loss: 0.6928494572639465\n",
      "validation loss: 0.6931827664375305\n",
      "training epoch nr 9\n",
      "training loss: 0.6929099559783936\n",
      "validation loss: 0.692981481552124\n",
      "training epoch nr 10\n",
      "training loss: 0.6928213834762573\n",
      "validation loss: 0.6929044723510742\n",
      "training epoch nr 11\n",
      "training loss: 0.6927747130393982\n",
      "validation loss: 0.6929836273193359\n",
      "training epoch nr 12\n",
      "training loss: 0.6927812099456787\n",
      "validation loss: 0.6929000616073608\n",
      "training epoch nr 13\n",
      "training loss: 0.6927759051322937\n",
      "validation loss: 0.6929346919059753\n",
      "training epoch nr 14\n",
      "training loss: 0.692739725112915\n",
      "validation loss: 0.6929500102996826\n",
      "training epoch nr 15\n",
      "training loss: 0.6927580833435059\n",
      "validation loss: 0.6928927898406982\n",
      "training epoch nr 16\n",
      "training loss: 0.6927493214607239\n",
      "validation loss: 0.6928778290748596\n",
      "training epoch nr 17\n",
      "training loss: 0.6926780343055725\n",
      "validation loss: 0.6928565502166748\n",
      "training epoch nr 18\n",
      "training loss: 0.6926705241203308\n",
      "validation loss: 0.6930555701255798\n",
      "training epoch nr 19\n",
      "training loss: 0.6926375031471252\n",
      "validation loss: 0.6928161978721619\n",
      "training epoch nr 20\n",
      "training loss: 0.6927276849746704\n",
      "validation loss: 0.6929441094398499\n",
      "training epoch nr 21\n",
      "training loss: 0.6925814151763916\n",
      "validation loss: 0.6929393410682678\n",
      "training epoch nr 22\n",
      "training loss: 0.6926574110984802\n",
      "validation loss: 0.6929501295089722\n",
      "training epoch nr 23\n",
      "training loss: 0.6926525831222534\n",
      "validation loss: 0.6928194761276245\n",
      "training epoch nr 24\n",
      "training loss: 0.6926208138465881\n",
      "validation loss: 0.6929330229759216\n",
      "training epoch nr 25\n",
      "training loss: 0.6925637125968933\n",
      "validation loss: 0.6931002140045166\n",
      "training epoch nr 26\n",
      "training loss: 0.6926290988922119\n",
      "validation loss: 0.6928870677947998\n",
      "training epoch nr 27\n",
      "training loss: 0.692489504814148\n",
      "validation loss: 0.6929422616958618\n",
      "training epoch nr 28\n",
      "training loss: 0.6925775408744812\n",
      "validation loss: 0.692947268486023\n",
      "training epoch nr 29\n",
      "training loss: 0.6924936771392822\n",
      "validation loss: 0.6929195523262024\n",
      "training epoch nr 30\n",
      "training loss: 0.6924570798873901\n",
      "validation loss: 0.6929236054420471\n",
      "training epoch nr 31\n",
      "training loss: 0.6925374865531921\n",
      "validation loss: 0.6928620934486389\n",
      "training epoch nr 32\n",
      "training loss: 0.6925243735313416\n",
      "validation loss: 0.6932976841926575\n",
      "training epoch nr 33\n",
      "training loss: 0.6925488710403442\n",
      "validation loss: 0.693029522895813\n",
      "training epoch nr 34\n",
      "training loss: 0.6924118995666504\n",
      "validation loss: 0.6930497884750366\n",
      "training epoch nr 35\n",
      "training loss: 0.6923566460609436\n",
      "validation loss: 0.6930493116378784\n",
      "training epoch nr 36\n",
      "training loss: 0.6923730373382568\n",
      "validation loss: 0.6930435299873352\n",
      "training epoch nr 37\n",
      "training loss: 0.6923962235450745\n",
      "validation loss: 0.6929545402526855\n",
      "training epoch nr 38\n",
      "training loss: 0.6924667954444885\n",
      "validation loss: 0.6930745244026184\n",
      "training epoch nr 39\n",
      "training loss: 0.6924043893814087\n",
      "validation loss: 0.6930859684944153\n",
      "training epoch nr 40\n",
      "training loss: 0.6923580169677734\n",
      "validation loss: 0.6932428479194641\n",
      "training epoch nr 41\n",
      "training loss: 0.6923403739929199\n",
      "validation loss: 0.693209707736969\n",
      "training epoch nr 42\n",
      "training loss: 0.6923168301582336\n",
      "validation loss: 0.6929996609687805\n",
      "training epoch nr 43\n",
      "training loss: 0.6923065781593323\n",
      "validation loss: 0.6930198669433594\n",
      "training epoch nr 44\n",
      "training loss: 0.692373514175415\n",
      "validation loss: 0.6931376457214355\n",
      "training epoch nr 45\n",
      "training loss: 0.6922216415405273\n",
      "validation loss: 0.6931471228599548\n",
      "training epoch nr 46\n",
      "training loss: 0.6921807527542114\n",
      "validation loss: 0.6931109428405762\n",
      "training epoch nr 47\n",
      "training loss: 0.6922318935394287\n",
      "validation loss: 0.6931782364845276\n",
      "training epoch nr 48\n",
      "training loss: 0.6922525763511658\n",
      "validation loss: 0.6931485533714294\n",
      "training epoch nr 49\n",
      "training loss: 0.6921288967132568\n",
      "validation loss: 0.6931942105293274\n",
      "training epoch nr 50\n",
      "training loss: 0.6921765804290771\n",
      "validation loss: 0.693576455116272\n",
      "training epoch nr 51\n",
      "training loss: 0.6922128796577454\n",
      "validation loss: 0.6934198141098022\n",
      "training epoch nr 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6921729445457458\n",
      "validation loss: 0.6931846737861633\n",
      "training epoch nr 53\n",
      "training loss: 0.6921829581260681\n",
      "validation loss: 0.693294107913971\n",
      "training epoch nr 54\n",
      "training loss: 0.6920837163925171\n",
      "validation loss: 0.6931566596031189\n",
      "training epoch nr 55\n",
      "training loss: 0.6920790076255798\n",
      "validation loss: 0.6933624148368835\n",
      "training epoch nr 56\n",
      "training loss: 0.692138135433197\n",
      "validation loss: 0.693336009979248\n",
      "training epoch nr 57\n",
      "training loss: 0.6919810771942139\n",
      "validation loss: 0.6935259699821472\n",
      "training epoch nr 58\n",
      "training loss: 0.6919556260108948\n",
      "validation loss: 0.6933832168579102\n",
      "training epoch nr 59\n",
      "training loss: 0.6919387578964233\n",
      "validation loss: 0.6933124661445618\n",
      "training epoch nr 60\n",
      "training loss: 0.6919011473655701\n",
      "validation loss: 0.6933375000953674\n",
      "training epoch nr 61\n",
      "training loss: 0.6919958591461182\n",
      "validation loss: 0.6933783292770386\n",
      "training epoch nr 62\n",
      "training loss: 0.6919989585876465\n",
      "validation loss: 0.6935216784477234\n",
      "training epoch nr 63\n",
      "training loss: 0.6919077038764954\n",
      "validation loss: 0.6935976147651672\n",
      "training epoch nr 64\n",
      "training loss: 0.6919052600860596\n",
      "validation loss: 0.6935787796974182\n",
      "training epoch nr 65\n",
      "training loss: 0.691852867603302\n",
      "validation loss: 0.6937519311904907\n",
      "training epoch nr 66\n",
      "training loss: 0.6917431354522705\n",
      "validation loss: 0.6936943531036377\n",
      "training epoch nr 67\n",
      "training loss: 0.6917910575866699\n",
      "validation loss: 0.6936668753623962\n",
      "training epoch nr 68\n",
      "training loss: 0.6918033957481384\n",
      "validation loss: 0.6937018036842346\n",
      "training epoch nr 69\n",
      "training loss: 0.6917094588279724\n",
      "validation loss: 0.6936817169189453\n",
      "training epoch nr 70\n",
      "training loss: 0.6918060183525085\n",
      "validation loss: 0.6934985518455505\n",
      "training epoch nr 71\n",
      "training loss: 0.6916649341583252\n",
      "validation loss: 0.693423867225647\n",
      "training epoch nr 72\n",
      "training loss: 0.6917489171028137\n",
      "validation loss: 0.6936575770378113\n",
      "training epoch nr 73\n",
      "training loss: 0.6916851997375488\n",
      "validation loss: 0.6938161849975586\n",
      "training epoch nr 74\n",
      "training loss: 0.6916278600692749\n",
      "validation loss: 0.6937422752380371\n",
      "training epoch nr 75\n",
      "training loss: 0.6916568279266357\n",
      "validation loss: 0.693831205368042\n",
      "training epoch nr 76\n",
      "training loss: 0.691594660282135\n",
      "validation loss: 0.6937350630760193\n",
      "training epoch nr 77\n",
      "training loss: 0.6917291283607483\n",
      "validation loss: 0.6939877271652222\n",
      "training epoch nr 78\n",
      "training loss: 0.6915246844291687\n",
      "validation loss: 0.6936137080192566\n",
      "training epoch nr 79\n",
      "training loss: 0.6915968060493469\n",
      "validation loss: 0.6940053701400757\n",
      "training epoch nr 80\n",
      "training loss: 0.6915525197982788\n",
      "validation loss: 0.6938795447349548\n",
      "training epoch nr 81\n",
      "training loss: 0.6914340853691101\n",
      "validation loss: 0.6940310597419739\n",
      "training epoch nr 82\n",
      "training loss: 0.6915904879570007\n",
      "validation loss: 0.6940322518348694\n",
      "training epoch nr 83\n",
      "training loss: 0.6914907097816467\n",
      "validation loss: 0.694061279296875\n",
      "training epoch nr 84\n",
      "training loss: 0.6914698481559753\n",
      "validation loss: 0.6939847469329834\n",
      "training epoch nr 85\n",
      "training loss: 0.6914141774177551\n",
      "validation loss: 0.6939299702644348\n",
      "training epoch nr 86\n",
      "training loss: 0.6914729475975037\n",
      "validation loss: 0.6940113306045532\n",
      "training epoch nr 87\n",
      "training loss: 0.6914976239204407\n",
      "validation loss: 0.6944977045059204\n",
      "training epoch nr 88\n",
      "training loss: 0.6912738680839539\n",
      "validation loss: 0.6943618059158325\n",
      "training epoch nr 89\n",
      "training loss: 0.6912950873374939\n",
      "validation loss: 0.6942858099937439\n",
      "training epoch nr 90\n",
      "training loss: 0.691248893737793\n",
      "validation loss: 0.6941922903060913\n",
      "training epoch nr 91\n",
      "training loss: 0.6913229823112488\n",
      "validation loss: 0.6941850185394287\n",
      "training epoch nr 92\n",
      "training loss: 0.6913143396377563\n",
      "validation loss: 0.6939512491226196\n",
      "training epoch nr 93\n",
      "training loss: 0.6912775635719299\n",
      "validation loss: 0.6940954327583313\n",
      "training epoch nr 94\n",
      "training loss: 0.6912785172462463\n",
      "validation loss: 0.6942481994628906\n",
      "training epoch nr 95\n",
      "training loss: 0.6912120580673218\n",
      "validation loss: 0.6938689947128296\n",
      "training epoch nr 96\n",
      "training loss: 0.6911547780036926\n",
      "validation loss: 0.6945294737815857\n",
      "training epoch nr 97\n",
      "training loss: 0.6911547780036926\n",
      "validation loss: 0.6943528056144714\n",
      "training epoch nr 98\n",
      "training loss: 0.6910089254379272\n",
      "validation loss: 0.6948009133338928\n",
      "training epoch nr 99\n",
      "training loss: 0.691088855266571\n",
      "validation loss: 0.6943263411521912\n",
      "Training model nr 5...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933525204658508\n",
      "validation loss: 0.6932042241096497\n",
      "training epoch nr 1\n",
      "training loss: 0.6931454539299011\n",
      "validation loss: 0.6930648684501648\n",
      "training epoch nr 2\n",
      "training loss: 0.6930508017539978\n",
      "validation loss: 0.6931984424591064\n",
      "training epoch nr 3\n",
      "training loss: 0.6930691003799438\n",
      "validation loss: 0.6931856274604797\n",
      "training epoch nr 4\n",
      "training loss: 0.6930845975875854\n",
      "validation loss: 0.6930707693099976\n",
      "training epoch nr 5\n",
      "training loss: 0.6930819749832153\n",
      "validation loss: 0.6930312514305115\n",
      "training epoch nr 6\n",
      "training loss: 0.6930261850357056\n",
      "validation loss: 0.6929931640625\n",
      "training epoch nr 7\n",
      "training loss: 0.692939817905426\n",
      "validation loss: 0.6929921507835388\n",
      "training epoch nr 8\n",
      "training loss: 0.6929242014884949\n",
      "validation loss: 0.6929619908332825\n",
      "training epoch nr 9\n",
      "training loss: 0.6929545402526855\n",
      "validation loss: 0.6929295659065247\n",
      "training epoch nr 10\n",
      "training loss: 0.6928181052207947\n",
      "validation loss: 0.6930831670761108\n",
      "training epoch nr 11\n",
      "training loss: 0.6927515864372253\n",
      "validation loss: 0.6928113102912903\n",
      "training epoch nr 12\n",
      "training loss: 0.692820131778717\n",
      "validation loss: 0.6928317546844482\n",
      "training epoch nr 13\n",
      "training loss: 0.6927797794342041\n",
      "validation loss: 0.6927576065063477\n",
      "training epoch nr 14\n",
      "training loss: 0.6927116513252258\n",
      "validation loss: 0.6929153800010681\n",
      "training epoch nr 15\n",
      "training loss: 0.692705512046814\n",
      "validation loss: 0.692876398563385\n",
      "training epoch nr 16\n",
      "training loss: 0.692803680896759\n",
      "validation loss: 0.692767322063446\n",
      "training epoch nr 17\n",
      "training loss: 0.692699134349823\n",
      "validation loss: 0.6928750872612\n",
      "training epoch nr 18\n",
      "training loss: 0.6926136016845703\n",
      "validation loss: 0.6927986741065979\n",
      "training epoch nr 19\n",
      "training loss: 0.6926841139793396\n",
      "validation loss: 0.6931175589561462\n",
      "training epoch nr 20\n",
      "training loss: 0.6926186084747314\n",
      "validation loss: 0.692888617515564\n",
      "training epoch nr 21\n",
      "training loss: 0.6926317811012268\n",
      "validation loss: 0.6928665637969971\n",
      "training epoch nr 22\n",
      "training loss: 0.6926199793815613\n",
      "validation loss: 0.693173348903656\n",
      "training epoch nr 23\n",
      "training loss: 0.692601203918457\n",
      "validation loss: 0.6928998231887817\n",
      "training epoch nr 24\n",
      "training loss: 0.6925822496414185\n",
      "validation loss: 0.693061888217926\n",
      "training epoch nr 25\n",
      "training loss: 0.6925441026687622\n",
      "validation loss: 0.6928730607032776\n",
      "training epoch nr 26\n",
      "training loss: 0.6924694180488586\n",
      "validation loss: 0.6932414174079895\n",
      "training epoch nr 27\n",
      "training loss: 0.6924780607223511\n",
      "validation loss: 0.6931716799736023\n",
      "training epoch nr 28\n",
      "training loss: 0.6924465894699097\n",
      "validation loss: 0.6931442022323608\n",
      "training epoch nr 29\n",
      "training loss: 0.6925079822540283\n",
      "validation loss: 0.6928846836090088\n",
      "training epoch nr 30\n",
      "training loss: 0.6925175786018372\n",
      "validation loss: 0.6930673718452454\n",
      "training epoch nr 31\n",
      "training loss: 0.6924194693565369\n",
      "validation loss: 0.6929115056991577\n",
      "training epoch nr 32\n",
      "training loss: 0.6924842000007629\n",
      "validation loss: 0.6931770443916321\n",
      "training epoch nr 33\n",
      "training loss: 0.6924624443054199\n",
      "validation loss: 0.6932490468025208\n",
      "training epoch nr 34\n",
      "training loss: 0.6925313472747803\n",
      "validation loss: 0.6928950548171997\n",
      "training epoch nr 35\n",
      "training loss: 0.6924414038658142\n",
      "validation loss: 0.6929891705513\n",
      "training epoch nr 36\n",
      "training loss: 0.6923408508300781\n",
      "validation loss: 0.6930830478668213\n",
      "training epoch nr 37\n",
      "training loss: 0.6924368739128113\n",
      "validation loss: 0.693290650844574\n",
      "training epoch nr 38\n",
      "training loss: 0.6922807097434998\n",
      "validation loss: 0.6932379603385925\n",
      "training epoch nr 39\n",
      "training loss: 0.6923094987869263\n",
      "validation loss: 0.6931162476539612\n",
      "training epoch nr 40\n",
      "training loss: 0.6923766136169434\n",
      "validation loss: 0.6933934688568115\n",
      "training epoch nr 41\n",
      "training loss: 0.6923073530197144\n",
      "validation loss: 0.6932461857795715\n",
      "training epoch nr 42\n",
      "training loss: 0.6922016739845276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6933934092521667\n",
      "training epoch nr 43\n",
      "training loss: 0.6923066973686218\n",
      "validation loss: 0.6933528780937195\n",
      "training epoch nr 44\n",
      "training loss: 0.6921607851982117\n",
      "validation loss: 0.693278431892395\n",
      "training epoch nr 45\n",
      "training loss: 0.6922233700752258\n",
      "validation loss: 0.6934046745300293\n",
      "training epoch nr 46\n",
      "training loss: 0.6921082139015198\n",
      "validation loss: 0.6935862302780151\n",
      "training epoch nr 47\n",
      "training loss: 0.6921659708023071\n",
      "validation loss: 0.6933176517486572\n",
      "training epoch nr 48\n",
      "training loss: 0.6921831965446472\n",
      "validation loss: 0.6932965517044067\n",
      "training epoch nr 49\n",
      "training loss: 0.6920884847640991\n",
      "validation loss: 0.6934993267059326\n",
      "training epoch nr 50\n",
      "training loss: 0.6920489072799683\n",
      "validation loss: 0.6938296556472778\n",
      "training epoch nr 51\n",
      "training loss: 0.6920707821846008\n",
      "validation loss: 0.6935250163078308\n",
      "training epoch nr 52\n",
      "training loss: 0.6920574903488159\n",
      "validation loss: 0.6934195756912231\n",
      "training epoch nr 53\n",
      "training loss: 0.6921036839485168\n",
      "validation loss: 0.6935577988624573\n",
      "training epoch nr 54\n",
      "training loss: 0.6919592022895813\n",
      "validation loss: 0.6936413049697876\n",
      "training epoch nr 55\n",
      "training loss: 0.6919628381729126\n",
      "validation loss: 0.6938478946685791\n",
      "training epoch nr 56\n",
      "training loss: 0.6919609904289246\n",
      "validation loss: 0.6937264800071716\n",
      "training epoch nr 57\n",
      "training loss: 0.6920660138130188\n",
      "validation loss: 0.6935634613037109\n",
      "training epoch nr 58\n",
      "training loss: 0.6919237971305847\n",
      "validation loss: 0.6936561465263367\n",
      "training epoch nr 59\n",
      "training loss: 0.6919837594032288\n",
      "validation loss: 0.6935606002807617\n",
      "training epoch nr 60\n",
      "training loss: 0.6918891072273254\n",
      "validation loss: 0.6935637593269348\n",
      "training epoch nr 61\n",
      "training loss: 0.6919036507606506\n",
      "validation loss: 0.6935375928878784\n",
      "training epoch nr 62\n",
      "training loss: 0.69179368019104\n",
      "validation loss: 0.6937631368637085\n",
      "training epoch nr 63\n",
      "training loss: 0.6918765306472778\n",
      "validation loss: 0.6936269998550415\n",
      "training epoch nr 64\n",
      "training loss: 0.6918624043464661\n",
      "validation loss: 0.6936874389648438\n",
      "training epoch nr 65\n",
      "training loss: 0.6918564438819885\n",
      "validation loss: 0.6934729218482971\n",
      "training epoch nr 66\n",
      "training loss: 0.6917515397071838\n",
      "validation loss: 0.6937162280082703\n",
      "training epoch nr 67\n",
      "training loss: 0.6917772889137268\n",
      "validation loss: 0.6938183307647705\n",
      "training epoch nr 68\n",
      "training loss: 0.6918516159057617\n",
      "validation loss: 0.6938709020614624\n",
      "training epoch nr 69\n",
      "training loss: 0.6917426586151123\n",
      "validation loss: 0.694005012512207\n",
      "training epoch nr 70\n",
      "training loss: 0.6917039752006531\n",
      "validation loss: 0.6935847401618958\n",
      "training epoch nr 71\n",
      "training loss: 0.6916568875312805\n",
      "validation loss: 0.6939780712127686\n",
      "training epoch nr 72\n",
      "training loss: 0.691689670085907\n",
      "validation loss: 0.6937762498855591\n",
      "training epoch nr 73\n",
      "training loss: 0.6917086243629456\n",
      "validation loss: 0.6938719749450684\n",
      "training epoch nr 74\n",
      "training loss: 0.6916186809539795\n",
      "validation loss: 0.6937243938446045\n",
      "training epoch nr 75\n",
      "training loss: 0.691625714302063\n",
      "validation loss: 0.6936718821525574\n",
      "training epoch nr 76\n",
      "training loss: 0.6915737986564636\n",
      "validation loss: 0.6942421793937683\n",
      "training epoch nr 77\n",
      "training loss: 0.6916301846504211\n",
      "validation loss: 0.6941897869110107\n",
      "training epoch nr 78\n",
      "training loss: 0.6915202140808105\n",
      "validation loss: 0.6939030289649963\n",
      "training epoch nr 79\n",
      "training loss: 0.6915372014045715\n",
      "validation loss: 0.6940404176712036\n",
      "training epoch nr 80\n",
      "training loss: 0.691514253616333\n",
      "validation loss: 0.693820059299469\n",
      "training epoch nr 81\n",
      "training loss: 0.6914600133895874\n",
      "validation loss: 0.6940929293632507\n",
      "training epoch nr 82\n",
      "training loss: 0.691371738910675\n",
      "validation loss: 0.694115400314331\n",
      "training epoch nr 83\n",
      "training loss: 0.6913260817527771\n",
      "validation loss: 0.6946072578430176\n",
      "training epoch nr 84\n",
      "training loss: 0.6914252042770386\n",
      "validation loss: 0.6938728094100952\n",
      "training epoch nr 85\n",
      "training loss: 0.6912915706634521\n",
      "validation loss: 0.6941010355949402\n",
      "training epoch nr 86\n",
      "training loss: 0.6914021372795105\n",
      "validation loss: 0.694073498249054\n",
      "training epoch nr 87\n",
      "training loss: 0.6913476586341858\n",
      "validation loss: 0.6945281624794006\n",
      "training epoch nr 88\n",
      "training loss: 0.6913757920265198\n",
      "validation loss: 0.6940175294876099\n",
      "training epoch nr 89\n",
      "training loss: 0.6913595795631409\n",
      "validation loss: 0.6939786076545715\n",
      "training epoch nr 90\n",
      "training loss: 0.6914316415786743\n",
      "validation loss: 0.6940240263938904\n",
      "training epoch nr 91\n",
      "training loss: 0.6912515163421631\n",
      "validation loss: 0.6941243410110474\n",
      "training epoch nr 92\n",
      "training loss: 0.6913215517997742\n",
      "validation loss: 0.6941815614700317\n",
      "training epoch nr 93\n",
      "training loss: 0.6913427710533142\n",
      "validation loss: 0.6941075325012207\n",
      "training epoch nr 94\n",
      "training loss: 0.6912455558776855\n",
      "validation loss: 0.69410640001297\n",
      "training epoch nr 95\n",
      "training loss: 0.6912943720817566\n",
      "validation loss: 0.6941239833831787\n",
      "training epoch nr 96\n",
      "training loss: 0.6913650035858154\n",
      "validation loss: 0.6943156719207764\n",
      "training epoch nr 97\n",
      "training loss: 0.6911771893501282\n",
      "validation loss: 0.6940777897834778\n",
      "training epoch nr 98\n",
      "training loss: 0.6912899613380432\n",
      "validation loss: 0.694656252861023\n",
      "training epoch nr 99\n",
      "training loss: 0.6913154125213623\n",
      "validation loss: 0.69452303647995\n",
      "Training model nr 6...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932772994041443\n",
      "validation loss: 0.6931995153427124\n",
      "training epoch nr 1\n",
      "training loss: 0.6931251883506775\n",
      "validation loss: 0.6930938959121704\n",
      "training epoch nr 2\n",
      "training loss: 0.6930967569351196\n",
      "validation loss: 0.693163275718689\n",
      "training epoch nr 3\n",
      "training loss: 0.6931083798408508\n",
      "validation loss: 0.6930473446846008\n",
      "training epoch nr 4\n",
      "training loss: 0.6930751800537109\n",
      "validation loss: 0.6930421590805054\n",
      "training epoch nr 5\n",
      "training loss: 0.6929821968078613\n",
      "validation loss: 0.6929663419723511\n",
      "training epoch nr 6\n",
      "training loss: 0.6929824352264404\n",
      "validation loss: 0.6930307149887085\n",
      "training epoch nr 7\n",
      "training loss: 0.6929342150688171\n",
      "validation loss: 0.6929751634597778\n",
      "training epoch nr 8\n",
      "training loss: 0.6930269598960876\n",
      "validation loss: 0.6928976774215698\n",
      "training epoch nr 9\n",
      "training loss: 0.692876398563385\n",
      "validation loss: 0.6928787231445312\n",
      "training epoch nr 10\n",
      "training loss: 0.6928725242614746\n",
      "validation loss: 0.6930723786354065\n",
      "training epoch nr 11\n",
      "training loss: 0.6927849054336548\n",
      "validation loss: 0.6929668188095093\n",
      "training epoch nr 12\n",
      "training loss: 0.6927773356437683\n",
      "validation loss: 0.6928452253341675\n",
      "training epoch nr 13\n",
      "training loss: 0.69279545545578\n",
      "validation loss: 0.6929962635040283\n",
      "training epoch nr 14\n",
      "training loss: 0.6927686929702759\n",
      "validation loss: 0.6929327249526978\n",
      "training epoch nr 15\n",
      "training loss: 0.6927880048751831\n",
      "validation loss: 0.6928712725639343\n",
      "training epoch nr 16\n",
      "training loss: 0.6926909685134888\n",
      "validation loss: 0.6928592324256897\n",
      "training epoch nr 17\n",
      "training loss: 0.6926445960998535\n",
      "validation loss: 0.6929463744163513\n",
      "training epoch nr 18\n",
      "training loss: 0.6926278471946716\n",
      "validation loss: 0.6929045915603638\n",
      "training epoch nr 19\n",
      "training loss: 0.6926695108413696\n",
      "validation loss: 0.6929248571395874\n",
      "training epoch nr 20\n",
      "training loss: 0.6926541328430176\n",
      "validation loss: 0.6929098963737488\n",
      "training epoch nr 21\n",
      "training loss: 0.6926776170730591\n",
      "validation loss: 0.6928862929344177\n",
      "training epoch nr 22\n",
      "training loss: 0.6925278902053833\n",
      "validation loss: 0.6929886341094971\n",
      "training epoch nr 23\n",
      "training loss: 0.6926113367080688\n",
      "validation loss: 0.6928882598876953\n",
      "training epoch nr 24\n",
      "training loss: 0.6925405263900757\n",
      "validation loss: 0.6930662393569946\n",
      "training epoch nr 25\n",
      "training loss: 0.6925118565559387\n",
      "validation loss: 0.6929879188537598\n",
      "training epoch nr 26\n",
      "training loss: 0.6924377679824829\n",
      "validation loss: 0.6929621696472168\n",
      "training epoch nr 27\n",
      "training loss: 0.6924679279327393\n",
      "validation loss: 0.6930328607559204\n",
      "training epoch nr 28\n",
      "training loss: 0.6925743222236633\n",
      "validation loss: 0.692950963973999\n",
      "training epoch nr 29\n",
      "training loss: 0.692435085773468\n",
      "validation loss: 0.6931209564208984\n",
      "training epoch nr 30\n",
      "training loss: 0.6924319863319397\n",
      "validation loss: 0.6930690407752991\n",
      "training epoch nr 31\n",
      "training loss: 0.692399263381958\n",
      "validation loss: 0.6932762861251831\n",
      "training epoch nr 32\n",
      "training loss: 0.6923569440841675\n",
      "validation loss: 0.6930359601974487\n",
      "training epoch nr 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6923003196716309\n",
      "validation loss: 0.6930958032608032\n",
      "training epoch nr 34\n",
      "training loss: 0.6923609375953674\n",
      "validation loss: 0.6932232975959778\n",
      "training epoch nr 35\n",
      "training loss: 0.6923403739929199\n",
      "validation loss: 0.6931245923042297\n",
      "training epoch nr 36\n",
      "training loss: 0.6923523545265198\n",
      "validation loss: 0.6931108832359314\n",
      "training epoch nr 37\n",
      "training loss: 0.6923301815986633\n",
      "validation loss: 0.6933255195617676\n",
      "training epoch nr 38\n",
      "training loss: 0.6921852231025696\n",
      "validation loss: 0.693166196346283\n",
      "training epoch nr 39\n",
      "training loss: 0.6921879649162292\n",
      "validation loss: 0.6933891177177429\n",
      "training epoch nr 40\n",
      "training loss: 0.6921969056129456\n",
      "validation loss: 0.6931802034378052\n",
      "training epoch nr 41\n",
      "training loss: 0.6922004818916321\n",
      "validation loss: 0.6931835412979126\n",
      "training epoch nr 42\n",
      "training loss: 0.6922093033790588\n",
      "validation loss: 0.6932344436645508\n",
      "training epoch nr 43\n",
      "training loss: 0.6921924352645874\n",
      "validation loss: 0.6933586597442627\n",
      "training epoch nr 44\n",
      "training loss: 0.6921453475952148\n",
      "validation loss: 0.693394124507904\n",
      "training epoch nr 45\n",
      "training loss: 0.6921283602714539\n",
      "validation loss: 0.6934362649917603\n",
      "training epoch nr 46\n",
      "training loss: 0.6921038627624512\n",
      "validation loss: 0.6934959888458252\n",
      "training epoch nr 47\n",
      "training loss: 0.6920799016952515\n",
      "validation loss: 0.6931016445159912\n",
      "training epoch nr 48\n",
      "training loss: 0.6920703649520874\n",
      "validation loss: 0.693420946598053\n",
      "training epoch nr 49\n",
      "training loss: 0.6919519305229187\n",
      "validation loss: 0.693538248538971\n",
      "training epoch nr 50\n",
      "training loss: 0.691968560218811\n",
      "validation loss: 0.6936144828796387\n",
      "training epoch nr 51\n",
      "training loss: 0.6921272277832031\n",
      "validation loss: 0.6934895515441895\n",
      "training epoch nr 52\n",
      "training loss: 0.6919636130332947\n",
      "validation loss: 0.6934344172477722\n",
      "training epoch nr 53\n",
      "training loss: 0.6919186115264893\n",
      "validation loss: 0.693233072757721\n",
      "training epoch nr 54\n",
      "training loss: 0.6918970942497253\n",
      "validation loss: 0.6935699582099915\n",
      "training epoch nr 55\n",
      "training loss: 0.6918876767158508\n",
      "validation loss: 0.693673312664032\n",
      "training epoch nr 56\n",
      "training loss: 0.6917908191680908\n",
      "validation loss: 0.6934801936149597\n",
      "training epoch nr 57\n",
      "training loss: 0.691806972026825\n",
      "validation loss: 0.6934331655502319\n",
      "training epoch nr 58\n",
      "training loss: 0.691767156124115\n",
      "validation loss: 0.6937011480331421\n",
      "training epoch nr 59\n",
      "training loss: 0.6918193697929382\n",
      "validation loss: 0.6935403943061829\n",
      "training epoch nr 60\n",
      "training loss: 0.6918463110923767\n",
      "validation loss: 0.6934919953346252\n",
      "training epoch nr 61\n",
      "training loss: 0.6917814612388611\n",
      "validation loss: 0.6937443017959595\n",
      "training epoch nr 62\n",
      "training loss: 0.6917861104011536\n",
      "validation loss: 0.6937708258628845\n",
      "training epoch nr 63\n",
      "training loss: 0.6916731595993042\n",
      "validation loss: 0.6936094760894775\n",
      "training epoch nr 64\n",
      "training loss: 0.691705584526062\n",
      "validation loss: 0.6936208009719849\n",
      "training epoch nr 65\n",
      "training loss: 0.6915844082832336\n",
      "validation loss: 0.6938481330871582\n",
      "training epoch nr 66\n",
      "training loss: 0.6915979981422424\n",
      "validation loss: 0.6940967440605164\n",
      "training epoch nr 67\n",
      "training loss: 0.6915696859359741\n",
      "validation loss: 0.6938526034355164\n",
      "training epoch nr 68\n",
      "training loss: 0.6915207505226135\n",
      "validation loss: 0.694043755531311\n",
      "training epoch nr 69\n",
      "training loss: 0.6916245222091675\n",
      "validation loss: 0.6939573884010315\n",
      "training epoch nr 70\n",
      "training loss: 0.6915140151977539\n",
      "validation loss: 0.6937842965126038\n",
      "training epoch nr 71\n",
      "training loss: 0.6915065050125122\n",
      "validation loss: 0.6942690014839172\n",
      "training epoch nr 72\n",
      "training loss: 0.6915193200111389\n",
      "validation loss: 0.694149911403656\n",
      "training epoch nr 73\n",
      "training loss: 0.6914633512496948\n",
      "validation loss: 0.6939313411712646\n",
      "training epoch nr 74\n",
      "training loss: 0.6913489103317261\n",
      "validation loss: 0.6938750743865967\n",
      "training epoch nr 75\n",
      "training loss: 0.6912698149681091\n",
      "validation loss: 0.6941372752189636\n",
      "training epoch nr 76\n",
      "training loss: 0.6913530230522156\n",
      "validation loss: 0.694238543510437\n",
      "training epoch nr 77\n",
      "training loss: 0.6913230419158936\n",
      "validation loss: 0.6945420503616333\n",
      "training epoch nr 78\n",
      "training loss: 0.6914114952087402\n",
      "validation loss: 0.6936718821525574\n",
      "training epoch nr 79\n",
      "training loss: 0.6913135051727295\n",
      "validation loss: 0.6942418813705444\n",
      "training epoch nr 80\n",
      "training loss: 0.6911765933036804\n",
      "validation loss: 0.6942631602287292\n",
      "training epoch nr 81\n",
      "training loss: 0.6913139820098877\n",
      "validation loss: 0.6939978003501892\n",
      "training epoch nr 82\n",
      "training loss: 0.6913141012191772\n",
      "validation loss: 0.6942381262779236\n",
      "training epoch nr 83\n",
      "training loss: 0.6911688446998596\n",
      "validation loss: 0.6944590210914612\n",
      "training epoch nr 84\n",
      "training loss: 0.6912014484405518\n",
      "validation loss: 0.6945363283157349\n",
      "training epoch nr 85\n",
      "training loss: 0.6912729144096375\n",
      "validation loss: 0.6942611336708069\n",
      "training epoch nr 86\n",
      "training loss: 0.6912797093391418\n",
      "validation loss: 0.6944085955619812\n",
      "training epoch nr 87\n",
      "training loss: 0.6910507082939148\n",
      "validation loss: 0.6941891312599182\n",
      "training epoch nr 88\n",
      "training loss: 0.691067636013031\n",
      "validation loss: 0.69447922706604\n",
      "training epoch nr 89\n",
      "training loss: 0.6911030411720276\n",
      "validation loss: 0.6945144534111023\n",
      "training epoch nr 90\n",
      "training loss: 0.6909666657447815\n",
      "validation loss: 0.6943691968917847\n",
      "training epoch nr 91\n",
      "training loss: 0.6910993456840515\n",
      "validation loss: 0.6943045854568481\n",
      "training epoch nr 92\n",
      "training loss: 0.6909834742546082\n",
      "validation loss: 0.6943261623382568\n",
      "training epoch nr 93\n",
      "training loss: 0.6911283135414124\n",
      "validation loss: 0.694821834564209\n",
      "training epoch nr 94\n",
      "training loss: 0.6910088658332825\n",
      "validation loss: 0.6946535706520081\n",
      "training epoch nr 95\n",
      "training loss: 0.6910863518714905\n",
      "validation loss: 0.6942164301872253\n",
      "training epoch nr 96\n",
      "training loss: 0.6908776760101318\n",
      "validation loss: 0.6944060921669006\n",
      "training epoch nr 97\n",
      "training loss: 0.6909218430519104\n",
      "validation loss: 0.6944218277931213\n",
      "training epoch nr 98\n",
      "training loss: 0.6909849047660828\n",
      "validation loss: 0.694229245185852\n",
      "training epoch nr 99\n",
      "training loss: 0.6910121440887451\n",
      "validation loss: 0.6944913268089294\n",
      "Training model nr 7...\n",
      "training epoch nr 0\n",
      "training loss: 0.6933099627494812\n",
      "validation loss: 0.6931506395339966\n",
      "training epoch nr 1\n",
      "training loss: 0.6931048035621643\n",
      "validation loss: 0.693108081817627\n",
      "training epoch nr 2\n",
      "training loss: 0.6930726170539856\n",
      "validation loss: 0.693311870098114\n",
      "training epoch nr 3\n",
      "training loss: 0.6930540204048157\n",
      "validation loss: 0.6930787563323975\n",
      "training epoch nr 4\n",
      "training loss: 0.6930201053619385\n",
      "validation loss: 0.6930269002914429\n",
      "training epoch nr 5\n",
      "training loss: 0.6930199861526489\n",
      "validation loss: 0.6929780840873718\n",
      "training epoch nr 6\n",
      "training loss: 0.6930104494094849\n",
      "validation loss: 0.6929517388343811\n",
      "training epoch nr 7\n",
      "training loss: 0.6929759979248047\n",
      "validation loss: 0.6929950714111328\n",
      "training epoch nr 8\n",
      "training loss: 0.6929484009742737\n",
      "validation loss: 0.6929325461387634\n",
      "training epoch nr 9\n",
      "training loss: 0.6928562521934509\n",
      "validation loss: 0.6930882930755615\n",
      "training epoch nr 10\n",
      "training loss: 0.6928308010101318\n",
      "validation loss: 0.6929727792739868\n",
      "training epoch nr 11\n",
      "training loss: 0.6928091645240784\n",
      "validation loss: 0.6929160356521606\n",
      "training epoch nr 12\n",
      "training loss: 0.6928294897079468\n",
      "validation loss: 0.6928824782371521\n",
      "training epoch nr 13\n",
      "training loss: 0.6927211284637451\n",
      "validation loss: 0.692926824092865\n",
      "training epoch nr 14\n",
      "training loss: 0.6927667856216431\n",
      "validation loss: 0.6929137110710144\n",
      "training epoch nr 15\n",
      "training loss: 0.6927080154418945\n",
      "validation loss: 0.6929783821105957\n",
      "training epoch nr 16\n",
      "training loss: 0.6927064657211304\n",
      "validation loss: 0.6928624510765076\n",
      "training epoch nr 17\n",
      "training loss: 0.6926681995391846\n",
      "validation loss: 0.6929106116294861\n",
      "training epoch nr 18\n",
      "training loss: 0.692606508731842\n",
      "validation loss: 0.6929154396057129\n",
      "training epoch nr 19\n",
      "training loss: 0.692670464515686\n",
      "validation loss: 0.6928090453147888\n",
      "training epoch nr 20\n",
      "training loss: 0.6926214098930359\n",
      "validation loss: 0.6929922699928284\n",
      "training epoch nr 21\n",
      "training loss: 0.6926059722900391\n",
      "validation loss: 0.6930298209190369\n",
      "training epoch nr 22\n",
      "training loss: 0.6925852298736572\n",
      "validation loss: 0.6929441690444946\n",
      "training epoch nr 23\n",
      "training loss: 0.6925215125083923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.6930369138717651\n",
      "training epoch nr 24\n",
      "training loss: 0.692470133304596\n",
      "validation loss: 0.6931178569793701\n",
      "training epoch nr 25\n",
      "training loss: 0.6924535632133484\n",
      "validation loss: 0.6930464506149292\n",
      "training epoch nr 26\n",
      "training loss: 0.6924849152565002\n",
      "validation loss: 0.6929125785827637\n",
      "training epoch nr 27\n",
      "training loss: 0.6923926472663879\n",
      "validation loss: 0.6930100917816162\n",
      "training epoch nr 28\n",
      "training loss: 0.6923136115074158\n",
      "validation loss: 0.6931660771369934\n",
      "training epoch nr 29\n",
      "training loss: 0.6923646926879883\n",
      "validation loss: 0.6932098865509033\n",
      "training epoch nr 30\n",
      "training loss: 0.6923887729644775\n",
      "validation loss: 0.6930358409881592\n",
      "training epoch nr 31\n",
      "training loss: 0.6923261284828186\n",
      "validation loss: 0.6931883692741394\n",
      "training epoch nr 32\n",
      "training loss: 0.6922976970672607\n",
      "validation loss: 0.6931703686714172\n",
      "training epoch nr 33\n",
      "training loss: 0.6921765208244324\n",
      "validation loss: 0.6932862401008606\n",
      "training epoch nr 34\n",
      "training loss: 0.6922177076339722\n",
      "validation loss: 0.6936110854148865\n",
      "training epoch nr 35\n",
      "training loss: 0.6921562552452087\n",
      "validation loss: 0.693200409412384\n",
      "training epoch nr 36\n",
      "training loss: 0.6921355724334717\n",
      "validation loss: 0.6933020353317261\n",
      "training epoch nr 37\n",
      "training loss: 0.6920908093452454\n",
      "validation loss: 0.6933436989784241\n",
      "training epoch nr 38\n",
      "training loss: 0.6920508146286011\n",
      "validation loss: 0.6930579543113708\n",
      "training epoch nr 39\n",
      "training loss: 0.692081093788147\n",
      "validation loss: 0.6933377981185913\n",
      "training epoch nr 40\n",
      "training loss: 0.692013680934906\n",
      "validation loss: 0.6932765245437622\n",
      "training epoch nr 41\n",
      "training loss: 0.6920127868652344\n",
      "validation loss: 0.693325400352478\n",
      "training epoch nr 42\n",
      "training loss: 0.6919970512390137\n",
      "validation loss: 0.6933016180992126\n",
      "training epoch nr 43\n",
      "training loss: 0.6919535994529724\n",
      "validation loss: 0.6935229301452637\n",
      "training epoch nr 44\n",
      "training loss: 0.691996693611145\n",
      "validation loss: 0.6935068964958191\n",
      "training epoch nr 45\n",
      "training loss: 0.6919050812721252\n",
      "validation loss: 0.6934177279472351\n",
      "training epoch nr 46\n",
      "training loss: 0.6918895244598389\n",
      "validation loss: 0.6937510371208191\n",
      "training epoch nr 47\n",
      "training loss: 0.6918627023696899\n",
      "validation loss: 0.6932448744773865\n",
      "training epoch nr 48\n",
      "training loss: 0.6918386220932007\n",
      "validation loss: 0.6935702562332153\n",
      "training epoch nr 49\n",
      "training loss: 0.6917734742164612\n",
      "validation loss: 0.6933925151824951\n",
      "training epoch nr 50\n",
      "training loss: 0.6917281746864319\n",
      "validation loss: 0.6937859654426575\n",
      "training epoch nr 51\n",
      "training loss: 0.6917867660522461\n",
      "validation loss: 0.6933537125587463\n",
      "training epoch nr 52\n",
      "training loss: 0.6917533874511719\n",
      "validation loss: 0.6938059329986572\n",
      "training epoch nr 53\n",
      "training loss: 0.6918136477470398\n",
      "validation loss: 0.6940008997917175\n",
      "training epoch nr 54\n",
      "training loss: 0.6916419863700867\n",
      "validation loss: 0.6935293078422546\n",
      "training epoch nr 55\n",
      "training loss: 0.6916518807411194\n",
      "validation loss: 0.6937254071235657\n",
      "training epoch nr 56\n",
      "training loss: 0.6916194558143616\n",
      "validation loss: 0.6936783194541931\n",
      "training epoch nr 57\n",
      "training loss: 0.6915350556373596\n",
      "validation loss: 0.693858802318573\n",
      "training epoch nr 58\n",
      "training loss: 0.6914762854576111\n",
      "validation loss: 0.6940171122550964\n",
      "training epoch nr 59\n",
      "training loss: 0.6915561556816101\n",
      "validation loss: 0.6941033005714417\n",
      "training epoch nr 60\n",
      "training loss: 0.6915748715400696\n",
      "validation loss: 0.6940258741378784\n",
      "training epoch nr 61\n",
      "training loss: 0.6914498209953308\n",
      "validation loss: 0.6941070556640625\n",
      "training epoch nr 62\n",
      "training loss: 0.6913297176361084\n",
      "validation loss: 0.6939245462417603\n",
      "training epoch nr 63\n",
      "training loss: 0.691415548324585\n",
      "validation loss: 0.6937752962112427\n",
      "training epoch nr 64\n",
      "training loss: 0.6913831233978271\n",
      "validation loss: 0.6941125392913818\n",
      "training epoch nr 65\n",
      "training loss: 0.6914129853248596\n",
      "validation loss: 0.6940473318099976\n",
      "training epoch nr 66\n",
      "training loss: 0.6913636326789856\n",
      "validation loss: 0.6939697265625\n",
      "training epoch nr 67\n",
      "training loss: 0.6913541555404663\n",
      "validation loss: 0.6939378976821899\n",
      "training epoch nr 68\n",
      "training loss: 0.6911766529083252\n",
      "validation loss: 0.6941378712654114\n",
      "training epoch nr 69\n",
      "training loss: 0.6911935210227966\n",
      "validation loss: 0.6939087510108948\n",
      "training epoch nr 70\n",
      "training loss: 0.6913134455680847\n",
      "validation loss: 0.6940253973007202\n",
      "training epoch nr 71\n",
      "training loss: 0.6913302540779114\n",
      "validation loss: 0.694053053855896\n",
      "training epoch nr 72\n",
      "training loss: 0.6911844611167908\n",
      "validation loss: 0.694300651550293\n",
      "training epoch nr 73\n",
      "training loss: 0.6912252306938171\n",
      "validation loss: 0.6939963102340698\n",
      "training epoch nr 74\n",
      "training loss: 0.6911627650260925\n",
      "validation loss: 0.694015622138977\n",
      "training epoch nr 75\n",
      "training loss: 0.6911851763725281\n",
      "validation loss: 0.6942662000656128\n",
      "training epoch nr 76\n",
      "training loss: 0.6911892294883728\n",
      "validation loss: 0.6939490437507629\n",
      "training epoch nr 77\n",
      "training loss: 0.6910727024078369\n",
      "validation loss: 0.6942590475082397\n",
      "training epoch nr 78\n",
      "training loss: 0.6911366581916809\n",
      "validation loss: 0.6940966248512268\n",
      "training epoch nr 79\n",
      "training loss: 0.6910457611083984\n",
      "validation loss: 0.6942628026008606\n",
      "training epoch nr 80\n",
      "training loss: 0.6910727024078369\n",
      "validation loss: 0.694179892539978\n",
      "training epoch nr 81\n",
      "training loss: 0.6910153031349182\n",
      "validation loss: 0.694485068321228\n",
      "training epoch nr 82\n",
      "training loss: 0.690955638885498\n",
      "validation loss: 0.6947876214981079\n",
      "training epoch nr 83\n",
      "training loss: 0.6909575462341309\n",
      "validation loss: 0.6948389410972595\n",
      "training epoch nr 84\n",
      "training loss: 0.6909580230712891\n",
      "validation loss: 0.6945329308509827\n",
      "training epoch nr 85\n",
      "training loss: 0.6909295916557312\n",
      "validation loss: 0.6951970458030701\n",
      "training epoch nr 86\n",
      "training loss: 0.6909234523773193\n",
      "validation loss: 0.6947228312492371\n",
      "training epoch nr 87\n",
      "training loss: 0.6907408833503723\n",
      "validation loss: 0.6946545839309692\n",
      "training epoch nr 88\n",
      "training loss: 0.6907822489738464\n",
      "validation loss: 0.6946309804916382\n",
      "training epoch nr 89\n",
      "training loss: 0.6907246708869934\n",
      "validation loss: 0.6942409873008728\n",
      "training epoch nr 90\n",
      "training loss: 0.6907699704170227\n",
      "validation loss: 0.6946781873703003\n",
      "training epoch nr 91\n",
      "training loss: 0.6907716989517212\n",
      "validation loss: 0.6950743794441223\n",
      "training epoch nr 92\n",
      "training loss: 0.6908154487609863\n",
      "validation loss: 0.6945648193359375\n",
      "training epoch nr 93\n",
      "training loss: 0.690614640712738\n",
      "validation loss: 0.6945623159408569\n",
      "training epoch nr 94\n",
      "training loss: 0.690639317035675\n",
      "validation loss: 0.6947105526924133\n",
      "training epoch nr 95\n",
      "training loss: 0.6906933188438416\n",
      "validation loss: 0.6947166323661804\n",
      "training epoch nr 96\n",
      "training loss: 0.6905795335769653\n",
      "validation loss: 0.6949036121368408\n",
      "training epoch nr 97\n",
      "training loss: 0.6905225515365601\n",
      "validation loss: 0.694849967956543\n",
      "training epoch nr 98\n",
      "training loss: 0.690650224685669\n",
      "validation loss: 0.69468092918396\n",
      "training epoch nr 99\n",
      "training loss: 0.6904321312904358\n",
      "validation loss: 0.6952773332595825\n",
      "Training model nr 8...\n",
      "training epoch nr 0\n",
      "training loss: 0.693300187587738\n",
      "validation loss: 0.6932499408721924\n",
      "training epoch nr 1\n",
      "training loss: 0.6931909322738647\n",
      "validation loss: 0.6931419372558594\n",
      "training epoch nr 2\n",
      "training loss: 0.6931783556938171\n",
      "validation loss: 0.6931323409080505\n",
      "training epoch nr 3\n",
      "training loss: 0.6931150555610657\n",
      "validation loss: 0.693123996257782\n",
      "training epoch nr 4\n",
      "training loss: 0.6931126713752747\n",
      "validation loss: 0.6930761933326721\n",
      "training epoch nr 5\n",
      "training loss: 0.6929689645767212\n",
      "validation loss: 0.6929693222045898\n",
      "training epoch nr 6\n",
      "training loss: 0.6929736137390137\n",
      "validation loss: 0.6930129528045654\n",
      "training epoch nr 7\n",
      "training loss: 0.6929356455802917\n",
      "validation loss: 0.6929214596748352\n",
      "training epoch nr 8\n",
      "training loss: 0.6929712891578674\n",
      "validation loss: 0.6929304003715515\n",
      "training epoch nr 9\n",
      "training loss: 0.692893385887146\n",
      "validation loss: 0.693039059638977\n",
      "training epoch nr 10\n",
      "training loss: 0.6928867101669312\n",
      "validation loss: 0.6929196119308472\n",
      "training epoch nr 11\n",
      "training loss: 0.6928366422653198\n",
      "validation loss: 0.6931023001670837\n",
      "training epoch nr 12\n",
      "training loss: 0.6927605271339417\n",
      "validation loss: 0.6929869055747986\n",
      "training epoch nr 13\n",
      "training loss: 0.6927934885025024\n",
      "validation loss: 0.6928825974464417\n",
      "training epoch nr 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6928734183311462\n",
      "validation loss: 0.6928950548171997\n",
      "training epoch nr 15\n",
      "training loss: 0.692744255065918\n",
      "validation loss: 0.6928768754005432\n",
      "training epoch nr 16\n",
      "training loss: 0.6927787661552429\n",
      "validation loss: 0.6929330229759216\n",
      "training epoch nr 17\n",
      "training loss: 0.69269198179245\n",
      "validation loss: 0.6929281949996948\n",
      "training epoch nr 18\n",
      "training loss: 0.6926786303520203\n",
      "validation loss: 0.6928935050964355\n",
      "training epoch nr 19\n",
      "training loss: 0.6926341652870178\n",
      "validation loss: 0.6930919885635376\n",
      "training epoch nr 20\n",
      "training loss: 0.6926212906837463\n",
      "validation loss: 0.6932684779167175\n",
      "training epoch nr 21\n",
      "training loss: 0.6926352977752686\n",
      "validation loss: 0.6930117607116699\n",
      "training epoch nr 22\n",
      "training loss: 0.6925264596939087\n",
      "validation loss: 0.6930029392242432\n",
      "training epoch nr 23\n",
      "training loss: 0.6926083564758301\n",
      "validation loss: 0.6934126615524292\n",
      "training epoch nr 24\n",
      "training loss: 0.69260174036026\n",
      "validation loss: 0.6929417252540588\n",
      "training epoch nr 25\n",
      "training loss: 0.6925498247146606\n",
      "validation loss: 0.6930720806121826\n",
      "training epoch nr 26\n",
      "training loss: 0.6924880146980286\n",
      "validation loss: 0.6930350661277771\n",
      "training epoch nr 27\n",
      "training loss: 0.6925660371780396\n",
      "validation loss: 0.6929700970649719\n",
      "training epoch nr 28\n",
      "training loss: 0.6924684643745422\n",
      "validation loss: 0.6930221915245056\n",
      "training epoch nr 29\n",
      "training loss: 0.692345917224884\n",
      "validation loss: 0.6931126117706299\n",
      "training epoch nr 30\n",
      "training loss: 0.6923955082893372\n",
      "validation loss: 0.6930822134017944\n",
      "training epoch nr 31\n",
      "training loss: 0.6923678517341614\n",
      "validation loss: 0.6930699348449707\n",
      "training epoch nr 32\n",
      "training loss: 0.692328155040741\n",
      "validation loss: 0.6931813359260559\n",
      "training epoch nr 33\n",
      "training loss: 0.6923694014549255\n",
      "validation loss: 0.6933144330978394\n",
      "training epoch nr 34\n",
      "training loss: 0.6923167109489441\n",
      "validation loss: 0.693213701248169\n",
      "training epoch nr 35\n",
      "training loss: 0.6922388076782227\n",
      "validation loss: 0.6931935548782349\n",
      "training epoch nr 36\n",
      "training loss: 0.6922827959060669\n",
      "validation loss: 0.6933165192604065\n",
      "training epoch nr 37\n",
      "training loss: 0.6922053694725037\n",
      "validation loss: 0.6936725378036499\n",
      "training epoch nr 38\n",
      "training loss: 0.6922329664230347\n",
      "validation loss: 0.6933239698410034\n",
      "training epoch nr 39\n",
      "training loss: 0.6921780109405518\n",
      "validation loss: 0.693578839302063\n",
      "training epoch nr 40\n",
      "training loss: 0.6921467781066895\n",
      "validation loss: 0.6937628984451294\n",
      "training epoch nr 41\n",
      "training loss: 0.6921747922897339\n",
      "validation loss: 0.6932703852653503\n",
      "training epoch nr 42\n",
      "training loss: 0.692034900188446\n",
      "validation loss: 0.6936230659484863\n",
      "training epoch nr 43\n",
      "training loss: 0.6920483112335205\n",
      "validation loss: 0.6933265924453735\n",
      "training epoch nr 44\n",
      "training loss: 0.6920544505119324\n",
      "validation loss: 0.6934881210327148\n",
      "training epoch nr 45\n",
      "training loss: 0.692113995552063\n",
      "validation loss: 0.6934738159179688\n",
      "training epoch nr 46\n",
      "training loss: 0.6919247508049011\n",
      "validation loss: 0.6935927867889404\n",
      "training epoch nr 47\n",
      "training loss: 0.6918904185295105\n",
      "validation loss: 0.6935771107673645\n",
      "training epoch nr 48\n",
      "training loss: 0.691779613494873\n",
      "validation loss: 0.6936518549919128\n",
      "training epoch nr 49\n",
      "training loss: 0.6919607520103455\n",
      "validation loss: 0.6935725212097168\n",
      "training epoch nr 50\n",
      "training loss: 0.6917778849601746\n",
      "validation loss: 0.693579912185669\n",
      "training epoch nr 51\n",
      "training loss: 0.6918269991874695\n",
      "validation loss: 0.6939533352851868\n",
      "training epoch nr 52\n",
      "training loss: 0.6918280720710754\n",
      "validation loss: 0.6938260197639465\n",
      "training epoch nr 53\n",
      "training loss: 0.6917456388473511\n",
      "validation loss: 0.693695604801178\n",
      "training epoch nr 54\n",
      "training loss: 0.6918507814407349\n",
      "validation loss: 0.6937674880027771\n",
      "training epoch nr 55\n",
      "training loss: 0.6917663216590881\n",
      "validation loss: 0.6936450004577637\n",
      "training epoch nr 56\n",
      "training loss: 0.6916630268096924\n",
      "validation loss: 0.6937049627304077\n",
      "training epoch nr 57\n",
      "training loss: 0.6916808485984802\n",
      "validation loss: 0.6947165131568909\n",
      "training epoch nr 58\n",
      "training loss: 0.6916465163230896\n",
      "validation loss: 0.6939094662666321\n",
      "training epoch nr 59\n",
      "training loss: 0.6916981339454651\n",
      "validation loss: 0.6939778923988342\n",
      "training epoch nr 60\n",
      "training loss: 0.6916446685791016\n",
      "validation loss: 0.6940401792526245\n",
      "training epoch nr 61\n",
      "training loss: 0.6916239261627197\n",
      "validation loss: 0.6939891576766968\n",
      "training epoch nr 62\n",
      "training loss: 0.6914602518081665\n",
      "validation loss: 0.6937484741210938\n",
      "training epoch nr 63\n",
      "training loss: 0.6914462447166443\n",
      "validation loss: 0.694087028503418\n",
      "training epoch nr 64\n",
      "training loss: 0.6914432048797607\n",
      "validation loss: 0.693946361541748\n",
      "training epoch nr 65\n",
      "training loss: 0.6914669871330261\n",
      "validation loss: 0.6942307353019714\n",
      "training epoch nr 66\n",
      "training loss: 0.6914195418357849\n",
      "validation loss: 0.6944812536239624\n",
      "training epoch nr 67\n",
      "training loss: 0.6913421750068665\n",
      "validation loss: 0.6940646767616272\n",
      "training epoch nr 68\n",
      "training loss: 0.6913179159164429\n",
      "validation loss: 0.6941546201705933\n",
      "training epoch nr 69\n",
      "training loss: 0.6913104057312012\n",
      "validation loss: 0.6942471265792847\n",
      "training epoch nr 70\n",
      "training loss: 0.6913136839866638\n",
      "validation loss: 0.6940451264381409\n",
      "training epoch nr 71\n",
      "training loss: 0.6911489367485046\n",
      "validation loss: 0.6941649317741394\n",
      "training epoch nr 72\n",
      "training loss: 0.6912890076637268\n",
      "validation loss: 0.6943761706352234\n",
      "training epoch nr 73\n",
      "training loss: 0.6912487745285034\n",
      "validation loss: 0.6945074200630188\n",
      "training epoch nr 74\n",
      "training loss: 0.6912333369255066\n",
      "validation loss: 0.694158673286438\n",
      "training epoch nr 75\n",
      "training loss: 0.6911051869392395\n",
      "validation loss: 0.6943359971046448\n",
      "training epoch nr 76\n",
      "training loss: 0.6910368800163269\n",
      "validation loss: 0.6942204833030701\n",
      "training epoch nr 77\n",
      "training loss: 0.6910833716392517\n",
      "validation loss: 0.6944896578788757\n",
      "training epoch nr 78\n",
      "training loss: 0.691067636013031\n",
      "validation loss: 0.694870114326477\n",
      "training epoch nr 79\n",
      "training loss: 0.6909677386283875\n",
      "validation loss: 0.6947938203811646\n",
      "training epoch nr 80\n",
      "training loss: 0.6909921765327454\n",
      "validation loss: 0.6943141222000122\n",
      "training epoch nr 81\n",
      "training loss: 0.6908603310585022\n",
      "validation loss: 0.6944994926452637\n",
      "training epoch nr 82\n",
      "training loss: 0.6909189224243164\n",
      "validation loss: 0.6944751739501953\n",
      "training epoch nr 83\n",
      "training loss: 0.6908325552940369\n",
      "validation loss: 0.6947733163833618\n",
      "training epoch nr 84\n",
      "training loss: 0.6908328533172607\n",
      "validation loss: 0.6950922012329102\n",
      "training epoch nr 85\n",
      "training loss: 0.6907200217247009\n",
      "validation loss: 0.6944775581359863\n",
      "training epoch nr 86\n",
      "training loss: 0.6907519102096558\n",
      "validation loss: 0.694892942905426\n",
      "training epoch nr 87\n",
      "training loss: 0.690625786781311\n",
      "validation loss: 0.69455486536026\n",
      "training epoch nr 88\n",
      "training loss: 0.6907920241355896\n",
      "validation loss: 0.6949067711830139\n",
      "training epoch nr 89\n",
      "training loss: 0.6905888915061951\n",
      "validation loss: 0.695256769657135\n",
      "training epoch nr 90\n",
      "training loss: 0.6906716227531433\n",
      "validation loss: 0.6948637962341309\n",
      "training epoch nr 91\n",
      "training loss: 0.690526008605957\n",
      "validation loss: 0.695254921913147\n",
      "training epoch nr 92\n",
      "training loss: 0.690592885017395\n",
      "validation loss: 0.6955081820487976\n",
      "training epoch nr 93\n",
      "training loss: 0.6906009912490845\n",
      "validation loss: 0.6949232220649719\n",
      "training epoch nr 94\n",
      "training loss: 0.6904718279838562\n",
      "validation loss: 0.6949840784072876\n",
      "training epoch nr 95\n",
      "training loss: 0.6904617547988892\n",
      "validation loss: 0.6951028108596802\n",
      "training epoch nr 96\n",
      "training loss: 0.6904696822166443\n",
      "validation loss: 0.6946548223495483\n",
      "training epoch nr 97\n",
      "training loss: 0.6902866363525391\n",
      "validation loss: 0.6949940323829651\n",
      "training epoch nr 98\n",
      "training loss: 0.6904323101043701\n",
      "validation loss: 0.6948876976966858\n",
      "training epoch nr 99\n",
      "training loss: 0.6902631521224976\n",
      "validation loss: 0.6956796050071716\n",
      "Training model nr 9...\n",
      "training epoch nr 0\n",
      "training loss: 0.6932411193847656\n",
      "validation loss: 0.6931774020195007\n",
      "training epoch nr 1\n",
      "training loss: 0.6931667923927307\n",
      "validation loss: 0.6931683421134949\n",
      "training epoch nr 2\n",
      "training loss: 0.6931445598602295\n",
      "validation loss: 0.6931152939796448\n",
      "training epoch nr 3\n",
      "training loss: 0.6931687593460083\n",
      "validation loss: 0.6931107640266418\n",
      "training epoch nr 4\n",
      "training loss: 0.6930505037307739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.693078339099884\n",
      "training epoch nr 5\n",
      "training loss: 0.6929639577865601\n",
      "validation loss: 0.6930943131446838\n",
      "training epoch nr 6\n",
      "training loss: 0.6929739713668823\n",
      "validation loss: 0.6930409073829651\n",
      "training epoch nr 7\n",
      "training loss: 0.6929610967636108\n",
      "validation loss: 0.6932111382484436\n",
      "training epoch nr 8\n",
      "training loss: 0.6929855346679688\n",
      "validation loss: 0.6929931044578552\n",
      "training epoch nr 9\n",
      "training loss: 0.6929386258125305\n",
      "validation loss: 0.6929919123649597\n",
      "training epoch nr 10\n",
      "training loss: 0.6928797364234924\n",
      "validation loss: 0.6929677724838257\n",
      "training epoch nr 11\n",
      "training loss: 0.692887008190155\n",
      "validation loss: 0.6930912137031555\n",
      "training epoch nr 12\n",
      "training loss: 0.6928954124450684\n",
      "validation loss: 0.6929931044578552\n",
      "training epoch nr 13\n",
      "training loss: 0.6928495168685913\n",
      "validation loss: 0.6930425763130188\n",
      "training epoch nr 14\n",
      "training loss: 0.692820131778717\n",
      "validation loss: 0.6930117011070251\n",
      "training epoch nr 15\n",
      "training loss: 0.6928517818450928\n",
      "validation loss: 0.6930038928985596\n",
      "training epoch nr 16\n",
      "training loss: 0.6927904486656189\n",
      "validation loss: 0.692930281162262\n",
      "training epoch nr 17\n",
      "training loss: 0.6927387118339539\n",
      "validation loss: 0.6928749680519104\n",
      "training epoch nr 18\n",
      "training loss: 0.6928375959396362\n",
      "validation loss: 0.6931166052818298\n",
      "training epoch nr 19\n",
      "training loss: 0.6926812529563904\n",
      "validation loss: 0.6929180026054382\n",
      "training epoch nr 20\n",
      "training loss: 0.6926752924919128\n",
      "validation loss: 0.6929822564125061\n",
      "training epoch nr 21\n",
      "training loss: 0.6926693320274353\n",
      "validation loss: 0.692866325378418\n",
      "training epoch nr 22\n",
      "training loss: 0.6926777958869934\n",
      "validation loss: 0.6928778290748596\n",
      "training epoch nr 23\n",
      "training loss: 0.6925831437110901\n",
      "validation loss: 0.6930730938911438\n",
      "training epoch nr 24\n",
      "training loss: 0.6924957633018494\n",
      "validation loss: 0.693104088306427\n",
      "training epoch nr 25\n",
      "training loss: 0.6925674080848694\n",
      "validation loss: 0.6930790543556213\n",
      "training epoch nr 26\n",
      "training loss: 0.6925401091575623\n",
      "validation loss: 0.6930374503135681\n",
      "training epoch nr 27\n",
      "training loss: 0.6924470067024231\n",
      "validation loss: 0.6929983496665955\n",
      "training epoch nr 28\n",
      "training loss: 0.6924025416374207\n",
      "validation loss: 0.6929652690887451\n",
      "training epoch nr 29\n",
      "training loss: 0.6923941969871521\n",
      "validation loss: 0.6934914588928223\n",
      "training epoch nr 30\n",
      "training loss: 0.6924542188644409\n",
      "validation loss: 0.6932299733161926\n",
      "training epoch nr 31\n",
      "training loss: 0.6923770904541016\n",
      "validation loss: 0.6930610537528992\n",
      "training epoch nr 32\n",
      "training loss: 0.6923670768737793\n",
      "validation loss: 0.6932151913642883\n",
      "training epoch nr 33\n",
      "training loss: 0.6922955513000488\n",
      "validation loss: 0.6931657195091248\n",
      "training epoch nr 34\n",
      "training loss: 0.692277193069458\n",
      "validation loss: 0.6932345032691956\n",
      "training epoch nr 35\n",
      "training loss: 0.6922850608825684\n",
      "validation loss: 0.6932960152626038\n",
      "training epoch nr 36\n",
      "training loss: 0.6922613978385925\n",
      "validation loss: 0.693290650844574\n",
      "training epoch nr 37\n",
      "training loss: 0.6921918988227844\n",
      "validation loss: 0.6930770874023438\n",
      "training epoch nr 38\n",
      "training loss: 0.6921898126602173\n",
      "validation loss: 0.6932961344718933\n",
      "training epoch nr 39\n",
      "training loss: 0.6922053694725037\n",
      "validation loss: 0.6932476162910461\n",
      "training epoch nr 40\n",
      "training loss: 0.692183256149292\n",
      "validation loss: 0.6932605504989624\n",
      "training epoch nr 41\n",
      "training loss: 0.69209223985672\n",
      "validation loss: 0.6934488415718079\n",
      "training epoch nr 42\n",
      "training loss: 0.6920605897903442\n",
      "validation loss: 0.6933962106704712\n",
      "training epoch nr 43\n",
      "training loss: 0.6920392513275146\n",
      "validation loss: 0.6933650374412537\n",
      "training epoch nr 44\n",
      "training loss: 0.691961944103241\n",
      "validation loss: 0.693341851234436\n",
      "training epoch nr 45\n",
      "training loss: 0.6919819116592407\n",
      "validation loss: 0.6932777762413025\n",
      "training epoch nr 46\n",
      "training loss: 0.6919721364974976\n",
      "validation loss: 0.6935207843780518\n",
      "training epoch nr 47\n",
      "training loss: 0.6919828653335571\n",
      "validation loss: 0.6935446858406067\n",
      "training epoch nr 48\n",
      "training loss: 0.6919012069702148\n",
      "validation loss: 0.6938366293907166\n",
      "training epoch nr 49\n",
      "training loss: 0.691746711730957\n",
      "validation loss: 0.6937313675880432\n",
      "training epoch nr 50\n",
      "training loss: 0.6918283700942993\n",
      "validation loss: 0.6935055255889893\n",
      "training epoch nr 51\n",
      "training loss: 0.6918721795082092\n",
      "validation loss: 0.6934145092964172\n",
      "training epoch nr 52\n",
      "training loss: 0.6917724609375\n",
      "validation loss: 0.693706750869751\n",
      "training epoch nr 53\n",
      "training loss: 0.6916835308074951\n",
      "validation loss: 0.6937036514282227\n",
      "training epoch nr 54\n",
      "training loss: 0.6917917728424072\n",
      "validation loss: 0.6935904622077942\n",
      "training epoch nr 55\n",
      "training loss: 0.69175124168396\n",
      "validation loss: 0.6936692595481873\n",
      "training epoch nr 56\n",
      "training loss: 0.6916448473930359\n",
      "validation loss: 0.6938284039497375\n",
      "training epoch nr 57\n",
      "training loss: 0.6916034817695618\n",
      "validation loss: 0.6939606070518494\n",
      "training epoch nr 58\n",
      "training loss: 0.6916743516921997\n",
      "validation loss: 0.6939303874969482\n",
      "training epoch nr 59\n",
      "training loss: 0.6915889978408813\n",
      "validation loss: 0.6936423182487488\n",
      "training epoch nr 60\n",
      "training loss: 0.6915647387504578\n",
      "validation loss: 0.6938527226448059\n",
      "training epoch nr 61\n",
      "training loss: 0.691511869430542\n",
      "validation loss: 0.6940327286720276\n",
      "training epoch nr 62\n",
      "training loss: 0.6914047598838806\n",
      "validation loss: 0.6940860152244568\n",
      "training epoch nr 63\n",
      "training loss: 0.6915073990821838\n",
      "validation loss: 0.6940967440605164\n",
      "training epoch nr 64\n",
      "training loss: 0.6914328932762146\n",
      "validation loss: 0.6941036581993103\n",
      "training epoch nr 65\n",
      "training loss: 0.691411554813385\n",
      "validation loss: 0.6938703060150146\n",
      "training epoch nr 66\n",
      "training loss: 0.6914231181144714\n",
      "validation loss: 0.6942018866539001\n",
      "training epoch nr 67\n",
      "training loss: 0.6913043856620789\n",
      "validation loss: 0.6941095590591431\n",
      "training epoch nr 68\n",
      "training loss: 0.691365122795105\n",
      "validation loss: 0.6940569877624512\n",
      "training epoch nr 69\n",
      "training loss: 0.6913396716117859\n",
      "validation loss: 0.6940848231315613\n",
      "training epoch nr 70\n",
      "training loss: 0.6911633014678955\n",
      "validation loss: 0.6946319937705994\n",
      "training epoch nr 71\n",
      "training loss: 0.6912796497344971\n",
      "validation loss: 0.6939097046852112\n",
      "training epoch nr 72\n",
      "training loss: 0.6911262273788452\n",
      "validation loss: 0.6941169500350952\n",
      "training epoch nr 73\n",
      "training loss: 0.6910973787307739\n",
      "validation loss: 0.6944016814231873\n",
      "training epoch nr 74\n",
      "training loss: 0.6911213397979736\n",
      "validation loss: 0.6944870948791504\n",
      "training epoch nr 75\n",
      "training loss: 0.6910841464996338\n",
      "validation loss: 0.69440758228302\n",
      "training epoch nr 76\n",
      "training loss: 0.691122829914093\n",
      "validation loss: 0.6941529512405396\n",
      "training epoch nr 77\n",
      "training loss: 0.6909589171409607\n",
      "validation loss: 0.6944473385810852\n",
      "training epoch nr 78\n",
      "training loss: 0.6909664273262024\n",
      "validation loss: 0.6944951415061951\n",
      "training epoch nr 79\n",
      "training loss: 0.6909357309341431\n",
      "validation loss: 0.6942332983016968\n",
      "training epoch nr 80\n",
      "training loss: 0.6910155415534973\n",
      "validation loss: 0.6945741772651672\n",
      "training epoch nr 81\n",
      "training loss: 0.6908499002456665\n",
      "validation loss: 0.694556474685669\n",
      "training epoch nr 82\n",
      "training loss: 0.6908239722251892\n",
      "validation loss: 0.6949560642242432\n",
      "training epoch nr 83\n",
      "training loss: 0.6907971501350403\n",
      "validation loss: 0.6951513886451721\n",
      "training epoch nr 84\n",
      "training loss: 0.6906788945198059\n",
      "validation loss: 0.6944351196289062\n",
      "training epoch nr 85\n",
      "training loss: 0.6905795335769653\n",
      "validation loss: 0.6952106356620789\n",
      "training epoch nr 86\n",
      "training loss: 0.6907423138618469\n",
      "validation loss: 0.6949971318244934\n",
      "training epoch nr 87\n",
      "training loss: 0.6905838251113892\n",
      "validation loss: 0.6949072480201721\n",
      "training epoch nr 88\n",
      "training loss: 0.6906290650367737\n",
      "validation loss: 0.6948356032371521\n",
      "training epoch nr 89\n",
      "training loss: 0.690585732460022\n",
      "validation loss: 0.6946526169776917\n",
      "training epoch nr 90\n",
      "training loss: 0.6906265616416931\n",
      "validation loss: 0.6947433948516846\n",
      "training epoch nr 91\n",
      "training loss: 0.6906945109367371\n",
      "validation loss: 0.6945836544036865\n",
      "training epoch nr 92\n",
      "training loss: 0.6904535293579102\n",
      "validation loss: 0.6950370073318481\n",
      "training epoch nr 93\n",
      "training loss: 0.6905221939086914\n",
      "validation loss: 0.6948347687721252\n",
      "training epoch nr 94\n",
      "training loss: 0.6904875636100769\n",
      "validation loss: 0.6955506205558777\n",
      "training epoch nr 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6906145811080933\n",
      "validation loss: 0.694676399230957\n",
      "training epoch nr 96\n",
      "training loss: 0.6904985308647156\n",
      "validation loss: 0.6948760747909546\n",
      "training epoch nr 97\n",
      "training loss: 0.6904053688049316\n",
      "validation loss: 0.6950919032096863\n",
      "training epoch nr 98\n",
      "training loss: 0.6903219819068909\n",
      "validation loss: 0.6956607103347778\n",
      "training epoch nr 99\n",
      "training loss: 0.6902612447738647\n",
      "validation loss: 0.6952342391014099\n",
      "minimum validation loss epochs: [ 8 23 21 20 14 13 12  9 11 17]\n",
      "minimum validation loss epochs: [13 27 24 23 18 26 11 22 30 16]\n",
      "minimum validation loss epochs: [18 17 24 16 25 12 20 14 11 19]\n",
      "minimum validation loss epochs: [16  9 18 17 15 14  6 10 19  8]\n",
      "minimum validation loss epochs: [15 26 19 23 17 31 16 12 10 29]\n",
      "minimum validation loss epochs: [11 21 25 18 17 16 15 13 12 29]\n",
      "minimum validation loss epochs: [21  8 16 15 23 12  9 18 20 19]\n",
      "minimum validation loss epochs: [16 12 19 17 26 14 18  8 13 11]\n",
      "minimum validation loss epochs: [10 13 14 15 18  7 17  8 16 24]\n",
      "minimum validation loss epochs: [ 9 19 16 22 21 20 28 17 10 12]\n"
     ]
    }
   ],
   "source": [
    "train_classifier(**classifier_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a650ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from evaluation_utils import load_predictions, tprs_fprs_sics, minumum_validation_loss_ensemble, compare_on_various_runs\n",
    "# evalautes the ROCs and SICs of one given training with minimum validation loss epoch picking\n",
    "def full_single_evaluation(data_dir, prediction_dir, n_ensemble_epochs=10, extra_signal=True,\n",
    "                           sic_range=(0,20), savefig=None, suppress_show=False, return_all=False):\n",
    "    X_test, y_test, predictions, val_losses = load_predictions(\n",
    "        data_dir, prediction_dir, extra_signal=extra_signal)\n",
    "    if predictions.shape[1]==1: ## check if ensembling done already\n",
    "        min_val_loss_predictions = predictions\n",
    "    else:\n",
    "        min_val_loss_predictions = minumum_validation_loss_ensemble(\n",
    "            predictions, val_losses, n_epochs=n_ensemble_epochs)\n",
    "    tprs, fprs, sics = tprs_fprs_sics(min_val_loss_predictions, y_test, X_test)\n",
    "\n",
    "    return compare_on_various_runs(\n",
    "        [tprs], [fprs], [np.zeros(min_val_loss_predictions.shape[0])], [\"\"],\n",
    "        sic_lim=sic_range, savefig=savefig, only_median=False, continuous_colors=False,\n",
    "        reduced_legend=False, suppress_show=suppress_show, return_all=return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e66ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = full_single_evaluation(save_dir, save_dir, n_ensemble_epochs=10,\n",
    "                           extra_signal=not no_extra_signal, sic_range=(0, 20),\n",
    "                           savefig=os.path.join(save_dir, 'result_SIC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3924273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
