{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc0d8ed",
   "metadata": {},
   "source": [
    "# Notebook 02: Pipeline for supervised results\n",
    "\n",
    "This notebook goes through the pipeline for obtaining results using the supervised method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9c5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from run_ANODE_training import main as train_DE\n",
    "from run_classifier_data_creation import main as create_data\n",
    "from run_classifier_training import main as train_classifier\n",
    "from run_ANODE_evaluation import main as eval_ANODE\n",
    "from evaluation_utils import full_single_evaluation, classic_ANODE_eval, minimum_val_loss_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16125ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'supervised'\n",
    "data_dir = '../separated_data'\n",
    "save_dir = 'supervised_models'\n",
    "# Shift on jet mass variables to be applied.\n",
    "datashift = 0.\n",
    "# Shift is not correlated to the actual mjj but randomized.\n",
    "random_shift = False\n",
    "# Whether to apply an (ANODE paper) fiducial cut on the data (and samples).\n",
    "fiducial_cut = False\n",
    "# Suppress the processing of the extra signal sample.\n",
    "no_extra_signal = True\n",
    "verbose = False\n",
    "\n",
    "# ANODE model config file (.yml).\n",
    "DE_config_file = '../DE_MAF_model.yml'\n",
    "# 'Number of Density Estimation training epochs.'\n",
    "DE_epochs = 100\n",
    "# Batch size during density estimation training.\n",
    "DE_batch_size = 256\n",
    "# Skips the density estimation (loads existing files instead).\n",
    "DF_skip = False\n",
    "# Turns off the logit transform in the density estimator.\n",
    "DE_no_logit = False\n",
    "\n",
    "# File name for the density estimator.\n",
    "DE_file_name = 'my_ANODE_model'\n",
    "\n",
    "# Classifier model config file (.yml).\n",
    "cf_config_file = '../classifier.yml'\n",
    "\n",
    "# Number of classifier training epochs\n",
    "cf_epochs = 100\n",
    "# Number of samples to be generated. Currently the samples will be cut down to match data proportion.\n",
    "cf_n_samples = 130000\n",
    "\n",
    "# Sample the conditional from a KDE fit rather than a uniform distribution.\n",
    "cf_realistic_conditional = False\n",
    "# Bandwith of the KDE fit (used when realistic_conditional is selected)\n",
    "cf_KDE_bandwidth = 0.01\n",
    "# Add the full number of samples to the training set rather than mixing it in equal parts with data.\n",
    "cf_oversampling = True\n",
    "# Turns off logit tranform in the classifier.\n",
    "cf_no_logit = True\n",
    "# Space-separated list of pre-sampled npy files of physical variables if the sampling has been done externally. The format is \n",
    "# (mjj, mj1, dmj, tau21_1, tau21_2)\n",
    "cf_external_samples = \"\"\n",
    "# Lower boundary of signal region.\n",
    "cf_SR_min = 3.3\n",
    "# Upper boundary of signal region.\n",
    "cf_SR_max = 3.7\n",
    "# Number of independent classifier training runs.\n",
    "cf_n_runs = 10\n",
    "# Batch size during classifier training.\n",
    "cf_batch_size = 128\n",
    "# Use the conditional variable as classifier input during training.\n",
    "cf_use_mjj = False\n",
    "# Weight the classes according to their occurence in the training set. \n",
    "# Necessary if the training set was intentionally oversampled.'\n",
    "cf_use_class_weights = False\n",
    "# Central value of signal region. Must only be given for using CWoLa with weights.\n",
    "cf_SR_center = 3.5\n",
    "# Make use of extra background (for supervised and idealized AD).\n",
    "cf_extra_bkg = True\n",
    "# Define a separate validation set to pick the classifier epochs.\n",
    "cf_separate_val_set = True\n",
    "# Save the tensorflow model after each epoch instead of saving predictions.\n",
    "cf_save_model = True\n",
    "# Skips the creation of the classifier dataset (loads existing files instead).\n",
    "cf_skip_create = False\n",
    "# Skips the training of the classifier (loads existing files instead).\n",
    "cf_skip_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb5d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_creation_kwargs = {\n",
    "    'savedir': save_dir,\n",
    "    'datashift': datashift,\n",
    "    'data_dir': data_dir,\n",
    "    'random_shift': random_shift,\n",
    "    'config_file': DE_config_file,\n",
    "    'verbose': verbose,\n",
    "    'fiducial_cut': fiducial_cut,\n",
    "    'n_samples': cf_n_samples,\n",
    "    'realistic_conditional': cf_realistic_conditional,\n",
    "    'KDE_bandwidth': cf_KDE_bandwidth,\n",
    "    'oversampling': cf_oversampling,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'CWoLa': False,\n",
    "    'supervised': True,\n",
    "    'idealized_AD': False,\n",
    "    'no_logit': cf_no_logit,\n",
    "    'no_logit_trained': DE_no_logit,\n",
    "    'external_samples': cf_external_samples,\n",
    "    'SR_min': cf_SR_min,\n",
    "    'SR_max': cf_SR_max,\n",
    "    'extra_bkg': cf_extra_bkg,\n",
    "    'separate_val_set': cf_separate_val_set,\n",
    "    'ANODE_models': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b369791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data_handler import LHCORD_data_handler, sample_handler, mix_data_samples, plot_data_sample_comparison\n",
    "from density_estimator import DensityEstimator\n",
    "\n",
    "def create_data(**kwargs):\n",
    "\n",
    "    assert not ((not (kwargs['supervised'] or kwargs['idealized_AD'] or kwargs['CWoLa']) and\\\n",
    "                 kwargs['external_samples'] == \"\") and kwargs['ANODE_models'] == \"\"), (\n",
    "                     \"ANODE models need to be given unless CWoLa, supervised, idealized_AD or\"\n",
    "                     \" external sampling is used.\")\n",
    "\n",
    "    # selecting appropriate device\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    print(\"cuda available:\", CUDA)\n",
    "    device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "\n",
    "    # checking for data separation\n",
    "    data_files = os.listdir(kwargs['data_dir'])\n",
    "    if \"innerdata_val.npy\" in data_files:\n",
    "        finer_data_split = True\n",
    "    else:\n",
    "        finer_data_split = False\n",
    "\n",
    "    if finer_data_split:\n",
    "        innerdata_train_path = [os.path.join(kwargs['data_dir'], 'innerdata_train.npy')]\n",
    "        innerdata_val_path = [os.path.join(kwargs['data_dir'], 'innerdata_val.npy')]\n",
    "        innerdata_test_path = [os.path.join(kwargs['data_dir'], 'innerdata_test.npy')]\n",
    "        if \"innerdata_extrabkg_test.npy\" in data_files:\n",
    "            innerdata_test_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_test.npy'))\n",
    "        extrasig_path = None\n",
    "        if kwargs['supervised']:\n",
    "            innerdata_train_path = []\n",
    "            innerdata_val_path = []\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrasig_val.npy'))\n",
    "            innerdata_train_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'))\n",
    "            innerdata_val_path.append(os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy'))\n",
    "            extra_bkg = None\n",
    "        elif kwargs['idealized_AD']:\n",
    "            extra_bkg = [os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_train.npy'),\n",
    "                         os.path.join(kwargs['data_dir'], 'innerdata_extrabkg_val.npy')]\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "\n",
    "    else:\n",
    "        innerdata_train_path = os.path.join(kwargs['data_dir'], 'innerdata_train.npy')\n",
    "        extrasig_path = os.path.join(kwargs['data_dir'], 'innerdata_extrasig.npy')\n",
    "        if kwargs['extra_bkg']:\n",
    "            extra_bkg = os.path.join(kwargs['data_dir'], 'innerdata_extrabkg.npy')\n",
    "        else:\n",
    "            extra_bkg = None\n",
    "        innerdata_val_path = None\n",
    "        innerdata_test_path = os.path.join(kwargs['data_dir'], 'innerdata_test.npy')\n",
    "\n",
    "    # data preprocessing\n",
    "    data = LHCORD_data_handler(innerdata_train_path,\n",
    "                               innerdata_test_path,\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_train.npy'),\n",
    "                               os.path.join(kwargs['data_dir'], 'outerdata_test.npy'),\n",
    "                               extrasig_path,\n",
    "                               inner_extrabkg_path=extra_bkg,\n",
    "                               inner_val_path=innerdata_val_path,\n",
    "                               batch_size=256,\n",
    "                               device=device)\n",
    "    if kwargs['datashift'] != 0:\n",
    "        print(\"applying a datashift of\", kwargs['datashift'])\n",
    "        data.shift_data(kwargs['datashift'], constant_shift=False, random_shift=kwargs['random_shift'],\n",
    "                        shift_mj1=True, shift_dm=True, additional_shift=False)\n",
    "\n",
    "    if kwargs['CWoLa']:\n",
    "        # data preprocessing\n",
    "        samples = None\n",
    "        data.preprocess_CWoLa_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit'],\n",
    "                                   outer_range=(kwargs['SR_min']-0.2, kwargs['SR_max']+0.2))\n",
    "\n",
    "    else:\n",
    "        # data preprocessing\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'],\n",
    "                                   no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # model instantiation\n",
    "        if len(kwargs['external_samples']) > 0:\n",
    "            model_list = None\n",
    "            loaded_samples = [np.load(sample_path) for sample_path in kwargs['external_samples']]\n",
    "            external_sample = np.concatenate(loaded_samples)\n",
    "        else:\n",
    "            model_list = []\n",
    "            for model_path in kwargs['ANODE_models']:\n",
    "                anode = DensityEstimator(kwargs['config_file'],\n",
    "                                         eval_mode=True,\n",
    "                                         load_path=model_path,\n",
    "                                         device=device, verbose=kwargs['verbose'],\n",
    "                                         bound=kwargs['no_logit_trained'])\n",
    "                model_list.append(anode.model)\n",
    "            external_sample = None\n",
    "\n",
    "        # generate samples\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            uniform_cond = not kwargs['realistic_conditional']\n",
    "            samples = sample_handler(model_list, kwargs['n_samples'], data, cond_min=kwargs['SR_min'],\n",
    "                                     cond_max=kwargs['SR_max'], uniform_cond=uniform_cond,\n",
    "                                     external_sample=external_sample,\n",
    "                                     device=device, no_logit=kwargs['no_logit_trained'],\n",
    "                                     no_mean_shift=kwargs['no_logit_trained'],\n",
    "                                     KDE_bandwidth=kwargs['KDE_bandwidth'])\n",
    "        else:\n",
    "            samples = None\n",
    "\n",
    "        # redo data preprocessing if the classifier should not use logit but ANODE did\n",
    "        data.preprocess_ANODE_data(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                   no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "        # sample preprocessing\n",
    "        if not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "            samples.preprocess_samples(fiducial_cut=kwargs['fiducial_cut'], no_logit=kwargs['no_logit_trained'],\n",
    "                                       no_mean_shift=kwargs['no_logit_trained'])\n",
    "\n",
    "\n",
    "    # sample mixing\n",
    "    X_train, y_train, X_test, y_test, X_extrasig, y_extrasig = mix_data_samples(\n",
    "        data, samples_handler=samples, oversampling=kwargs['oversampling'],\n",
    "        savedir=kwargs['savedir'], CWoLa=kwargs['CWoLa'], supervised=kwargs['supervised'],\n",
    "        idealized_AD=kwargs['idealized_AD'], separate_val_set=kwargs['separate_val_set'] or finer_data_split)\n",
    "\n",
    "    # sanity checks\n",
    "    if not kwargs['CWoLa'] and not kwargs['supervised'] and not kwargs['idealized_AD']:\n",
    "        samples.sanity_check(savefig=os.path.join(kwargs['savedir'], \"sanity_check\"), suppress_show=True)\n",
    "        samples.sanity_check_after_cuts(savefig=os.path.join(kwargs['savedir'], \"sanity_check_cuts\"),\n",
    "                                        suppress_show=True)\n",
    "\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "        X_val = X_extrasig\n",
    "        if kwargs['supervised']:\n",
    "            y_train = X_train[:, -1]\n",
    "            y_test = X_test[:, -1]\n",
    "            y_val = X_val[:, -1]\n",
    "        else:\n",
    "            y_val = X_val[:, -2]\n",
    "        plot_data_sample_comparison(X_val, y_val, title=\"validation set\",\n",
    "                                    savefig=os.path.join(kwargs['savedir'],\n",
    "                                                         \"data_sample_comparison_val\"),\n",
    "                                    suppress_show=True)\n",
    "\n",
    "    plot_data_sample_comparison(X_train, y_train, title=\"training set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_train\"),\n",
    "                                suppress_show=True)\n",
    "    plot_data_sample_comparison(X_test, y_test, title=\"test set\",\n",
    "                                savefig=os.path.join(kwargs['savedir'], \"data_sample_comparison_test\"),\n",
    "                                suppress_show=True)\n",
    "\n",
    "    print(\"number of training data =\", X_train.shape[0])\n",
    "    print(\"number of test data =\", X_test.shape[0])\n",
    "    if not kwargs['no_extra_signal']:\n",
    "        if kwargs['supervised'] or kwargs['separate_val_set'] or finer_data_split:\n",
    "            print(\"number of validation data =\", X_val.shape[0])\n",
    "        elif extrasig_path is not None:\n",
    "            print(\"number of extra signal data =\", X_extrasig.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd60e4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cern.ch/work/c/chlcheng/local/miniconda/envs/root-latest/lib/python3.8/site-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "/afs/cern.ch/work/c/chlcheng/local/miniconda/envs/root-latest/lib/python3.8/site-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "/afs/cern.ch/work/c/chlcheng/local/miniconda/envs/root-latest/lib/python3.8/site-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data = 163668\n",
      "number of test data = 359949\n"
     ]
    }
   ],
   "source": [
    "create_data(**data_creation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fe263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_kwargs = {\n",
    "    'config_file': cf_config_file,\n",
    "    'data_dir': save_dir,\n",
    "    'savedir': save_dir,\n",
    "    'verbose': verbose,\n",
    "    'epochs': cf_epochs,\n",
    "    'n_runs': cf_n_runs,\n",
    "    'batch_size': cf_batch_size,\n",
    "    'no_extra_signal': no_extra_signal,\n",
    "    'use_mjj': cf_use_mjj,\n",
    "    'supervised': True,\n",
    "    'use_class_weights': cf_oversampling or cf_use_class_weights,\n",
    "    'CWoLa': False,\n",
    "    'SR_center': cf_SR_center,\n",
    "    'save_model': cf_save_model,\n",
    "    'separate_val_set': cf_separate_val_set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9bfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier_training_utils import train_n_models, plot_classifier_losses\n",
    "from evaluation_utils import minimum_val_loss_model_evaluation\n",
    "import matplotlib as mpl\n",
    "\n",
    "def train_classifier(**kwargs):\n",
    "\n",
    "    # loading the data\n",
    "    # TODO get rid of the y's since the information is fully included in X\n",
    "    X_train = np.load(os.path.join(kwargs['data_dir'], 'X_train.npy'))\n",
    "    X_test = np.load(os.path.join(kwargs['data_dir'], 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(kwargs['data_dir'], 'y_train.npy'))\n",
    "    y_test = np.load(os.path.join(kwargs['data_dir'], 'y_test.npy'))\n",
    "    if kwargs['no_extra_signal'] or kwargs['supervised']:\n",
    "        X_extrasig = None\n",
    "    else:\n",
    "        X_extrasig = np.load(os.path.join(kwargs['data_dir'], 'X_extrasig.npy'))\n",
    "    if kwargs['supervised'] or kwargs['separate_val_set']:\n",
    "        X_val = np.load(os.path.join(kwargs['data_dir'], 'X_validation.npy'))        \n",
    "    else:\n",
    "        X_val = None\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        if not os.path.exists(kwargs['savedir']):\n",
    "            os.makedirs(kwargs['savedir'])\n",
    "        save_model = os.path.join(kwargs['savedir'], \"model\")\n",
    "    else:\n",
    "        save_model = None\n",
    "\n",
    "    # actual training\n",
    "    loss_matris, val_loss_matris = train_n_models(\n",
    "        kwargs['n_runs'], kwargs['config_file'], kwargs['epochs'], X_train, y_train, X_test, y_test,\n",
    "        X_extrasig=X_extrasig, X_val=X_val, use_mjj=kwargs['use_mjj'], batch_size=kwargs['batch_size'],\n",
    "        supervised=kwargs['supervised'], use_class_weights=kwargs['use_class_weights'],\n",
    "        CWoLa=kwargs['CWoLa'], SR_center=kwargs['SR_center'], verbose=kwargs['verbose'],\n",
    "        savedir=kwargs['savedir'], save_model=save_model)\n",
    "\n",
    "    if kwargs['save_model']:\n",
    "        minimum_val_loss_model_evaluation(kwargs['data_dir'], kwargs['savedir'], n_epochs=10,\n",
    "                                use_mjj=kwargs['use_mjj'], extra_signal=not kwargs['no_extra_signal'])\n",
    "\n",
    "    for i in range(loss_matris.shape[0]):\n",
    "        plot_classifier_losses(\n",
    "            loss_matris[i], val_loss_matris[i],\n",
    "            savefig=save_model+\"_run\"+str(i)+\"_loss_plot\",\n",
    "            suppress_show=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee22390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model nr 0...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2777864336967468\n",
      "validation loss: 0.23900023102760315\n",
      "training epoch nr 1\n",
      "training loss: 0.23200424015522003\n",
      "validation loss: 0.2299966961145401\n",
      "training epoch nr 2\n",
      "training loss: 0.22715002298355103\n",
      "validation loss: 0.22659741342067719\n",
      "training epoch nr 3\n",
      "training loss: 0.2241223156452179\n",
      "validation loss: 0.2269790768623352\n",
      "training epoch nr 4\n",
      "training loss: 0.22208960354328156\n",
      "validation loss: 0.22076523303985596\n",
      "training epoch nr 5\n",
      "training loss: 0.22081901133060455\n",
      "validation loss: 0.2215927392244339\n",
      "training epoch nr 6\n",
      "training loss: 0.21962608397006989\n",
      "validation loss: 0.2197658121585846\n",
      "training epoch nr 7\n",
      "training loss: 0.21914184093475342\n",
      "validation loss: 0.21811790764331818\n",
      "training epoch nr 8\n",
      "training loss: 0.2185097187757492\n",
      "validation loss: 0.21846142411231995\n",
      "training epoch nr 9\n",
      "training loss: 0.21782803535461426\n",
      "validation loss: 0.21953751146793365\n",
      "training epoch nr 10\n",
      "training loss: 0.21742428839206696\n",
      "validation loss: 0.21798665821552277\n",
      "training epoch nr 11\n",
      "training loss: 0.21683841943740845\n",
      "validation loss: 0.2169232815504074\n",
      "training epoch nr 12\n",
      "training loss: 0.2167477011680603\n",
      "validation loss: 0.21861305832862854\n",
      "training epoch nr 13\n",
      "training loss: 0.21585451066493988\n",
      "validation loss: 0.22183676064014435\n",
      "training epoch nr 14\n",
      "training loss: 0.21598099172115326\n",
      "validation loss: 0.21705463528633118\n",
      "training epoch nr 15\n",
      "training loss: 0.21535144746303558\n",
      "validation loss: 0.21812984347343445\n",
      "training epoch nr 16\n",
      "training loss: 0.21512722969055176\n",
      "validation loss: 0.21996693313121796\n",
      "training epoch nr 17\n",
      "training loss: 0.2151096761226654\n",
      "validation loss: 0.21777386963367462\n",
      "training epoch nr 18\n",
      "training loss: 0.21450667083263397\n",
      "validation loss: 0.21737909317016602\n",
      "training epoch nr 19\n",
      "training loss: 0.21470148861408234\n",
      "validation loss: 0.21685421466827393\n",
      "training epoch nr 20\n",
      "training loss: 0.2142244577407837\n",
      "validation loss: 0.21817977726459503\n",
      "training epoch nr 21\n",
      "training loss: 0.21423467993736267\n",
      "validation loss: 0.21742579340934753\n",
      "training epoch nr 22\n",
      "training loss: 0.21402227878570557\n",
      "validation loss: 0.21886703372001648\n",
      "training epoch nr 23\n",
      "training loss: 0.21360494196414948\n",
      "validation loss: 0.21711507439613342\n",
      "training epoch nr 24\n",
      "training loss: 0.21312078833580017\n",
      "validation loss: 0.21723178029060364\n",
      "training epoch nr 25\n",
      "training loss: 0.21311841905117035\n",
      "validation loss: 0.21729159355163574\n",
      "training epoch nr 26\n",
      "training loss: 0.2130972146987915\n",
      "validation loss: 0.21817775070667267\n",
      "training epoch nr 27\n",
      "training loss: 0.21323712170124054\n",
      "validation loss: 0.21745406091213226\n",
      "training epoch nr 28\n",
      "training loss: 0.21286657452583313\n",
      "validation loss: 0.21595920622348785\n",
      "training epoch nr 29\n",
      "training loss: 0.21273286640644073\n",
      "validation loss: 0.21649377048015594\n",
      "training epoch nr 30\n",
      "training loss: 0.21251162886619568\n",
      "validation loss: 0.21588650345802307\n",
      "training epoch nr 31\n",
      "training loss: 0.21262484788894653\n",
      "validation loss: 0.21648888289928436\n",
      "training epoch nr 32\n",
      "training loss: 0.21221685409545898\n",
      "validation loss: 0.21548037230968475\n",
      "training epoch nr 33\n",
      "training loss: 0.21185734868049622\n",
      "validation loss: 0.21629084646701813\n",
      "training epoch nr 34\n",
      "training loss: 0.21167324483394623\n",
      "validation loss: 0.21771810948848724\n",
      "training epoch nr 35\n",
      "training loss: 0.21168912947177887\n",
      "validation loss: 0.21747231483459473\n",
      "training epoch nr 36\n",
      "training loss: 0.21171320974826813\n",
      "validation loss: 0.21676349639892578\n",
      "training epoch nr 37\n",
      "training loss: 0.2117461860179901\n",
      "validation loss: 0.21574877202510834\n",
      "training epoch nr 38\n",
      "training loss: 0.2113070785999298\n",
      "validation loss: 0.2185565084218979\n",
      "training epoch nr 39\n",
      "training loss: 0.21141283214092255\n",
      "validation loss: 0.21698720753192902\n",
      "training epoch nr 40\n",
      "training loss: 0.21099889278411865\n",
      "validation loss: 0.2187695950269699\n",
      "training epoch nr 41\n",
      "training loss: 0.2110942155122757\n",
      "validation loss: 0.21839648485183716\n",
      "training epoch nr 42\n",
      "training loss: 0.21116480231285095\n",
      "validation loss: 0.21784284710884094\n",
      "training epoch nr 43\n",
      "training loss: 0.21080468595027924\n",
      "validation loss: 0.21719689667224884\n",
      "training epoch nr 44\n",
      "training loss: 0.21097512543201447\n",
      "validation loss: 0.21608838438987732\n",
      "training epoch nr 45\n",
      "training loss: 0.21058717370033264\n",
      "validation loss: 0.2182098925113678\n",
      "training epoch nr 46\n",
      "training loss: 0.21047934889793396\n",
      "validation loss: 0.21648555994033813\n",
      "training epoch nr 47\n",
      "training loss: 0.21038822829723358\n",
      "validation loss: 0.2204422652721405\n",
      "training epoch nr 48\n",
      "training loss: 0.2104082703590393\n",
      "validation loss: 0.21983729302883148\n",
      "training epoch nr 49\n",
      "training loss: 0.21030452847480774\n",
      "validation loss: 0.21606196463108063\n",
      "training epoch nr 50\n",
      "training loss: 0.2100764960050583\n",
      "validation loss: 0.2162131518125534\n",
      "training epoch nr 51\n",
      "training loss: 0.20972703397274017\n",
      "validation loss: 0.21791154146194458\n",
      "training epoch nr 52\n",
      "training loss: 0.20992204546928406\n",
      "validation loss: 0.21844740211963654\n",
      "training epoch nr 53\n",
      "training loss: 0.20986786484718323\n",
      "validation loss: 0.2209450751543045\n",
      "training epoch nr 54\n",
      "training loss: 0.20967380702495575\n",
      "validation loss: 0.21682734787464142\n",
      "training epoch nr 55\n",
      "training loss: 0.20948363840579987\n",
      "validation loss: 0.22094471752643585\n",
      "training epoch nr 56\n",
      "training loss: 0.20948511362075806\n",
      "validation loss: 0.21755282580852509\n",
      "training epoch nr 57\n",
      "training loss: 0.2096768021583557\n",
      "validation loss: 0.2188844233751297\n",
      "training epoch nr 58\n",
      "training loss: 0.20910635590553284\n",
      "validation loss: 0.2181902527809143\n",
      "training epoch nr 59\n",
      "training loss: 0.20928150415420532\n",
      "validation loss: 0.21757985651493073\n",
      "training epoch nr 60\n",
      "training loss: 0.20896703004837036\n",
      "validation loss: 0.2177693396806717\n",
      "training epoch nr 61\n",
      "training loss: 0.2093009352684021\n",
      "validation loss: 0.217291459441185\n",
      "training epoch nr 62\n",
      "training loss: 0.20890742540359497\n",
      "validation loss: 0.21997015178203583\n",
      "training epoch nr 63\n",
      "training loss: 0.20904798805713654\n",
      "validation loss: 0.21815894544124603\n",
      "training epoch nr 64\n",
      "training loss: 0.20893119275569916\n",
      "validation loss: 0.21766692399978638\n",
      "training epoch nr 65\n",
      "training loss: 0.20819471776485443\n",
      "validation loss: 0.21736595034599304\n",
      "training epoch nr 66\n",
      "training loss: 0.20838390290737152\n",
      "validation loss: 0.21778887510299683\n",
      "training epoch nr 67\n",
      "training loss: 0.20823833346366882\n",
      "validation loss: 0.21771560609340668\n",
      "training epoch nr 68\n",
      "training loss: 0.20823457837104797\n",
      "validation loss: 0.2166483849287033\n",
      "training epoch nr 69\n",
      "training loss: 0.20827461779117584\n",
      "validation loss: 0.21876154839992523\n",
      "training epoch nr 70\n",
      "training loss: 0.20790332555770874\n",
      "validation loss: 0.2181253284215927\n",
      "training epoch nr 71\n",
      "training loss: 0.2079569399356842\n",
      "validation loss: 0.22090677917003632\n",
      "training epoch nr 72\n",
      "training loss: 0.20799851417541504\n",
      "validation loss: 0.21791595220565796\n",
      "training epoch nr 73\n",
      "training loss: 0.20815317332744598\n",
      "validation loss: 0.21813561022281647\n",
      "training epoch nr 74\n",
      "training loss: 0.20798566937446594\n",
      "validation loss: 0.21922101080417633\n",
      "training epoch nr 75\n",
      "training loss: 0.20777016878128052\n",
      "validation loss: 0.2189139872789383\n",
      "training epoch nr 76\n",
      "training loss: 0.20762720704078674\n",
      "validation loss: 0.21887078881263733\n",
      "training epoch nr 77\n",
      "training loss: 0.20757868885993958\n",
      "validation loss: 0.21943598985671997\n",
      "training epoch nr 78\n",
      "training loss: 0.20717757940292358\n",
      "validation loss: 0.21915322542190552\n",
      "training epoch nr 79\n",
      "training loss: 0.20760096609592438\n",
      "validation loss: 0.21964441239833832\n",
      "training epoch nr 80\n",
      "training loss: 0.20768220722675323\n",
      "validation loss: 0.21942298114299774\n",
      "training epoch nr 81\n",
      "training loss: 0.207261860370636\n",
      "validation loss: 0.21895815432071686\n",
      "training epoch nr 82\n",
      "training loss: 0.207044780254364\n",
      "validation loss: 0.21936534345149994\n",
      "training epoch nr 83\n",
      "training loss: 0.20682445168495178\n",
      "validation loss: 0.21927320957183838\n",
      "training epoch nr 84\n",
      "training loss: 0.20740610361099243\n",
      "validation loss: 0.21963761746883392\n",
      "training epoch nr 85\n",
      "training loss: 0.20688699185848236\n",
      "validation loss: 0.22044287621974945\n",
      "training epoch nr 86\n",
      "training loss: 0.20669186115264893\n",
      "validation loss: 0.21949881315231323\n",
      "training epoch nr 87\n",
      "training loss: 0.20666281878948212\n",
      "validation loss: 0.21957381069660187\n",
      "training epoch nr 88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.20654766261577606\n",
      "validation loss: 0.21912983059883118\n",
      "training epoch nr 89\n",
      "training loss: 0.20662467181682587\n",
      "validation loss: 0.21938170492649078\n",
      "training epoch nr 90\n",
      "training loss: 0.20639172196388245\n",
      "validation loss: 0.2196572721004486\n",
      "training epoch nr 91\n",
      "training loss: 0.20612020790576935\n",
      "validation loss: 0.22356414794921875\n",
      "training epoch nr 92\n",
      "training loss: 0.2061387449502945\n",
      "validation loss: 0.22078698873519897\n",
      "training epoch nr 93\n",
      "training loss: 0.20614902675151825\n",
      "validation loss: 0.21900416910648346\n",
      "training epoch nr 94\n",
      "training loss: 0.20596162974834442\n",
      "validation loss: 0.22175095975399017\n",
      "training epoch nr 95\n",
      "training loss: 0.20608656108379364\n",
      "validation loss: 0.22279411554336548\n",
      "training epoch nr 96\n",
      "training loss: 0.20558755099773407\n",
      "validation loss: 0.22289323806762695\n",
      "training epoch nr 97\n",
      "training loss: 0.2058422565460205\n",
      "validation loss: 0.21990355849266052\n",
      "training epoch nr 98\n",
      "training loss: 0.2055191695690155\n",
      "validation loss: 0.22080416977405548\n",
      "training epoch nr 99\n",
      "training loss: 0.20558367669582367\n",
      "validation loss: 0.22151967883110046\n",
      "Training model nr 1...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2688426971435547\n",
      "validation loss: 0.2325347661972046\n",
      "training epoch nr 1\n",
      "training loss: 0.23117931187152863\n",
      "validation loss: 0.22876890003681183\n",
      "training epoch nr 2\n",
      "training loss: 0.22590693831443787\n",
      "validation loss: 0.22342386841773987\n",
      "training epoch nr 3\n",
      "training loss: 0.22273185849189758\n",
      "validation loss: 0.22256430983543396\n",
      "training epoch nr 4\n",
      "training loss: 0.22076909244060516\n",
      "validation loss: 0.21978522837162018\n",
      "training epoch nr 5\n",
      "training loss: 0.22012312710285187\n",
      "validation loss: 0.21915209293365479\n",
      "training epoch nr 6\n",
      "training loss: 0.2190496176481247\n",
      "validation loss: 0.21969620883464813\n",
      "training epoch nr 7\n",
      "training loss: 0.21851983666419983\n",
      "validation loss: 0.21973827481269836\n",
      "training epoch nr 8\n",
      "training loss: 0.21825021505355835\n",
      "validation loss: 0.21929879486560822\n",
      "training epoch nr 9\n",
      "training loss: 0.2176540642976761\n",
      "validation loss: 0.2231019139289856\n",
      "training epoch nr 10\n",
      "training loss: 0.2165915071964264\n",
      "validation loss: 0.22130125761032104\n",
      "training epoch nr 11\n",
      "training loss: 0.21634705364704132\n",
      "validation loss: 0.21821239590644836\n",
      "training epoch nr 12\n",
      "training loss: 0.2160399705171585\n",
      "validation loss: 0.21729488670825958\n",
      "training epoch nr 13\n",
      "training loss: 0.2161102592945099\n",
      "validation loss: 0.21902184188365936\n",
      "training epoch nr 14\n",
      "training loss: 0.21564386785030365\n",
      "validation loss: 0.2192462533712387\n",
      "training epoch nr 15\n",
      "training loss: 0.21536889672279358\n",
      "validation loss: 0.2176411747932434\n",
      "training epoch nr 16\n",
      "training loss: 0.21511530876159668\n",
      "validation loss: 0.21690350770950317\n",
      "training epoch nr 17\n",
      "training loss: 0.21461261808872223\n",
      "validation loss: 0.2175351232290268\n",
      "training epoch nr 18\n",
      "training loss: 0.21482516825199127\n",
      "validation loss: 0.21926268935203552\n",
      "training epoch nr 19\n",
      "training loss: 0.21463604271411896\n",
      "validation loss: 0.22158576548099518\n",
      "training epoch nr 20\n",
      "training loss: 0.21463565528392792\n",
      "validation loss: 0.21644994616508484\n",
      "training epoch nr 21\n",
      "training loss: 0.21403326094150543\n",
      "validation loss: 0.2176300287246704\n",
      "training epoch nr 22\n",
      "training loss: 0.21351106464862823\n",
      "validation loss: 0.2197495400905609\n",
      "training epoch nr 23\n",
      "training loss: 0.2139814794063568\n",
      "validation loss: 0.21769042313098907\n",
      "training epoch nr 24\n",
      "training loss: 0.21360646188259125\n",
      "validation loss: 0.21599344909191132\n",
      "training epoch nr 25\n",
      "training loss: 0.2136821448802948\n",
      "validation loss: 0.21852053701877594\n",
      "training epoch nr 26\n",
      "training loss: 0.2134564220905304\n",
      "validation loss: 0.21686962246894836\n",
      "training epoch nr 27\n",
      "training loss: 0.2129741907119751\n",
      "validation loss: 0.21745721995830536\n",
      "training epoch nr 28\n",
      "training loss: 0.2127983272075653\n",
      "validation loss: 0.22132085263729095\n",
      "training epoch nr 29\n",
      "training loss: 0.21273411810398102\n",
      "validation loss: 0.21912527084350586\n",
      "training epoch nr 30\n",
      "training loss: 0.21260496973991394\n",
      "validation loss: 0.21615032851696014\n",
      "training epoch nr 31\n",
      "training loss: 0.21224865317344666\n",
      "validation loss: 0.21659520268440247\n",
      "training epoch nr 32\n",
      "training loss: 0.2126079499721527\n",
      "validation loss: 0.2187034785747528\n",
      "training epoch nr 33\n",
      "training loss: 0.21214114129543304\n",
      "validation loss: 0.21685530245304108\n",
      "training epoch nr 34\n",
      "training loss: 0.2123369723558426\n",
      "validation loss: 0.21605472266674042\n",
      "training epoch nr 35\n",
      "training loss: 0.21194252371788025\n",
      "validation loss: 0.2171015441417694\n",
      "training epoch nr 36\n",
      "training loss: 0.2123478204011917\n",
      "validation loss: 0.21676620841026306\n",
      "training epoch nr 37\n",
      "training loss: 0.21181334555149078\n",
      "validation loss: 0.21649082005023956\n",
      "training epoch nr 38\n",
      "training loss: 0.21157951653003693\n",
      "validation loss: 0.218063086271286\n",
      "training epoch nr 39\n",
      "training loss: 0.21140414476394653\n",
      "validation loss: 0.21701350808143616\n",
      "training epoch nr 40\n",
      "training loss: 0.21143747866153717\n",
      "validation loss: 0.2174120396375656\n",
      "training epoch nr 41\n",
      "training loss: 0.21145501732826233\n",
      "validation loss: 0.21679775416851044\n",
      "training epoch nr 42\n",
      "training loss: 0.21132394671440125\n",
      "validation loss: 0.21778324246406555\n",
      "training epoch nr 43\n",
      "training loss: 0.2108665406703949\n",
      "validation loss: 0.21832236647605896\n",
      "training epoch nr 44\n",
      "training loss: 0.21135975420475006\n",
      "validation loss: 0.21729536354541779\n",
      "training epoch nr 45\n",
      "training loss: 0.21087698638439178\n",
      "validation loss: 0.21833644807338715\n",
      "training epoch nr 46\n",
      "training loss: 0.21102124452590942\n",
      "validation loss: 0.21699759364128113\n",
      "training epoch nr 47\n",
      "training loss: 0.21044416725635529\n",
      "validation loss: 0.21798408031463623\n",
      "training epoch nr 48\n",
      "training loss: 0.210418701171875\n",
      "validation loss: 0.21751993894577026\n",
      "training epoch nr 49\n",
      "training loss: 0.21037966012954712\n",
      "validation loss: 0.2165534943342209\n",
      "training epoch nr 50\n",
      "training loss: 0.21047362685203552\n",
      "validation loss: 0.21720129251480103\n",
      "training epoch nr 51\n",
      "training loss: 0.2102183848619461\n",
      "validation loss: 0.2173890918493271\n",
      "training epoch nr 52\n",
      "training loss: 0.21002337336540222\n",
      "validation loss: 0.2184126377105713\n",
      "training epoch nr 53\n",
      "training loss: 0.21013784408569336\n",
      "validation loss: 0.22104015946388245\n",
      "training epoch nr 54\n",
      "training loss: 0.21027469635009766\n",
      "validation loss: 0.2164529711008072\n",
      "training epoch nr 55\n",
      "training loss: 0.20992018282413483\n",
      "validation loss: 0.21788448095321655\n",
      "training epoch nr 56\n",
      "training loss: 0.21022629737854004\n",
      "validation loss: 0.21939495205879211\n",
      "training epoch nr 57\n",
      "training loss: 0.2097764015197754\n",
      "validation loss: 0.21829469501972198\n",
      "training epoch nr 58\n",
      "training loss: 0.20955753326416016\n",
      "validation loss: 0.2174886167049408\n",
      "training epoch nr 59\n",
      "training loss: 0.2097688615322113\n",
      "validation loss: 0.2162531167268753\n",
      "training epoch nr 60\n",
      "training loss: 0.2093927413225174\n",
      "validation loss: 0.21807804703712463\n",
      "training epoch nr 61\n",
      "training loss: 0.20912910997867584\n",
      "validation loss: 0.21816377341747284\n",
      "training epoch nr 62\n",
      "training loss: 0.20959679782390594\n",
      "validation loss: 0.21788959205150604\n",
      "training epoch nr 63\n",
      "training loss: 0.20915715396404266\n",
      "validation loss: 0.21898774802684784\n",
      "training epoch nr 64\n",
      "training loss: 0.20896421372890472\n",
      "validation loss: 0.2185893952846527\n",
      "training epoch nr 65\n",
      "training loss: 0.20922507345676422\n",
      "validation loss: 0.21975894272327423\n",
      "training epoch nr 66\n",
      "training loss: 0.2093317061662674\n",
      "validation loss: 0.21915274858474731\n",
      "training epoch nr 67\n",
      "training loss: 0.2088935375213623\n",
      "validation loss: 0.21800833940505981\n",
      "training epoch nr 68\n",
      "training loss: 0.20906700193881989\n",
      "validation loss: 0.21814091503620148\n",
      "training epoch nr 69\n",
      "training loss: 0.2085740715265274\n",
      "validation loss: 0.21750929951667786\n",
      "training epoch nr 70\n",
      "training loss: 0.20849132537841797\n",
      "validation loss: 0.2198895812034607\n",
      "training epoch nr 71\n",
      "training loss: 0.20869864523410797\n",
      "validation loss: 0.2185041904449463\n",
      "training epoch nr 72\n",
      "training loss: 0.20882350206375122\n",
      "validation loss: 0.21955567598342896\n",
      "training epoch nr 73\n",
      "training loss: 0.2082982212305069\n",
      "validation loss: 0.21983925998210907\n",
      "training epoch nr 74\n",
      "training loss: 0.2085220068693161\n",
      "validation loss: 0.21854685246944427\n",
      "training epoch nr 75\n",
      "training loss: 0.208234503865242\n",
      "validation loss: 0.22154413163661957\n",
      "training epoch nr 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.20786093175411224\n",
      "validation loss: 0.21910080313682556\n",
      "training epoch nr 77\n",
      "training loss: 0.20870114862918854\n",
      "validation loss: 0.22002992033958435\n",
      "training epoch nr 78\n",
      "training loss: 0.20790426433086395\n",
      "validation loss: 0.21824993193149567\n",
      "training epoch nr 79\n",
      "training loss: 0.20784704387187958\n",
      "validation loss: 0.21993288397789001\n",
      "training epoch nr 80\n",
      "training loss: 0.2079704999923706\n",
      "validation loss: 0.21782156825065613\n",
      "training epoch nr 81\n",
      "training loss: 0.2077990025281906\n",
      "validation loss: 0.220712810754776\n",
      "training epoch nr 82\n",
      "training loss: 0.20784153044223785\n",
      "validation loss: 0.21989117562770844\n",
      "training epoch nr 83\n",
      "training loss: 0.20735687017440796\n",
      "validation loss: 0.22185219824314117\n",
      "training epoch nr 84\n",
      "training loss: 0.2076929658651352\n",
      "validation loss: 0.2220170497894287\n",
      "training epoch nr 85\n",
      "training loss: 0.20757025480270386\n",
      "validation loss: 0.22066150605678558\n",
      "training epoch nr 86\n",
      "training loss: 0.20745868980884552\n",
      "validation loss: 0.22109712660312653\n",
      "training epoch nr 87\n",
      "training loss: 0.2071361094713211\n",
      "validation loss: 0.2205098271369934\n",
      "training epoch nr 88\n",
      "training loss: 0.20762936770915985\n",
      "validation loss: 0.21993692219257355\n",
      "training epoch nr 89\n",
      "training loss: 0.20720328390598297\n",
      "validation loss: 0.2209392786026001\n",
      "training epoch nr 90\n",
      "training loss: 0.20716406404972076\n",
      "validation loss: 0.22142311930656433\n",
      "training epoch nr 91\n",
      "training loss: 0.2068183422088623\n",
      "validation loss: 0.2225055992603302\n",
      "training epoch nr 92\n",
      "training loss: 0.20696191489696503\n",
      "validation loss: 0.22325754165649414\n",
      "training epoch nr 93\n",
      "training loss: 0.20701536536216736\n",
      "validation loss: 0.22057029604911804\n",
      "training epoch nr 94\n",
      "training loss: 0.2067408710718155\n",
      "validation loss: 0.22169354557991028\n",
      "training epoch nr 95\n",
      "training loss: 0.20677794516086578\n",
      "validation loss: 0.22174231708049774\n",
      "training epoch nr 96\n",
      "training loss: 0.20662669837474823\n",
      "validation loss: 0.22245031595230103\n",
      "training epoch nr 97\n",
      "training loss: 0.20643606781959534\n",
      "validation loss: 0.22144050896167755\n",
      "training epoch nr 98\n",
      "training loss: 0.20661816000938416\n",
      "validation loss: 0.22059813141822815\n",
      "training epoch nr 99\n",
      "training loss: 0.20653769373893738\n",
      "validation loss: 0.22020584344863892\n",
      "Training model nr 2...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2639017105102539\n",
      "validation loss: 0.23221178352832794\n",
      "training epoch nr 1\n",
      "training loss: 0.23096972703933716\n",
      "validation loss: 0.22677965462207794\n",
      "training epoch nr 2\n",
      "training loss: 0.22550220787525177\n",
      "validation loss: 0.22425271570682526\n",
      "training epoch nr 3\n",
      "training loss: 0.2229195535182953\n",
      "validation loss: 0.2223789244890213\n",
      "training epoch nr 4\n",
      "training loss: 0.2213023602962494\n",
      "validation loss: 0.2196943461894989\n",
      "training epoch nr 5\n",
      "training loss: 0.22000466287136078\n",
      "validation loss: 0.22114063799381256\n",
      "training epoch nr 6\n",
      "training loss: 0.21891972422599792\n",
      "validation loss: 0.21856212615966797\n",
      "training epoch nr 7\n",
      "training loss: 0.21879874169826508\n",
      "validation loss: 0.22236864268779755\n",
      "training epoch nr 8\n",
      "training loss: 0.2178424596786499\n",
      "validation loss: 0.21935230493545532\n",
      "training epoch nr 9\n",
      "training loss: 0.21681758761405945\n",
      "validation loss: 0.21815738081932068\n",
      "training epoch nr 10\n",
      "training loss: 0.21668775379657745\n",
      "validation loss: 0.21847085654735565\n",
      "training epoch nr 11\n",
      "training loss: 0.2167043387889862\n",
      "validation loss: 0.2228228747844696\n",
      "training epoch nr 12\n",
      "training loss: 0.21646490693092346\n",
      "validation loss: 0.21860073506832123\n",
      "training epoch nr 13\n",
      "training loss: 0.21611127257347107\n",
      "validation loss: 0.21660083532333374\n",
      "training epoch nr 14\n",
      "training loss: 0.21571186184883118\n",
      "validation loss: 0.2194027304649353\n",
      "training epoch nr 15\n",
      "training loss: 0.21558116376399994\n",
      "validation loss: 0.21715833246707916\n",
      "training epoch nr 16\n",
      "training loss: 0.21489481627941132\n",
      "validation loss: 0.2187168151140213\n",
      "training epoch nr 17\n",
      "training loss: 0.21494515240192413\n",
      "validation loss: 0.21578003466129303\n",
      "training epoch nr 18\n",
      "training loss: 0.21448463201522827\n",
      "validation loss: 0.21775692701339722\n",
      "training epoch nr 19\n",
      "training loss: 0.2142089456319809\n",
      "validation loss: 0.22027501463890076\n",
      "training epoch nr 20\n",
      "training loss: 0.21410025656223297\n",
      "validation loss: 0.21633648872375488\n",
      "training epoch nr 21\n",
      "training loss: 0.2138153314590454\n",
      "validation loss: 0.21760350465774536\n",
      "training epoch nr 22\n",
      "training loss: 0.2134980410337448\n",
      "validation loss: 0.21771501004695892\n",
      "training epoch nr 23\n",
      "training loss: 0.2133137434720993\n",
      "validation loss: 0.21528595685958862\n",
      "training epoch nr 24\n",
      "training loss: 0.21352608501911163\n",
      "validation loss: 0.21722443401813507\n",
      "training epoch nr 25\n",
      "training loss: 0.2133156806230545\n",
      "validation loss: 0.21734699606895447\n",
      "training epoch nr 26\n",
      "training loss: 0.2129378467798233\n",
      "validation loss: 0.21771392226219177\n",
      "training epoch nr 27\n",
      "training loss: 0.21299250423908234\n",
      "validation loss: 0.21959055960178375\n",
      "training epoch nr 28\n",
      "training loss: 0.21282023191452026\n",
      "validation loss: 0.21812936663627625\n",
      "training epoch nr 29\n",
      "training loss: 0.21250945329666138\n",
      "validation loss: 0.2168114334344864\n",
      "training epoch nr 30\n",
      "training loss: 0.21250009536743164\n",
      "validation loss: 0.2199554294347763\n",
      "training epoch nr 31\n",
      "training loss: 0.21240860223770142\n",
      "validation loss: 0.21748749911785126\n",
      "training epoch nr 32\n",
      "training loss: 0.21218402683734894\n",
      "validation loss: 0.21654683351516724\n",
      "training epoch nr 33\n",
      "training loss: 0.2120766043663025\n",
      "validation loss: 0.21636466681957245\n",
      "training epoch nr 34\n",
      "training loss: 0.21208959817886353\n",
      "validation loss: 0.21627061069011688\n",
      "training epoch nr 35\n",
      "training loss: 0.21177929639816284\n",
      "validation loss: 0.2162284255027771\n",
      "training epoch nr 36\n",
      "training loss: 0.21163369715213776\n",
      "validation loss: 0.21812798082828522\n",
      "training epoch nr 37\n",
      "training loss: 0.21144990622997284\n",
      "validation loss: 0.2165166586637497\n",
      "training epoch nr 38\n",
      "training loss: 0.21160012483596802\n",
      "validation loss: 0.2195075899362564\n",
      "training epoch nr 39\n",
      "training loss: 0.21118266880512238\n",
      "validation loss: 0.21662785112857819\n",
      "training epoch nr 40\n",
      "training loss: 0.21115560829639435\n",
      "validation loss: 0.21602734923362732\n",
      "training epoch nr 41\n",
      "training loss: 0.21114246547222137\n",
      "validation loss: 0.21708598732948303\n",
      "training epoch nr 42\n",
      "training loss: 0.21118445694446564\n",
      "validation loss: 0.216441810131073\n",
      "training epoch nr 43\n",
      "training loss: 0.2107464224100113\n",
      "validation loss: 0.2167186141014099\n",
      "training epoch nr 44\n",
      "training loss: 0.2105511575937271\n",
      "validation loss: 0.21883980929851532\n",
      "training epoch nr 45\n",
      "training loss: 0.21048100292682648\n",
      "validation loss: 0.21658673882484436\n",
      "training epoch nr 46\n",
      "training loss: 0.21046055853366852\n",
      "validation loss: 0.21823139488697052\n",
      "training epoch nr 47\n",
      "training loss: 0.21014705300331116\n",
      "validation loss: 0.2167794108390808\n",
      "training epoch nr 48\n",
      "training loss: 0.2098715752363205\n",
      "validation loss: 0.2165081799030304\n",
      "training epoch nr 49\n",
      "training loss: 0.2101028561592102\n",
      "validation loss: 0.21806952357292175\n",
      "training epoch nr 50\n",
      "training loss: 0.21014969050884247\n",
      "validation loss: 0.21940016746520996\n",
      "training epoch nr 51\n",
      "training loss: 0.20999708771705627\n",
      "validation loss: 0.2171223759651184\n",
      "training epoch nr 52\n",
      "training loss: 0.20978926122188568\n",
      "validation loss: 0.21692220866680145\n",
      "training epoch nr 53\n",
      "training loss: 0.20980457961559296\n",
      "validation loss: 0.2186623066663742\n",
      "training epoch nr 54\n",
      "training loss: 0.2094978541135788\n",
      "validation loss: 0.2186374068260193\n",
      "training epoch nr 55\n",
      "training loss: 0.20955967903137207\n",
      "validation loss: 0.21840105950832367\n",
      "training epoch nr 56\n",
      "training loss: 0.2091699093580246\n",
      "validation loss: 0.21711212396621704\n",
      "training epoch nr 57\n",
      "training loss: 0.20930178463459015\n",
      "validation loss: 0.2189408391714096\n",
      "training epoch nr 58\n",
      "training loss: 0.20939095318317413\n",
      "validation loss: 0.21813412010669708\n",
      "training epoch nr 59\n",
      "training loss: 0.2091977894306183\n",
      "validation loss: 0.21850574016571045\n",
      "training epoch nr 60\n",
      "training loss: 0.20942427217960358\n",
      "validation loss: 0.21805687248706818\n",
      "training epoch nr 61\n",
      "training loss: 0.20866893231868744\n",
      "validation loss: 0.2179322987794876\n",
      "training epoch nr 62\n",
      "training loss: 0.20870785415172577\n",
      "validation loss: 0.21850860118865967\n",
      "training epoch nr 63\n",
      "training loss: 0.20834478735923767\n",
      "validation loss: 0.21971024572849274\n",
      "training epoch nr 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.20860816538333893\n",
      "validation loss: 0.21706876158714294\n",
      "training epoch nr 65\n",
      "training loss: 0.20817753672599792\n",
      "validation loss: 0.21814817190170288\n",
      "training epoch nr 66\n",
      "training loss: 0.2086290866136551\n",
      "validation loss: 0.21706177294254303\n",
      "training epoch nr 67\n",
      "training loss: 0.2082018256187439\n",
      "validation loss: 0.2188032567501068\n",
      "training epoch nr 68\n",
      "training loss: 0.2078743427991867\n",
      "validation loss: 0.21948829293251038\n",
      "training epoch nr 69\n",
      "training loss: 0.20790313184261322\n",
      "validation loss: 0.2184378057718277\n",
      "training epoch nr 70\n",
      "training loss: 0.20772239565849304\n",
      "validation loss: 0.219774529337883\n",
      "training epoch nr 71\n",
      "training loss: 0.20763985812664032\n",
      "validation loss: 0.2191118746995926\n",
      "training epoch nr 72\n",
      "training loss: 0.2079009860754013\n",
      "validation loss: 0.2176922857761383\n",
      "training epoch nr 73\n",
      "training loss: 0.20744067430496216\n",
      "validation loss: 0.21881310641765594\n",
      "training epoch nr 74\n",
      "training loss: 0.20741592347621918\n",
      "validation loss: 0.21930797398090363\n",
      "training epoch nr 75\n",
      "training loss: 0.20736443996429443\n",
      "validation loss: 0.21967163681983948\n",
      "training epoch nr 76\n",
      "training loss: 0.20744842290878296\n",
      "validation loss: 0.2185453176498413\n",
      "training epoch nr 77\n",
      "training loss: 0.20721958577632904\n",
      "validation loss: 0.22009439766407013\n",
      "training epoch nr 78\n",
      "training loss: 0.20671576261520386\n",
      "validation loss: 0.21917010843753815\n",
      "training epoch nr 79\n",
      "training loss: 0.206954687833786\n",
      "validation loss: 0.22001971304416656\n",
      "training epoch nr 80\n",
      "training loss: 0.20691445469856262\n",
      "validation loss: 0.2194640189409256\n",
      "training epoch nr 81\n",
      "training loss: 0.20701034367084503\n",
      "validation loss: 0.21907645463943481\n",
      "training epoch nr 82\n",
      "training loss: 0.2067917138338089\n",
      "validation loss: 0.22041676938533783\n",
      "training epoch nr 83\n",
      "training loss: 0.20671968162059784\n",
      "validation loss: 0.2206433117389679\n",
      "training epoch nr 84\n",
      "training loss: 0.20649884641170502\n",
      "validation loss: 0.22335156798362732\n",
      "training epoch nr 85\n",
      "training loss: 0.20658639073371887\n",
      "validation loss: 0.22059406340122223\n",
      "training epoch nr 86\n",
      "training loss: 0.20622490346431732\n",
      "validation loss: 0.2204248458147049\n",
      "training epoch nr 87\n",
      "training loss: 0.20600436627864838\n",
      "validation loss: 0.22213387489318848\n",
      "training epoch nr 88\n",
      "training loss: 0.2060289829969406\n",
      "validation loss: 0.22062042355537415\n",
      "training epoch nr 89\n",
      "training loss: 0.20599715411663055\n",
      "validation loss: 0.22046978771686554\n",
      "training epoch nr 90\n",
      "training loss: 0.20587265491485596\n",
      "validation loss: 0.22059565782546997\n",
      "training epoch nr 91\n",
      "training loss: 0.20581267774105072\n",
      "validation loss: 0.22169755399227142\n",
      "training epoch nr 92\n",
      "training loss: 0.20533844828605652\n",
      "validation loss: 0.2234657108783722\n",
      "training epoch nr 93\n",
      "training loss: 0.20603340864181519\n",
      "validation loss: 0.22107914090156555\n",
      "training epoch nr 94\n",
      "training loss: 0.20548613369464874\n",
      "validation loss: 0.2229161411523819\n",
      "training epoch nr 95\n",
      "training loss: 0.20568843185901642\n",
      "validation loss: 0.22199051082134247\n",
      "training epoch nr 96\n",
      "training loss: 0.20550274848937988\n",
      "validation loss: 0.2216784954071045\n",
      "training epoch nr 97\n",
      "training loss: 0.20542865991592407\n",
      "validation loss: 0.2216738611459732\n",
      "training epoch nr 98\n",
      "training loss: 0.20531226694583893\n",
      "validation loss: 0.221515953540802\n",
      "training epoch nr 99\n",
      "training loss: 0.2051120549440384\n",
      "validation loss: 0.2234376072883606\n",
      "Training model nr 3...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2661406695842743\n",
      "validation loss: 0.23450155556201935\n",
      "training epoch nr 1\n",
      "training loss: 0.23281006515026093\n",
      "validation loss: 0.2330637276172638\n",
      "training epoch nr 2\n",
      "training loss: 0.22792288661003113\n",
      "validation loss: 0.23503941297531128\n",
      "training epoch nr 3\n",
      "training loss: 0.22600597143173218\n",
      "validation loss: 0.2250281572341919\n",
      "training epoch nr 4\n",
      "training loss: 0.22351109981536865\n",
      "validation loss: 0.22826671600341797\n",
      "training epoch nr 5\n",
      "training loss: 0.22205281257629395\n",
      "validation loss: 0.2364918291568756\n",
      "training epoch nr 6\n",
      "training loss: 0.22074086964130402\n",
      "validation loss: 0.22136549651622772\n",
      "training epoch nr 7\n",
      "training loss: 0.21989348530769348\n",
      "validation loss: 0.22034309804439545\n",
      "training epoch nr 8\n",
      "training loss: 0.21909688413143158\n",
      "validation loss: 0.2204742729663849\n",
      "training epoch nr 9\n",
      "training loss: 0.2182689905166626\n",
      "validation loss: 0.2196720391511917\n",
      "training epoch nr 10\n",
      "training loss: 0.21720340847969055\n",
      "validation loss: 0.21837221086025238\n",
      "training epoch nr 11\n",
      "training loss: 0.21755655109882355\n",
      "validation loss: 0.21721698343753815\n",
      "training epoch nr 12\n",
      "training loss: 0.21666762232780457\n",
      "validation loss: 0.2189769297838211\n",
      "training epoch nr 13\n",
      "training loss: 0.2164032906293869\n",
      "validation loss: 0.21884594857692719\n",
      "training epoch nr 14\n",
      "training loss: 0.21602341532707214\n",
      "validation loss: 0.21863682568073273\n",
      "training epoch nr 15\n",
      "training loss: 0.215433731675148\n",
      "validation loss: 0.22136974334716797\n",
      "training epoch nr 16\n",
      "training loss: 0.2152465134859085\n",
      "validation loss: 0.21919013559818268\n",
      "training epoch nr 17\n",
      "training loss: 0.21479910612106323\n",
      "validation loss: 0.22017191350460052\n",
      "training epoch nr 18\n",
      "training loss: 0.2149663269519806\n",
      "validation loss: 0.21847961843013763\n",
      "training epoch nr 19\n",
      "training loss: 0.21461285650730133\n",
      "validation loss: 0.2167428731918335\n",
      "training epoch nr 20\n",
      "training loss: 0.21450106799602509\n",
      "validation loss: 0.2161959707736969\n",
      "training epoch nr 21\n",
      "training loss: 0.21454957127571106\n",
      "validation loss: 0.21892176568508148\n",
      "training epoch nr 22\n",
      "training loss: 0.21393075585365295\n",
      "validation loss: 0.21649384498596191\n",
      "training epoch nr 23\n",
      "training loss: 0.21387816965579987\n",
      "validation loss: 0.21686187386512756\n",
      "training epoch nr 24\n",
      "training loss: 0.21352903544902802\n",
      "validation loss: 0.2192300260066986\n",
      "training epoch nr 25\n",
      "training loss: 0.21355943381786346\n",
      "validation loss: 0.2171180099248886\n",
      "training epoch nr 26\n",
      "training loss: 0.21304884552955627\n",
      "validation loss: 0.21729646623134613\n",
      "training epoch nr 27\n",
      "training loss: 0.21289491653442383\n",
      "validation loss: 0.2167183756828308\n",
      "training epoch nr 28\n",
      "training loss: 0.21285752952098846\n",
      "validation loss: 0.21935567259788513\n",
      "training epoch nr 29\n",
      "training loss: 0.21291543543338776\n",
      "validation loss: 0.21656189858913422\n",
      "training epoch nr 30\n",
      "training loss: 0.21274296939373016\n",
      "validation loss: 0.21722500026226044\n",
      "training epoch nr 31\n",
      "training loss: 0.2124861180782318\n",
      "validation loss: 0.21696852147579193\n",
      "training epoch nr 32\n",
      "training loss: 0.21217133104801178\n",
      "validation loss: 0.22050969302654266\n",
      "training epoch nr 33\n",
      "training loss: 0.2122887670993805\n",
      "validation loss: 0.21698270738124847\n",
      "training epoch nr 34\n",
      "training loss: 0.21222618222236633\n",
      "validation loss: 0.21616250276565552\n",
      "training epoch nr 35\n",
      "training loss: 0.21193312108516693\n",
      "validation loss: 0.21791665256023407\n",
      "training epoch nr 36\n",
      "training loss: 0.21215757727622986\n",
      "validation loss: 0.21989277005195618\n",
      "training epoch nr 37\n",
      "training loss: 0.21191301941871643\n",
      "validation loss: 0.21662576496601105\n",
      "training epoch nr 38\n",
      "training loss: 0.21149399876594543\n",
      "validation loss: 0.21689897775650024\n",
      "training epoch nr 39\n",
      "training loss: 0.2112933099269867\n",
      "validation loss: 0.21652594208717346\n",
      "training epoch nr 40\n",
      "training loss: 0.21153637766838074\n",
      "validation loss: 0.21672360599040985\n",
      "training epoch nr 41\n",
      "training loss: 0.21096977591514587\n",
      "validation loss: 0.21617788076400757\n",
      "training epoch nr 42\n",
      "training loss: 0.21103060245513916\n",
      "validation loss: 0.21808277070522308\n",
      "training epoch nr 43\n",
      "training loss: 0.21104182302951813\n",
      "validation loss: 0.21642464399337769\n",
      "training epoch nr 44\n",
      "training loss: 0.21066561341285706\n",
      "validation loss: 0.22038985788822174\n",
      "training epoch nr 45\n",
      "training loss: 0.21044829487800598\n",
      "validation loss: 0.2162494957447052\n",
      "training epoch nr 46\n",
      "training loss: 0.2106797844171524\n",
      "validation loss: 0.21538786590099335\n",
      "training epoch nr 47\n",
      "training loss: 0.21035136282444\n",
      "validation loss: 0.21776439249515533\n",
      "training epoch nr 48\n",
      "training loss: 0.21035052835941315\n",
      "validation loss: 0.21652835607528687\n",
      "training epoch nr 49\n",
      "training loss: 0.21002313494682312\n",
      "validation loss: 0.21885491907596588\n",
      "training epoch nr 50\n",
      "training loss: 0.21016301214694977\n",
      "validation loss: 0.21739214658737183\n",
      "training epoch nr 51\n",
      "training loss: 0.2102021723985672\n",
      "validation loss: 0.21892639994621277\n",
      "training epoch nr 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.20982994139194489\n",
      "validation loss: 0.2175634205341339\n",
      "training epoch nr 53\n",
      "training loss: 0.20980796217918396\n",
      "validation loss: 0.21670004725456238\n",
      "training epoch nr 54\n",
      "training loss: 0.20936807990074158\n",
      "validation loss: 0.21933333575725555\n",
      "training epoch nr 55\n",
      "training loss: 0.20963183045387268\n",
      "validation loss: 0.21619993448257446\n",
      "training epoch nr 56\n",
      "training loss: 0.20958445966243744\n",
      "validation loss: 0.2187022715806961\n",
      "training epoch nr 57\n",
      "training loss: 0.2093384861946106\n",
      "validation loss: 0.21866875886917114\n",
      "training epoch nr 58\n",
      "training loss: 0.20926526188850403\n",
      "validation loss: 0.2170756608247757\n",
      "training epoch nr 59\n",
      "training loss: 0.20905430614948273\n",
      "validation loss: 0.21874749660491943\n",
      "training epoch nr 60\n",
      "training loss: 0.20923812687397003\n",
      "validation loss: 0.22044776380062103\n",
      "training epoch nr 61\n",
      "training loss: 0.20884515345096588\n",
      "validation loss: 0.22052566707134247\n",
      "training epoch nr 62\n",
      "training loss: 0.20900671184062958\n",
      "validation loss: 0.2173640877008438\n",
      "training epoch nr 63\n",
      "training loss: 0.2087169736623764\n",
      "validation loss: 0.21847125887870789\n",
      "training epoch nr 64\n",
      "training loss: 0.20879220962524414\n",
      "validation loss: 0.2186127007007599\n",
      "training epoch nr 65\n",
      "training loss: 0.20857122540473938\n",
      "validation loss: 0.2180975377559662\n",
      "training epoch nr 66\n",
      "training loss: 0.20843505859375\n",
      "validation loss: 0.21960929036140442\n",
      "training epoch nr 67\n",
      "training loss: 0.20823770761489868\n",
      "validation loss: 0.21963092684745789\n",
      "training epoch nr 68\n",
      "training loss: 0.20813272893428802\n",
      "validation loss: 0.21899686753749847\n",
      "training epoch nr 69\n",
      "training loss: 0.20801571011543274\n",
      "validation loss: 0.2182648926973343\n",
      "training epoch nr 70\n",
      "training loss: 0.20771807432174683\n",
      "validation loss: 0.21830129623413086\n",
      "training epoch nr 71\n",
      "training loss: 0.2082405984401703\n",
      "validation loss: 0.21775610744953156\n",
      "training epoch nr 72\n",
      "training loss: 0.208058163523674\n",
      "validation loss: 0.21928571164608002\n",
      "training epoch nr 73\n",
      "training loss: 0.20783185958862305\n",
      "validation loss: 0.21894370019435883\n",
      "training epoch nr 74\n",
      "training loss: 0.20760338008403778\n",
      "validation loss: 0.22048036754131317\n",
      "training epoch nr 75\n",
      "training loss: 0.20757222175598145\n",
      "validation loss: 0.21819917857646942\n",
      "training epoch nr 76\n",
      "training loss: 0.2073018103837967\n",
      "validation loss: 0.22125935554504395\n",
      "training epoch nr 77\n",
      "training loss: 0.20747539401054382\n",
      "validation loss: 0.21992304921150208\n",
      "training epoch nr 78\n",
      "training loss: 0.2073078602552414\n",
      "validation loss: 0.2191922515630722\n",
      "training epoch nr 79\n",
      "training loss: 0.20726534724235535\n",
      "validation loss: 0.22155047953128815\n",
      "training epoch nr 80\n",
      "training loss: 0.2074485421180725\n",
      "validation loss: 0.2184911072254181\n",
      "training epoch nr 81\n",
      "training loss: 0.2067485749721527\n",
      "validation loss: 0.21897345781326294\n",
      "training epoch nr 82\n",
      "training loss: 0.20682881772518158\n",
      "validation loss: 0.2196827381849289\n",
      "training epoch nr 83\n",
      "training loss: 0.2065914273262024\n",
      "validation loss: 0.22243532538414001\n",
      "training epoch nr 84\n",
      "training loss: 0.2067909985780716\n",
      "validation loss: 0.21934522688388824\n",
      "training epoch nr 85\n",
      "training loss: 0.20653089880943298\n",
      "validation loss: 0.21957425773143768\n",
      "training epoch nr 86\n",
      "training loss: 0.20631076395511627\n",
      "validation loss: 0.22105735540390015\n",
      "training epoch nr 87\n",
      "training loss: 0.20635513961315155\n",
      "validation loss: 0.22396835684776306\n",
      "training epoch nr 88\n",
      "training loss: 0.2063102126121521\n",
      "validation loss: 0.21990859508514404\n",
      "training epoch nr 89\n",
      "training loss: 0.20622175931930542\n",
      "validation loss: 0.22091414034366608\n",
      "training epoch nr 90\n",
      "training loss: 0.20634391903877258\n",
      "validation loss: 0.22109486162662506\n",
      "training epoch nr 91\n",
      "training loss: 0.2057366818189621\n",
      "validation loss: 0.22364109754562378\n",
      "training epoch nr 92\n",
      "training loss: 0.20589052140712738\n",
      "validation loss: 0.21986426413059235\n",
      "training epoch nr 93\n",
      "training loss: 0.20585788786411285\n",
      "validation loss: 0.21936427056789398\n",
      "training epoch nr 94\n",
      "training loss: 0.20544986426830292\n",
      "validation loss: 0.22146964073181152\n",
      "training epoch nr 95\n",
      "training loss: 0.20572073757648468\n",
      "validation loss: 0.22120310366153717\n",
      "training epoch nr 96\n",
      "training loss: 0.20585793256759644\n",
      "validation loss: 0.22303730249404907\n",
      "training epoch nr 97\n",
      "training loss: 0.20514792203903198\n",
      "validation loss: 0.22149008512496948\n",
      "training epoch nr 98\n",
      "training loss: 0.2052145153284073\n",
      "validation loss: 0.2222438007593155\n",
      "training epoch nr 99\n",
      "training loss: 0.20500542223453522\n",
      "validation loss: 0.221112921833992\n",
      "Training model nr 4...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2740066945552826\n",
      "validation loss: 0.23382484912872314\n",
      "training epoch nr 1\n",
      "training loss: 0.23035168647766113\n",
      "validation loss: 0.22588814795017242\n",
      "training epoch nr 2\n",
      "training loss: 0.22522225975990295\n",
      "validation loss: 0.22329474985599518\n",
      "training epoch nr 3\n",
      "training loss: 0.22226384282112122\n",
      "validation loss: 0.22580201923847198\n",
      "training epoch nr 4\n",
      "training loss: 0.22092898190021515\n",
      "validation loss: 0.22052453458309174\n",
      "training epoch nr 5\n",
      "training loss: 0.21992367506027222\n",
      "validation loss: 0.22745607793331146\n",
      "training epoch nr 6\n",
      "training loss: 0.21956683695316315\n",
      "validation loss: 0.22630399465560913\n",
      "training epoch nr 7\n",
      "training loss: 0.21893203258514404\n",
      "validation loss: 0.21816281974315643\n",
      "training epoch nr 8\n",
      "training loss: 0.21843743324279785\n",
      "validation loss: 0.2235194444656372\n",
      "training epoch nr 9\n",
      "training loss: 0.21793442964553833\n",
      "validation loss: 0.2182120382785797\n",
      "training epoch nr 10\n",
      "training loss: 0.21738936007022858\n",
      "validation loss: 0.21952152252197266\n",
      "training epoch nr 11\n",
      "training loss: 0.21709465980529785\n",
      "validation loss: 0.21918736398220062\n",
      "training epoch nr 12\n",
      "training loss: 0.2168610394001007\n",
      "validation loss: 0.21844692528247833\n",
      "training epoch nr 13\n",
      "training loss: 0.21680738031864166\n",
      "validation loss: 0.21716974675655365\n",
      "training epoch nr 14\n",
      "training loss: 0.21635261178016663\n",
      "validation loss: 0.22168156504631042\n",
      "training epoch nr 15\n",
      "training loss: 0.21630410850048065\n",
      "validation loss: 0.2200140506029129\n",
      "training epoch nr 16\n",
      "training loss: 0.21583087742328644\n",
      "validation loss: 0.22054046392440796\n",
      "training epoch nr 17\n",
      "training loss: 0.21563264727592468\n",
      "validation loss: 0.21722590923309326\n",
      "training epoch nr 18\n",
      "training loss: 0.21550965309143066\n",
      "validation loss: 0.21677666902542114\n",
      "training epoch nr 19\n",
      "training loss: 0.2146448791027069\n",
      "validation loss: 0.21836140751838684\n",
      "training epoch nr 20\n",
      "training loss: 0.21492339670658112\n",
      "validation loss: 0.21728186309337616\n",
      "training epoch nr 21\n",
      "training loss: 0.21459951996803284\n",
      "validation loss: 0.2200940102338791\n",
      "training epoch nr 22\n",
      "training loss: 0.21435284614562988\n",
      "validation loss: 0.22149673104286194\n",
      "training epoch nr 23\n",
      "training loss: 0.21417136490345\n",
      "validation loss: 0.2175016850233078\n",
      "training epoch nr 24\n",
      "training loss: 0.21427279710769653\n",
      "validation loss: 0.2193557769060135\n",
      "training epoch nr 25\n",
      "training loss: 0.21411196887493134\n",
      "validation loss: 0.21768879890441895\n",
      "training epoch nr 26\n",
      "training loss: 0.21351507306098938\n",
      "validation loss: 0.21636506915092468\n",
      "training epoch nr 27\n",
      "training loss: 0.21340212225914001\n",
      "validation loss: 0.21947717666625977\n",
      "training epoch nr 28\n",
      "training loss: 0.21357765793800354\n",
      "validation loss: 0.21831382811069489\n",
      "training epoch nr 29\n",
      "training loss: 0.2132296860218048\n",
      "validation loss: 0.21666397154331207\n",
      "training epoch nr 30\n",
      "training loss: 0.2132626175880432\n",
      "validation loss: 0.21989259123802185\n",
      "training epoch nr 31\n",
      "training loss: 0.21316532790660858\n",
      "validation loss: 0.21743962168693542\n",
      "training epoch nr 32\n",
      "training loss: 0.213178351521492\n",
      "validation loss: 0.21783168613910675\n",
      "training epoch nr 33\n",
      "training loss: 0.21306060254573822\n",
      "validation loss: 0.21953801810741425\n",
      "training epoch nr 34\n",
      "training loss: 0.2124052792787552\n",
      "validation loss: 0.21650990843772888\n",
      "training epoch nr 35\n",
      "training loss: 0.21259774267673492\n",
      "validation loss: 0.2167738676071167\n",
      "training epoch nr 36\n",
      "training loss: 0.2126280814409256\n",
      "validation loss: 0.2168545424938202\n",
      "training epoch nr 37\n",
      "training loss: 0.21211744844913483\n",
      "validation loss: 0.21716633439064026\n",
      "training epoch nr 38\n",
      "training loss: 0.21220681071281433\n",
      "validation loss: 0.21558871865272522\n",
      "training epoch nr 39\n",
      "training loss: 0.2123500406742096\n",
      "validation loss: 0.21818004548549652\n",
      "training epoch nr 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.21189184486865997\n",
      "validation loss: 0.2209012806415558\n",
      "training epoch nr 41\n",
      "training loss: 0.21172957122325897\n",
      "validation loss: 0.2167162001132965\n",
      "training epoch nr 42\n",
      "training loss: 0.21158365905284882\n",
      "validation loss: 0.21828989684581757\n",
      "training epoch nr 43\n",
      "training loss: 0.2118532359600067\n",
      "validation loss: 0.21655015647411346\n",
      "training epoch nr 44\n",
      "training loss: 0.21188542246818542\n",
      "validation loss: 0.21646150946617126\n",
      "training epoch nr 45\n",
      "training loss: 0.2117512822151184\n",
      "validation loss: 0.2159683257341385\n",
      "training epoch nr 46\n",
      "training loss: 0.21155601739883423\n",
      "validation loss: 0.2164163887500763\n",
      "training epoch nr 47\n",
      "training loss: 0.2111426740884781\n",
      "validation loss: 0.21858753263950348\n",
      "training epoch nr 48\n",
      "training loss: 0.21110008656978607\n",
      "validation loss: 0.2165045142173767\n",
      "training epoch nr 49\n",
      "training loss: 0.21164579689502716\n",
      "validation loss: 0.21712768077850342\n",
      "training epoch nr 50\n",
      "training loss: 0.2111506164073944\n",
      "validation loss: 0.21729618310928345\n",
      "training epoch nr 51\n",
      "training loss: 0.2108495682477951\n",
      "validation loss: 0.21855410933494568\n",
      "training epoch nr 52\n",
      "training loss: 0.21147535741329193\n",
      "validation loss: 0.21799661219120026\n",
      "training epoch nr 53\n",
      "training loss: 0.2110517919063568\n",
      "validation loss: 0.21909505128860474\n",
      "training epoch nr 54\n",
      "training loss: 0.2107204645872116\n",
      "validation loss: 0.216368168592453\n",
      "training epoch nr 55\n",
      "training loss: 0.2106175571680069\n",
      "validation loss: 0.21631450951099396\n",
      "training epoch nr 56\n",
      "training loss: 0.21070553362369537\n",
      "validation loss: 0.21962501108646393\n",
      "training epoch nr 57\n",
      "training loss: 0.210959792137146\n",
      "validation loss: 0.21928958594799042\n",
      "training epoch nr 58\n",
      "training loss: 0.21056228876113892\n",
      "validation loss: 0.21730369329452515\n",
      "training epoch nr 59\n",
      "training loss: 0.21041715145111084\n",
      "validation loss: 0.21702854335308075\n",
      "training epoch nr 60\n",
      "training loss: 0.21057868003845215\n",
      "validation loss: 0.2176511585712433\n",
      "training epoch nr 61\n",
      "training loss: 0.21036463975906372\n",
      "validation loss: 0.21724356710910797\n",
      "training epoch nr 62\n",
      "training loss: 0.21044933795928955\n",
      "validation loss: 0.21773946285247803\n",
      "training epoch nr 63\n",
      "training loss: 0.2104232758283615\n",
      "validation loss: 0.21817044913768768\n",
      "training epoch nr 64\n",
      "training loss: 0.21043117344379425\n",
      "validation loss: 0.21782241761684418\n",
      "training epoch nr 65\n",
      "training loss: 0.20994554460048676\n",
      "validation loss: 0.2173965573310852\n",
      "training epoch nr 66\n",
      "training loss: 0.2097509205341339\n",
      "validation loss: 0.21891842782497406\n",
      "training epoch nr 67\n",
      "training loss: 0.21023079752922058\n",
      "validation loss: 0.21773482859134674\n",
      "training epoch nr 68\n",
      "training loss: 0.20978911221027374\n",
      "validation loss: 0.2166319042444229\n",
      "training epoch nr 69\n",
      "training loss: 0.20952853560447693\n",
      "validation loss: 0.21946845948696136\n",
      "training epoch nr 70\n",
      "training loss: 0.20962491631507874\n",
      "validation loss: 0.21876198053359985\n",
      "training epoch nr 71\n",
      "training loss: 0.20966076850891113\n",
      "validation loss: 0.21799229085445404\n",
      "training epoch nr 72\n",
      "training loss: 0.20945389568805695\n",
      "validation loss: 0.21743814647197723\n",
      "training epoch nr 73\n",
      "training loss: 0.209718257188797\n",
      "validation loss: 0.21626755595207214\n",
      "training epoch nr 74\n",
      "training loss: 0.20931555330753326\n",
      "validation loss: 0.2175043672323227\n",
      "training epoch nr 75\n",
      "training loss: 0.20913037657737732\n",
      "validation loss: 0.21884524822235107\n",
      "training epoch nr 76\n",
      "training loss: 0.2094230204820633\n",
      "validation loss: 0.21691803634166718\n",
      "training epoch nr 77\n",
      "training loss: 0.20896483957767487\n",
      "validation loss: 0.2188180685043335\n",
      "training epoch nr 78\n",
      "training loss: 0.20924998819828033\n",
      "validation loss: 0.2183634340763092\n",
      "training epoch nr 79\n",
      "training loss: 0.20908194780349731\n",
      "validation loss: 0.21728582680225372\n",
      "training epoch nr 80\n",
      "training loss: 0.20871706306934357\n",
      "validation loss: 0.21960735321044922\n",
      "training epoch nr 81\n",
      "training loss: 0.20890769362449646\n",
      "validation loss: 0.2187364250421524\n",
      "training epoch nr 82\n",
      "training loss: 0.20884612202644348\n",
      "validation loss: 0.2179139256477356\n",
      "training epoch nr 83\n",
      "training loss: 0.20867463946342468\n",
      "validation loss: 0.21770529448986053\n",
      "training epoch nr 84\n",
      "training loss: 0.20833906531333923\n",
      "validation loss: 0.22017203271389008\n",
      "training epoch nr 85\n",
      "training loss: 0.20891980826854706\n",
      "validation loss: 0.21734857559204102\n",
      "training epoch nr 86\n",
      "training loss: 0.2087021768093109\n",
      "validation loss: 0.21962730586528778\n",
      "training epoch nr 87\n",
      "training loss: 0.208426833152771\n",
      "validation loss: 0.22100409865379333\n",
      "training epoch nr 88\n",
      "training loss: 0.20836932957172394\n",
      "validation loss: 0.22008191049098969\n",
      "training epoch nr 89\n",
      "training loss: 0.20823770761489868\n",
      "validation loss: 0.2182813286781311\n",
      "training epoch nr 90\n",
      "training loss: 0.2077837586402893\n",
      "validation loss: 0.21795885264873505\n",
      "training epoch nr 91\n",
      "training loss: 0.20826727151870728\n",
      "validation loss: 0.21865512430667877\n",
      "training epoch nr 92\n",
      "training loss: 0.2081136554479599\n",
      "validation loss: 0.21834327280521393\n",
      "training epoch nr 93\n",
      "training loss: 0.20801234245300293\n",
      "validation loss: 0.2189164012670517\n",
      "training epoch nr 94\n",
      "training loss: 0.20815858244895935\n",
      "validation loss: 0.2194526493549347\n",
      "training epoch nr 95\n",
      "training loss: 0.20792323350906372\n",
      "validation loss: 0.22063633799552917\n",
      "training epoch nr 96\n",
      "training loss: 0.20758481323719025\n",
      "validation loss: 0.21858324110507965\n",
      "training epoch nr 97\n",
      "training loss: 0.2077430784702301\n",
      "validation loss: 0.2188352793455124\n",
      "training epoch nr 98\n",
      "training loss: 0.20749442279338837\n",
      "validation loss: 0.21866169571876526\n",
      "training epoch nr 99\n",
      "training loss: 0.20733413100242615\n",
      "validation loss: 0.21937204897403717\n",
      "Training model nr 5...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.27217039465904236\n",
      "validation loss: 0.24597063660621643\n",
      "training epoch nr 1\n",
      "training loss: 0.23282742500305176\n",
      "validation loss: 0.23171789944171906\n",
      "training epoch nr 2\n",
      "training loss: 0.22729158401489258\n",
      "validation loss: 0.22565753757953644\n",
      "training epoch nr 3\n",
      "training loss: 0.2245754897594452\n",
      "validation loss: 0.22859209775924683\n",
      "training epoch nr 4\n",
      "training loss: 0.221719890832901\n",
      "validation loss: 0.22354061901569366\n",
      "training epoch nr 5\n",
      "training loss: 0.2201344519853592\n",
      "validation loss: 0.22218407690525055\n",
      "training epoch nr 6\n",
      "training loss: 0.21967950463294983\n",
      "validation loss: 0.21953508257865906\n",
      "training epoch nr 7\n",
      "training loss: 0.21886372566223145\n",
      "validation loss: 0.21861492097377777\n",
      "training epoch nr 8\n",
      "training loss: 0.21783891320228577\n",
      "validation loss: 0.21796734631061554\n",
      "training epoch nr 9\n",
      "training loss: 0.21782420575618744\n",
      "validation loss: 0.21752195060253143\n",
      "training epoch nr 10\n",
      "training loss: 0.21690873801708221\n",
      "validation loss: 0.22623299062252045\n",
      "training epoch nr 11\n",
      "training loss: 0.21663762629032135\n",
      "validation loss: 0.21814854443073273\n",
      "training epoch nr 12\n",
      "training loss: 0.21626858413219452\n",
      "validation loss: 0.21735191345214844\n",
      "training epoch nr 13\n",
      "training loss: 0.21604961156845093\n",
      "validation loss: 0.21727865934371948\n",
      "training epoch nr 14\n",
      "training loss: 0.2158232033252716\n",
      "validation loss: 0.22026817500591278\n",
      "training epoch nr 15\n",
      "training loss: 0.21535596251487732\n",
      "validation loss: 0.21687795221805573\n",
      "training epoch nr 16\n",
      "training loss: 0.2151554673910141\n",
      "validation loss: 0.2176264077425003\n",
      "training epoch nr 17\n",
      "training loss: 0.21525953710079193\n",
      "validation loss: 0.218036949634552\n",
      "training epoch nr 18\n",
      "training loss: 0.21482597291469574\n",
      "validation loss: 0.2164684534072876\n",
      "training epoch nr 19\n",
      "training loss: 0.21425072848796844\n",
      "validation loss: 0.2178756594657898\n",
      "training epoch nr 20\n",
      "training loss: 0.2146802544593811\n",
      "validation loss: 0.22736424207687378\n",
      "training epoch nr 21\n",
      "training loss: 0.21411509811878204\n",
      "validation loss: 0.21623006463050842\n",
      "training epoch nr 22\n",
      "training loss: 0.21435821056365967\n",
      "validation loss: 0.21574971079826355\n",
      "training epoch nr 23\n",
      "training loss: 0.21389885246753693\n",
      "validation loss: 0.2162579894065857\n",
      "training epoch nr 24\n",
      "training loss: 0.2140032947063446\n",
      "validation loss: 0.2149534374475479\n",
      "training epoch nr 25\n",
      "training loss: 0.2137341946363449\n",
      "validation loss: 0.21727603673934937\n",
      "training epoch nr 26\n",
      "training loss: 0.2135619819164276\n",
      "validation loss: 0.21658165752887726\n",
      "training epoch nr 27\n",
      "training loss: 0.21362820267677307\n",
      "validation loss: 0.21927152574062347\n",
      "training epoch nr 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.21311110258102417\n",
      "validation loss: 0.21896104514598846\n",
      "training epoch nr 29\n",
      "training loss: 0.21303772926330566\n",
      "validation loss: 0.21822889149188995\n",
      "training epoch nr 30\n",
      "training loss: 0.21303631365299225\n",
      "validation loss: 0.21654409170150757\n",
      "training epoch nr 31\n",
      "training loss: 0.21290573477745056\n",
      "validation loss: 0.21731796860694885\n",
      "training epoch nr 32\n",
      "training loss: 0.21283023059368134\n",
      "validation loss: 0.21817468106746674\n",
      "training epoch nr 33\n",
      "training loss: 0.21270258724689484\n",
      "validation loss: 0.22134211659431458\n",
      "training epoch nr 34\n",
      "training loss: 0.21250277757644653\n",
      "validation loss: 0.21754556894302368\n",
      "training epoch nr 35\n",
      "training loss: 0.21260994672775269\n",
      "validation loss: 0.21627819538116455\n",
      "training epoch nr 36\n",
      "training loss: 0.2121584713459015\n",
      "validation loss: 0.21633462607860565\n",
      "training epoch nr 37\n",
      "training loss: 0.2119145393371582\n",
      "validation loss: 0.21766172349452972\n",
      "training epoch nr 38\n",
      "training loss: 0.21246108412742615\n",
      "validation loss: 0.21560470759868622\n",
      "training epoch nr 39\n",
      "training loss: 0.2121456265449524\n",
      "validation loss: 0.21786700189113617\n",
      "training epoch nr 40\n",
      "training loss: 0.21199406683444977\n",
      "validation loss: 0.2174360752105713\n",
      "training epoch nr 41\n",
      "training loss: 0.21168206632137299\n",
      "validation loss: 0.2195388525724411\n",
      "training epoch nr 42\n",
      "training loss: 0.21152018010616302\n",
      "validation loss: 0.2163410186767578\n",
      "training epoch nr 43\n",
      "training loss: 0.21154510974884033\n",
      "validation loss: 0.21528905630111694\n",
      "training epoch nr 44\n",
      "training loss: 0.2115841507911682\n",
      "validation loss: 0.21713529527187347\n",
      "training epoch nr 45\n",
      "training loss: 0.2112324833869934\n",
      "validation loss: 0.2195892482995987\n",
      "training epoch nr 46\n",
      "training loss: 0.2112186700105667\n",
      "validation loss: 0.21673572063446045\n",
      "training epoch nr 47\n",
      "training loss: 0.21087011694908142\n",
      "validation loss: 0.217367023229599\n",
      "training epoch nr 48\n",
      "training loss: 0.21137064695358276\n",
      "validation loss: 0.21703650057315826\n",
      "training epoch nr 49\n",
      "training loss: 0.21077050268650055\n",
      "validation loss: 0.2178364098072052\n",
      "training epoch nr 50\n",
      "training loss: 0.2107858657836914\n",
      "validation loss: 0.21839340031147003\n",
      "training epoch nr 51\n",
      "training loss: 0.21063148975372314\n",
      "validation loss: 0.21733444929122925\n",
      "training epoch nr 52\n",
      "training loss: 0.21085010468959808\n",
      "validation loss: 0.21702030301094055\n",
      "training epoch nr 53\n",
      "training loss: 0.2106829136610031\n",
      "validation loss: 0.21601709723472595\n",
      "training epoch nr 54\n",
      "training loss: 0.21058854460716248\n",
      "validation loss: 0.21853181719779968\n",
      "training epoch nr 55\n",
      "training loss: 0.21039600670337677\n",
      "validation loss: 0.21822790801525116\n",
      "training epoch nr 56\n",
      "training loss: 0.21033984422683716\n",
      "validation loss: 0.2169029861688614\n",
      "training epoch nr 57\n",
      "training loss: 0.21015852689743042\n",
      "validation loss: 0.21717777848243713\n",
      "training epoch nr 58\n",
      "training loss: 0.20972974598407745\n",
      "validation loss: 0.21957984566688538\n",
      "training epoch nr 59\n",
      "training loss: 0.21004857122898102\n",
      "validation loss: 0.2173565924167633\n",
      "training epoch nr 60\n",
      "training loss: 0.20973710715770721\n",
      "validation loss: 0.2187044471502304\n",
      "training epoch nr 61\n",
      "training loss: 0.20970557630062103\n",
      "validation loss: 0.21688240766525269\n",
      "training epoch nr 62\n",
      "training loss: 0.21003538370132446\n",
      "validation loss: 0.22000475227832794\n",
      "training epoch nr 63\n",
      "training loss: 0.20978029072284698\n",
      "validation loss: 0.21697363257408142\n",
      "training epoch nr 64\n",
      "training loss: 0.2094300240278244\n",
      "validation loss: 0.21676473319530487\n",
      "training epoch nr 65\n",
      "training loss: 0.20945817232131958\n",
      "validation loss: 0.21828794479370117\n",
      "training epoch nr 66\n",
      "training loss: 0.20934276282787323\n",
      "validation loss: 0.21774594485759735\n",
      "training epoch nr 67\n",
      "training loss: 0.20934979617595673\n",
      "validation loss: 0.21809665858745575\n",
      "training epoch nr 68\n",
      "training loss: 0.20901672542095184\n",
      "validation loss: 0.21770334243774414\n",
      "training epoch nr 69\n",
      "training loss: 0.20925681293010712\n",
      "validation loss: 0.21810659766197205\n",
      "training epoch nr 70\n",
      "training loss: 0.20907607674598694\n",
      "validation loss: 0.21888765692710876\n",
      "training epoch nr 71\n",
      "training loss: 0.20914870500564575\n",
      "validation loss: 0.21803967654705048\n",
      "training epoch nr 72\n",
      "training loss: 0.2091262936592102\n",
      "validation loss: 0.21825504302978516\n",
      "training epoch nr 73\n",
      "training loss: 0.20866678655147552\n",
      "validation loss: 0.21860237419605255\n",
      "training epoch nr 74\n",
      "training loss: 0.20890310406684875\n",
      "validation loss: 0.2169879972934723\n",
      "training epoch nr 75\n",
      "training loss: 0.20870867371559143\n",
      "validation loss: 0.2179054319858551\n",
      "training epoch nr 76\n",
      "training loss: 0.20872709155082703\n",
      "validation loss: 0.21687470376491547\n",
      "training epoch nr 77\n",
      "training loss: 0.2085420787334442\n",
      "validation loss: 0.21780149638652802\n",
      "training epoch nr 78\n",
      "training loss: 0.2085602581501007\n",
      "validation loss: 0.21810917556285858\n",
      "training epoch nr 79\n",
      "training loss: 0.20830805599689484\n",
      "validation loss: 0.21894307434558868\n",
      "training epoch nr 80\n",
      "training loss: 0.20843413472175598\n",
      "validation loss: 0.2179105579853058\n",
      "training epoch nr 81\n",
      "training loss: 0.20843830704689026\n",
      "validation loss: 0.22020792961120605\n",
      "training epoch nr 82\n",
      "training loss: 0.20810800790786743\n",
      "validation loss: 0.2196107655763626\n",
      "training epoch nr 83\n",
      "training loss: 0.20783831179141998\n",
      "validation loss: 0.2217661291360855\n",
      "training epoch nr 84\n",
      "training loss: 0.20779423415660858\n",
      "validation loss: 0.2189846634864807\n",
      "training epoch nr 85\n",
      "training loss: 0.20774449408054352\n",
      "validation loss: 0.21904703974723816\n",
      "training epoch nr 86\n",
      "training loss: 0.20776315033435822\n",
      "validation loss: 0.21870610117912292\n",
      "training epoch nr 87\n",
      "training loss: 0.20773537456989288\n",
      "validation loss: 0.21880313754081726\n",
      "training epoch nr 88\n",
      "training loss: 0.20782282948493958\n",
      "validation loss: 0.21863408386707306\n",
      "training epoch nr 89\n",
      "training loss: 0.20774796605110168\n",
      "validation loss: 0.2193034142255783\n",
      "training epoch nr 90\n",
      "training loss: 0.20747968554496765\n",
      "validation loss: 0.21817150712013245\n",
      "training epoch nr 91\n",
      "training loss: 0.20763826370239258\n",
      "validation loss: 0.21834240853786469\n",
      "training epoch nr 92\n",
      "training loss: 0.20729833841323853\n",
      "validation loss: 0.21866177022457123\n",
      "training epoch nr 93\n",
      "training loss: 0.20732955634593964\n",
      "validation loss: 0.21891020238399506\n",
      "training epoch nr 94\n",
      "training loss: 0.2073010951280594\n",
      "validation loss: 0.219002366065979\n",
      "training epoch nr 95\n",
      "training loss: 0.20723389089107513\n",
      "validation loss: 0.21894405782222748\n",
      "training epoch nr 96\n",
      "training loss: 0.20728839933872223\n",
      "validation loss: 0.2195771485567093\n",
      "training epoch nr 97\n",
      "training loss: 0.20725077390670776\n",
      "validation loss: 0.2204522043466568\n",
      "training epoch nr 98\n",
      "training loss: 0.20735813677310944\n",
      "validation loss: 0.21893659234046936\n",
      "training epoch nr 99\n",
      "training loss: 0.20681139826774597\n",
      "validation loss: 0.22254176437854767\n",
      "Training model nr 6...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.27400559186935425\n",
      "validation loss: 0.23576298356056213\n",
      "training epoch nr 1\n",
      "training loss: 0.2314736396074295\n",
      "validation loss: 0.2270616590976715\n",
      "training epoch nr 2\n",
      "training loss: 0.22614093124866486\n",
      "validation loss: 0.22369177639484406\n",
      "training epoch nr 3\n",
      "training loss: 0.22251683473587036\n",
      "validation loss: 0.22293147444725037\n",
      "training epoch nr 4\n",
      "training loss: 0.221359521150589\n",
      "validation loss: 0.22103938460350037\n",
      "training epoch nr 5\n",
      "training loss: 0.2198338657617569\n",
      "validation loss: 0.22119541466236115\n",
      "training epoch nr 6\n",
      "training loss: 0.21879003942012787\n",
      "validation loss: 0.21772891283035278\n",
      "training epoch nr 7\n",
      "training loss: 0.21783402562141418\n",
      "validation loss: 0.2194768190383911\n",
      "training epoch nr 8\n",
      "training loss: 0.21816718578338623\n",
      "validation loss: 0.21859002113342285\n",
      "training epoch nr 9\n",
      "training loss: 0.2172732949256897\n",
      "validation loss: 0.2243790328502655\n",
      "training epoch nr 10\n",
      "training loss: 0.21693602204322815\n",
      "validation loss: 0.2175421565771103\n",
      "training epoch nr 11\n",
      "training loss: 0.21615231037139893\n",
      "validation loss: 0.21646907925605774\n",
      "training epoch nr 12\n",
      "training loss: 0.21590396761894226\n",
      "validation loss: 0.2179294377565384\n",
      "training epoch nr 13\n",
      "training loss: 0.21545013785362244\n",
      "validation loss: 0.21788978576660156\n",
      "training epoch nr 14\n",
      "training loss: 0.21562132239341736\n",
      "validation loss: 0.21963730454444885\n",
      "training epoch nr 15\n",
      "training loss: 0.21523451805114746\n",
      "validation loss: 0.2172532081604004\n",
      "training epoch nr 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.21457016468048096\n",
      "validation loss: 0.21700550615787506\n",
      "training epoch nr 17\n",
      "training loss: 0.21482600271701813\n",
      "validation loss: 0.2171909064054489\n",
      "training epoch nr 18\n",
      "training loss: 0.21438559889793396\n",
      "validation loss: 0.2178259789943695\n",
      "training epoch nr 19\n",
      "training loss: 0.21381118893623352\n",
      "validation loss: 0.2186523675918579\n",
      "training epoch nr 20\n",
      "training loss: 0.2137349545955658\n",
      "validation loss: 0.21962516009807587\n",
      "training epoch nr 21\n",
      "training loss: 0.2141512930393219\n",
      "validation loss: 0.21829918026924133\n",
      "training epoch nr 22\n",
      "training loss: 0.21340319514274597\n",
      "validation loss: 0.21759635210037231\n",
      "training epoch nr 23\n",
      "training loss: 0.21397462487220764\n",
      "validation loss: 0.21690234541893005\n",
      "training epoch nr 24\n",
      "training loss: 0.21389532089233398\n",
      "validation loss: 0.2153298407793045\n",
      "training epoch nr 25\n",
      "training loss: 0.2131940871477127\n",
      "validation loss: 0.21701224148273468\n",
      "training epoch nr 26\n",
      "training loss: 0.2131909877061844\n",
      "validation loss: 0.21601392328739166\n",
      "training epoch nr 27\n",
      "training loss: 0.2131095677614212\n",
      "validation loss: 0.21646153926849365\n",
      "training epoch nr 28\n",
      "training loss: 0.2130686193704605\n",
      "validation loss: 0.21642589569091797\n",
      "training epoch nr 29\n",
      "training loss: 0.21264687180519104\n",
      "validation loss: 0.21864868700504303\n",
      "training epoch nr 30\n",
      "training loss: 0.2129765897989273\n",
      "validation loss: 0.21643656492233276\n",
      "training epoch nr 31\n",
      "training loss: 0.2125667929649353\n",
      "validation loss: 0.21868360042572021\n",
      "training epoch nr 32\n",
      "training loss: 0.2126433402299881\n",
      "validation loss: 0.21632632613182068\n",
      "training epoch nr 33\n",
      "training loss: 0.21259146928787231\n",
      "validation loss: 0.21651434898376465\n",
      "training epoch nr 34\n",
      "training loss: 0.21210594475269318\n",
      "validation loss: 0.21761323511600494\n",
      "training epoch nr 35\n",
      "training loss: 0.2122785747051239\n",
      "validation loss: 0.21648472547531128\n",
      "training epoch nr 36\n",
      "training loss: 0.21194790303707123\n",
      "validation loss: 0.21808059513568878\n",
      "training epoch nr 37\n",
      "training loss: 0.21158669888973236\n",
      "validation loss: 0.215998575091362\n",
      "training epoch nr 38\n",
      "training loss: 0.21178807318210602\n",
      "validation loss: 0.21800872683525085\n",
      "training epoch nr 39\n",
      "training loss: 0.21144701540470123\n",
      "validation loss: 0.21696726977825165\n",
      "training epoch nr 40\n",
      "training loss: 0.21187545359134674\n",
      "validation loss: 0.21921899914741516\n",
      "training epoch nr 41\n",
      "training loss: 0.21143193542957306\n",
      "validation loss: 0.21573518216609955\n",
      "training epoch nr 42\n",
      "training loss: 0.21139423549175262\n",
      "validation loss: 0.2169344276189804\n",
      "training epoch nr 43\n",
      "training loss: 0.21120776236057281\n",
      "validation loss: 0.216155007481575\n",
      "training epoch nr 44\n",
      "training loss: 0.2113124430179596\n",
      "validation loss: 0.2159424126148224\n",
      "training epoch nr 45\n",
      "training loss: 0.21107934415340424\n",
      "validation loss: 0.21589507162570953\n",
      "training epoch nr 46\n",
      "training loss: 0.2107941210269928\n",
      "validation loss: 0.21785537898540497\n",
      "training epoch nr 47\n",
      "training loss: 0.21076303720474243\n",
      "validation loss: 0.21736735105514526\n",
      "training epoch nr 48\n",
      "training loss: 0.21064597368240356\n",
      "validation loss: 0.21802622079849243\n",
      "training epoch nr 49\n",
      "training loss: 0.21078278124332428\n",
      "validation loss: 0.21780328452587128\n",
      "training epoch nr 50\n",
      "training loss: 0.21067193150520325\n",
      "validation loss: 0.2184726446866989\n",
      "training epoch nr 51\n",
      "training loss: 0.2106783539056778\n",
      "validation loss: 0.21918907761573792\n",
      "training epoch nr 52\n",
      "training loss: 0.21042349934577942\n",
      "validation loss: 0.217643141746521\n",
      "training epoch nr 53\n",
      "training loss: 0.21019181609153748\n",
      "validation loss: 0.21699835360050201\n",
      "training epoch nr 54\n",
      "training loss: 0.2100311517715454\n",
      "validation loss: 0.21795792877674103\n",
      "training epoch nr 55\n",
      "training loss: 0.20989088714122772\n",
      "validation loss: 0.21749627590179443\n",
      "training epoch nr 56\n",
      "training loss: 0.20999138057231903\n",
      "validation loss: 0.21911504864692688\n",
      "training epoch nr 57\n",
      "training loss: 0.20989641547203064\n",
      "validation loss: 0.2181701362133026\n",
      "training epoch nr 58\n",
      "training loss: 0.2098008543252945\n",
      "validation loss: 0.21929070353507996\n",
      "training epoch nr 59\n",
      "training loss: 0.2095032036304474\n",
      "validation loss: 0.2176666408777237\n",
      "training epoch nr 60\n",
      "training loss: 0.20959381759166718\n",
      "validation loss: 0.2175779789686203\n",
      "training epoch nr 61\n",
      "training loss: 0.2097020447254181\n",
      "validation loss: 0.2183104008436203\n",
      "training epoch nr 62\n",
      "training loss: 0.20916125178337097\n",
      "validation loss: 0.21958112716674805\n",
      "training epoch nr 63\n",
      "training loss: 0.20917509496212006\n",
      "validation loss: 0.2172180861234665\n",
      "training epoch nr 64\n",
      "training loss: 0.209333598613739\n",
      "validation loss: 0.21752838790416718\n",
      "training epoch nr 65\n",
      "training loss: 0.20903794467449188\n",
      "validation loss: 0.2188323438167572\n",
      "training epoch nr 66\n",
      "training loss: 0.20912910997867584\n",
      "validation loss: 0.22423920035362244\n",
      "training epoch nr 67\n",
      "training loss: 0.20912109315395355\n",
      "validation loss: 0.21853847801685333\n",
      "training epoch nr 68\n",
      "training loss: 0.20918132364749908\n",
      "validation loss: 0.21842776238918304\n",
      "training epoch nr 69\n",
      "training loss: 0.20872794091701508\n",
      "validation loss: 0.2184228003025055\n",
      "training epoch nr 70\n",
      "training loss: 0.2087298035621643\n",
      "validation loss: 0.21959932148456573\n",
      "training epoch nr 71\n",
      "training loss: 0.2085113227367401\n",
      "validation loss: 0.2186073213815689\n",
      "training epoch nr 72\n",
      "training loss: 0.2084122896194458\n",
      "validation loss: 0.21801161766052246\n",
      "training epoch nr 73\n",
      "training loss: 0.20870773494243622\n",
      "validation loss: 0.21879848837852478\n",
      "training epoch nr 74\n",
      "training loss: 0.20835669338703156\n",
      "validation loss: 0.21882615983486176\n",
      "training epoch nr 75\n",
      "training loss: 0.20830845832824707\n",
      "validation loss: 0.22116386890411377\n",
      "training epoch nr 76\n",
      "training loss: 0.2086675614118576\n",
      "validation loss: 0.21869121491909027\n",
      "training epoch nr 77\n",
      "training loss: 0.2079930454492569\n",
      "validation loss: 0.21878348290920258\n",
      "training epoch nr 78\n",
      "training loss: 0.20831382274627686\n",
      "validation loss: 0.2189386934041977\n",
      "training epoch nr 79\n",
      "training loss: 0.2080153524875641\n",
      "validation loss: 0.2202688604593277\n",
      "training epoch nr 80\n",
      "training loss: 0.2078041136264801\n",
      "validation loss: 0.21941441297531128\n",
      "training epoch nr 81\n",
      "training loss: 0.20775064826011658\n",
      "validation loss: 0.2211039513349533\n",
      "training epoch nr 82\n",
      "training loss: 0.2080582231283188\n",
      "validation loss: 0.218885600566864\n",
      "training epoch nr 83\n",
      "training loss: 0.20749254524707794\n",
      "validation loss: 0.21928708255290985\n",
      "training epoch nr 84\n",
      "training loss: 0.20759285986423492\n",
      "validation loss: 0.21839943528175354\n",
      "training epoch nr 85\n",
      "training loss: 0.20760387182235718\n",
      "validation loss: 0.22012026607990265\n",
      "training epoch nr 86\n",
      "training loss: 0.2074284553527832\n",
      "validation loss: 0.22064629197120667\n",
      "training epoch nr 87\n",
      "training loss: 0.20779165625572205\n",
      "validation loss: 0.22090576589107513\n",
      "training epoch nr 88\n",
      "training loss: 0.20708362758159637\n",
      "validation loss: 0.22297412157058716\n",
      "training epoch nr 89\n",
      "training loss: 0.20690667629241943\n",
      "validation loss: 0.2209748774766922\n",
      "training epoch nr 90\n",
      "training loss: 0.20716062188148499\n",
      "validation loss: 0.2197893112897873\n",
      "training epoch nr 91\n",
      "training loss: 0.20696251094341278\n",
      "validation loss: 0.2198949009180069\n",
      "training epoch nr 92\n",
      "training loss: 0.20674532651901245\n",
      "validation loss: 0.2206445187330246\n",
      "training epoch nr 93\n",
      "training loss: 0.20677582919597626\n",
      "validation loss: 0.22051362693309784\n",
      "training epoch nr 94\n",
      "training loss: 0.20669417083263397\n",
      "validation loss: 0.221002459526062\n",
      "training epoch nr 95\n",
      "training loss: 0.20652927458286285\n",
      "validation loss: 0.2219298928976059\n",
      "training epoch nr 96\n",
      "training loss: 0.206546351313591\n",
      "validation loss: 0.22173434495925903\n",
      "training epoch nr 97\n",
      "training loss: 0.20651622116565704\n",
      "validation loss: 0.22195053100585938\n",
      "training epoch nr 98\n",
      "training loss: 0.20632445812225342\n",
      "validation loss: 0.22062788903713226\n",
      "training epoch nr 99\n",
      "training loss: 0.20619472861289978\n",
      "validation loss: 0.2219037562608719\n",
      "Training model nr 7...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2726787030696869\n",
      "validation loss: 0.24603699147701263\n",
      "training epoch nr 1\n",
      "training loss: 0.2314407378435135\n",
      "validation loss: 0.23157908022403717\n",
      "training epoch nr 2\n",
      "training loss: 0.2248195856809616\n",
      "validation loss: 0.2264041304588318\n",
      "training epoch nr 3\n",
      "training loss: 0.2224232703447342\n",
      "validation loss: 0.22285538911819458\n",
      "training epoch nr 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2214488536119461\n",
      "validation loss: 0.2219044715166092\n",
      "training epoch nr 5\n",
      "training loss: 0.2200639247894287\n",
      "validation loss: 0.21971647441387177\n",
      "training epoch nr 6\n",
      "training loss: 0.2195703685283661\n",
      "validation loss: 0.21790678799152374\n",
      "training epoch nr 7\n",
      "training loss: 0.2186463326215744\n",
      "validation loss: 0.22052302956581116\n",
      "training epoch nr 8\n",
      "training loss: 0.21793444454669952\n",
      "validation loss: 0.21843716502189636\n",
      "training epoch nr 9\n",
      "training loss: 0.21698947250843048\n",
      "validation loss: 0.21837688982486725\n",
      "training epoch nr 10\n",
      "training loss: 0.2168710082769394\n",
      "validation loss: 0.21888960897922516\n",
      "training epoch nr 11\n",
      "training loss: 0.216077983379364\n",
      "validation loss: 0.21862490475177765\n",
      "training epoch nr 12\n",
      "training loss: 0.21610337495803833\n",
      "validation loss: 0.21789583563804626\n",
      "training epoch nr 13\n",
      "training loss: 0.21587032079696655\n",
      "validation loss: 0.22023963928222656\n",
      "training epoch nr 14\n",
      "training loss: 0.21538807451725006\n",
      "validation loss: 0.2161852866411209\n",
      "training epoch nr 15\n",
      "training loss: 0.21521539986133575\n",
      "validation loss: 0.2171221822500229\n",
      "training epoch nr 16\n",
      "training loss: 0.21462757885456085\n",
      "validation loss: 0.21615120768547058\n",
      "training epoch nr 17\n",
      "training loss: 0.2147553265094757\n",
      "validation loss: 0.21590428054332733\n",
      "training epoch nr 18\n",
      "training loss: 0.2148120105266571\n",
      "validation loss: 0.2187339961528778\n",
      "training epoch nr 19\n",
      "training loss: 0.21453456580638885\n",
      "validation loss: 0.21673116087913513\n",
      "training epoch nr 20\n",
      "training loss: 0.21430140733718872\n",
      "validation loss: 0.21753819286823273\n",
      "training epoch nr 21\n",
      "training loss: 0.2140151411294937\n",
      "validation loss: 0.2173573523759842\n",
      "training epoch nr 22\n",
      "training loss: 0.21361815929412842\n",
      "validation loss: 0.21749842166900635\n",
      "training epoch nr 23\n",
      "training loss: 0.21376186609268188\n",
      "validation loss: 0.21831782162189484\n",
      "training epoch nr 24\n",
      "training loss: 0.21343855559825897\n",
      "validation loss: 0.21567142009735107\n",
      "training epoch nr 25\n",
      "training loss: 0.21295519173145294\n",
      "validation loss: 0.21637235581874847\n",
      "training epoch nr 26\n",
      "training loss: 0.2132342904806137\n",
      "validation loss: 0.21620243787765503\n",
      "training epoch nr 27\n",
      "training loss: 0.21312755346298218\n",
      "validation loss: 0.21685655415058136\n",
      "training epoch nr 28\n",
      "training loss: 0.21290312707424164\n",
      "validation loss: 0.21654750406742096\n",
      "training epoch nr 29\n",
      "training loss: 0.2129562944173813\n",
      "validation loss: 0.2174658328294754\n",
      "training epoch nr 30\n",
      "training loss: 0.21283791959285736\n",
      "validation loss: 0.21737495064735413\n",
      "training epoch nr 31\n",
      "training loss: 0.2124897986650467\n",
      "validation loss: 0.21990840137004852\n",
      "training epoch nr 32\n",
      "training loss: 0.21225672960281372\n",
      "validation loss: 0.21608750522136688\n",
      "training epoch nr 33\n",
      "training loss: 0.21177539229393005\n",
      "validation loss: 0.2183476835489273\n",
      "training epoch nr 34\n",
      "training loss: 0.21223753690719604\n",
      "validation loss: 0.21573932468891144\n",
      "training epoch nr 35\n",
      "training loss: 0.21209946274757385\n",
      "validation loss: 0.21579508483409882\n",
      "training epoch nr 36\n",
      "training loss: 0.21174639463424683\n",
      "validation loss: 0.21634069085121155\n",
      "training epoch nr 37\n",
      "training loss: 0.2116718739271164\n",
      "validation loss: 0.21599382162094116\n",
      "training epoch nr 38\n",
      "training loss: 0.21157583594322205\n",
      "validation loss: 0.21731536090373993\n",
      "training epoch nr 39\n",
      "training loss: 0.2114373743534088\n",
      "validation loss: 0.21767418086528778\n",
      "training epoch nr 40\n",
      "training loss: 0.21140576899051666\n",
      "validation loss: 0.21962468326091766\n",
      "training epoch nr 41\n",
      "training loss: 0.2111438512802124\n",
      "validation loss: 0.21717995405197144\n",
      "training epoch nr 42\n",
      "training loss: 0.21119514107704163\n",
      "validation loss: 0.21624760329723358\n",
      "training epoch nr 43\n",
      "training loss: 0.2108728438615799\n",
      "validation loss: 0.21698538959026337\n",
      "training epoch nr 44\n",
      "training loss: 0.21064957976341248\n",
      "validation loss: 0.21795956790447235\n",
      "training epoch nr 45\n",
      "training loss: 0.21068331599235535\n",
      "validation loss: 0.21759486198425293\n",
      "training epoch nr 46\n",
      "training loss: 0.2107895165681839\n",
      "validation loss: 0.21792063117027283\n",
      "training epoch nr 47\n",
      "training loss: 0.2105986773967743\n",
      "validation loss: 0.21980181336402893\n",
      "training epoch nr 48\n",
      "training loss: 0.2103254795074463\n",
      "validation loss: 0.21655578911304474\n",
      "training epoch nr 49\n",
      "training loss: 0.21020467579364777\n",
      "validation loss: 0.21722522377967834\n",
      "training epoch nr 50\n",
      "training loss: 0.21011948585510254\n",
      "validation loss: 0.21614675223827362\n",
      "training epoch nr 51\n",
      "training loss: 0.2097480744123459\n",
      "validation loss: 0.21823611855506897\n",
      "training epoch nr 52\n",
      "training loss: 0.20991860330104828\n",
      "validation loss: 0.21761848032474518\n",
      "training epoch nr 53\n",
      "training loss: 0.21011139452457428\n",
      "validation loss: 0.21684588491916656\n",
      "training epoch nr 54\n",
      "training loss: 0.20970310270786285\n",
      "validation loss: 0.21765044331550598\n",
      "training epoch nr 55\n",
      "training loss: 0.21002411842346191\n",
      "validation loss: 0.21839554607868195\n",
      "training epoch nr 56\n",
      "training loss: 0.209614098072052\n",
      "validation loss: 0.2163742035627365\n",
      "training epoch nr 57\n",
      "training loss: 0.209375262260437\n",
      "validation loss: 0.21717992424964905\n",
      "training epoch nr 58\n",
      "training loss: 0.20948626101016998\n",
      "validation loss: 0.21666103601455688\n",
      "training epoch nr 59\n",
      "training loss: 0.20945590734481812\n",
      "validation loss: 0.21976129710674286\n",
      "training epoch nr 60\n",
      "training loss: 0.20930780470371246\n",
      "validation loss: 0.21732991933822632\n",
      "training epoch nr 61\n",
      "training loss: 0.2090114802122116\n",
      "validation loss: 0.21877622604370117\n",
      "training epoch nr 62\n",
      "training loss: 0.2085687518119812\n",
      "validation loss: 0.21755845844745636\n",
      "training epoch nr 63\n",
      "training loss: 0.2089584320783615\n",
      "validation loss: 0.2189328670501709\n",
      "training epoch nr 64\n",
      "training loss: 0.20866331458091736\n",
      "validation loss: 0.2172194868326187\n",
      "training epoch nr 65\n",
      "training loss: 0.20835033059120178\n",
      "validation loss: 0.21909669041633606\n",
      "training epoch nr 66\n",
      "training loss: 0.20837406814098358\n",
      "validation loss: 0.21888136863708496\n",
      "training epoch nr 67\n",
      "training loss: 0.20804519951343536\n",
      "validation loss: 0.21920283138751984\n",
      "training epoch nr 68\n",
      "training loss: 0.20819559693336487\n",
      "validation loss: 0.21803885698318481\n",
      "training epoch nr 69\n",
      "training loss: 0.2079267054796219\n",
      "validation loss: 0.22227433323860168\n",
      "training epoch nr 70\n",
      "training loss: 0.20814631879329681\n",
      "validation loss: 0.21906335651874542\n",
      "training epoch nr 71\n",
      "training loss: 0.2081279307603836\n",
      "validation loss: 0.21834303438663483\n",
      "training epoch nr 72\n",
      "training loss: 0.2082383930683136\n",
      "validation loss: 0.21925154328346252\n",
      "training epoch nr 73\n",
      "training loss: 0.20791886746883392\n",
      "validation loss: 0.2197725623846054\n",
      "training epoch nr 74\n",
      "training loss: 0.2078121304512024\n",
      "validation loss: 0.21865758299827576\n",
      "training epoch nr 75\n",
      "training loss: 0.20773029327392578\n",
      "validation loss: 0.21780598163604736\n",
      "training epoch nr 76\n",
      "training loss: 0.20755353569984436\n",
      "validation loss: 0.21836024522781372\n",
      "training epoch nr 77\n",
      "training loss: 0.20740476250648499\n",
      "validation loss: 0.21936149895191193\n",
      "training epoch nr 78\n",
      "training loss: 0.20716699957847595\n",
      "validation loss: 0.21898949146270752\n",
      "training epoch nr 79\n",
      "training loss: 0.20742695033550262\n",
      "validation loss: 0.22042343020439148\n",
      "training epoch nr 80\n",
      "training loss: 0.20735715329647064\n",
      "validation loss: 0.21902050077915192\n",
      "training epoch nr 81\n",
      "training loss: 0.20728197693824768\n",
      "validation loss: 0.22033801674842834\n",
      "training epoch nr 82\n",
      "training loss: 0.2068348526954651\n",
      "validation loss: 0.21950334310531616\n",
      "training epoch nr 83\n",
      "training loss: 0.20660673081874847\n",
      "validation loss: 0.2224489003419876\n",
      "training epoch nr 84\n",
      "training loss: 0.20658792555332184\n",
      "validation loss: 0.22237944602966309\n",
      "training epoch nr 85\n",
      "training loss: 0.20662495493888855\n",
      "validation loss: 0.22056466341018677\n",
      "training epoch nr 86\n",
      "training loss: 0.20649495720863342\n",
      "validation loss: 0.21908918023109436\n",
      "training epoch nr 87\n",
      "training loss: 0.20632794499397278\n",
      "validation loss: 0.22186127305030823\n",
      "training epoch nr 88\n",
      "training loss: 0.20652784407138824\n",
      "validation loss: 0.22067759931087494\n",
      "training epoch nr 89\n",
      "training loss: 0.2059648334980011\n",
      "validation loss: 0.22008197009563446\n",
      "training epoch nr 90\n",
      "training loss: 0.20592662692070007\n",
      "validation loss: 0.2207459956407547\n",
      "training epoch nr 91\n",
      "training loss: 0.20621567964553833\n",
      "validation loss: 0.22245126962661743\n",
      "training epoch nr 92\n",
      "training loss: 0.2058839648962021\n",
      "validation loss: 0.22192972898483276\n",
      "training epoch nr 93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2057347446680069\n",
      "validation loss: 0.2235286384820938\n",
      "training epoch nr 94\n",
      "training loss: 0.2060060054063797\n",
      "validation loss: 0.22063715755939484\n",
      "training epoch nr 95\n",
      "training loss: 0.20575805008411407\n",
      "validation loss: 0.22072423994541168\n",
      "training epoch nr 96\n",
      "training loss: 0.2058558166027069\n",
      "validation loss: 0.22181719541549683\n",
      "training epoch nr 97\n",
      "training loss: 0.20571641623973846\n",
      "validation loss: 0.22086039185523987\n",
      "training epoch nr 98\n",
      "training loss: 0.2052917778491974\n",
      "validation loss: 0.2220257669687271\n",
      "training epoch nr 99\n",
      "training loss: 0.20586198568344116\n",
      "validation loss: 0.22238734364509583\n",
      "Training model nr 8...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2683088481426239\n",
      "validation loss: 0.23525655269622803\n",
      "training epoch nr 1\n",
      "training loss: 0.22936506569385529\n",
      "validation loss: 0.2262234389781952\n",
      "training epoch nr 2\n",
      "training loss: 0.2238752394914627\n",
      "validation loss: 0.22170905768871307\n",
      "training epoch nr 3\n",
      "training loss: 0.2218078225851059\n",
      "validation loss: 0.2222181260585785\n",
      "training epoch nr 4\n",
      "training loss: 0.22112183272838593\n",
      "validation loss: 0.22202840447425842\n",
      "training epoch nr 5\n",
      "training loss: 0.2195785790681839\n",
      "validation loss: 0.22146038711071014\n",
      "training epoch nr 6\n",
      "training loss: 0.21817730367183685\n",
      "validation loss: 0.21921366453170776\n",
      "training epoch nr 7\n",
      "training loss: 0.21784327924251556\n",
      "validation loss: 0.2203318327665329\n",
      "training epoch nr 8\n",
      "training loss: 0.2173146903514862\n",
      "validation loss: 0.21979771554470062\n",
      "training epoch nr 9\n",
      "training loss: 0.2166443169116974\n",
      "validation loss: 0.21940137445926666\n",
      "training epoch nr 10\n",
      "training loss: 0.21644547581672668\n",
      "validation loss: 0.22079584002494812\n",
      "training epoch nr 11\n",
      "training loss: 0.21584869921207428\n",
      "validation loss: 0.21839112043380737\n",
      "training epoch nr 12\n",
      "training loss: 0.215563103556633\n",
      "validation loss: 0.21763268113136292\n",
      "training epoch nr 13\n",
      "training loss: 0.2155558466911316\n",
      "validation loss: 0.2173912227153778\n",
      "training epoch nr 14\n",
      "training loss: 0.2155555784702301\n",
      "validation loss: 0.21770264208316803\n",
      "training epoch nr 15\n",
      "training loss: 0.21485446393489838\n",
      "validation loss: 0.21838736534118652\n",
      "training epoch nr 16\n",
      "training loss: 0.2144152820110321\n",
      "validation loss: 0.21576610207557678\n",
      "training epoch nr 17\n",
      "training loss: 0.21414068341255188\n",
      "validation loss: 0.2204827517271042\n",
      "training epoch nr 18\n",
      "training loss: 0.214554101228714\n",
      "validation loss: 0.21985431015491486\n",
      "training epoch nr 19\n",
      "training loss: 0.21415425837039948\n",
      "validation loss: 0.21828322112560272\n",
      "training epoch nr 20\n",
      "training loss: 0.21391817927360535\n",
      "validation loss: 0.21609778702259064\n",
      "training epoch nr 21\n",
      "training loss: 0.21390102803707123\n",
      "validation loss: 0.21601083874702454\n",
      "training epoch nr 22\n",
      "training loss: 0.21356122195720673\n",
      "validation loss: 0.21628545224666595\n",
      "training epoch nr 23\n",
      "training loss: 0.21298952400684357\n",
      "validation loss: 0.217603862285614\n",
      "training epoch nr 24\n",
      "training loss: 0.21305757761001587\n",
      "validation loss: 0.21961002051830292\n",
      "training epoch nr 25\n",
      "training loss: 0.21307991445064545\n",
      "validation loss: 0.21742123365402222\n",
      "training epoch nr 26\n",
      "training loss: 0.21273238956928253\n",
      "validation loss: 0.21788272261619568\n",
      "training epoch nr 27\n",
      "training loss: 0.2129223793745041\n",
      "validation loss: 0.21943050622940063\n",
      "training epoch nr 28\n",
      "training loss: 0.2126336544752121\n",
      "validation loss: 0.21643440425395966\n",
      "training epoch nr 29\n",
      "training loss: 0.2121409773826599\n",
      "validation loss: 0.2178044319152832\n",
      "training epoch nr 30\n",
      "training loss: 0.21229475736618042\n",
      "validation loss: 0.21929912269115448\n",
      "training epoch nr 31\n",
      "training loss: 0.21215428411960602\n",
      "validation loss: 0.2173401266336441\n",
      "training epoch nr 32\n",
      "training loss: 0.21214787662029266\n",
      "validation loss: 0.2192329615354538\n",
      "training epoch nr 33\n",
      "training loss: 0.21201637387275696\n",
      "validation loss: 0.21638892590999603\n",
      "training epoch nr 34\n",
      "training loss: 0.21163873374462128\n",
      "validation loss: 0.2175610065460205\n",
      "training epoch nr 35\n",
      "training loss: 0.21182243525981903\n",
      "validation loss: 0.21871928870677948\n",
      "training epoch nr 36\n",
      "training loss: 0.21173882484436035\n",
      "validation loss: 0.21718744933605194\n",
      "training epoch nr 37\n",
      "training loss: 0.21129751205444336\n",
      "validation loss: 0.21890704333782196\n",
      "training epoch nr 38\n",
      "training loss: 0.21125274896621704\n",
      "validation loss: 0.21804405748844147\n",
      "training epoch nr 39\n",
      "training loss: 0.2114739567041397\n",
      "validation loss: 0.21678496897220612\n",
      "training epoch nr 40\n",
      "training loss: 0.21117731928825378\n",
      "validation loss: 0.2163039594888687\n",
      "training epoch nr 41\n",
      "training loss: 0.2108025997877121\n",
      "validation loss: 0.22061192989349365\n",
      "training epoch nr 42\n",
      "training loss: 0.21088193356990814\n",
      "validation loss: 0.21905815601348877\n",
      "training epoch nr 43\n",
      "training loss: 0.21087650954723358\n",
      "validation loss: 0.2201307862997055\n",
      "training epoch nr 44\n",
      "training loss: 0.21079856157302856\n",
      "validation loss: 0.21792685985565186\n",
      "training epoch nr 45\n",
      "training loss: 0.21061564981937408\n",
      "validation loss: 0.21763645112514496\n",
      "training epoch nr 46\n",
      "training loss: 0.21081654727458954\n",
      "validation loss: 0.21841378509998322\n",
      "training epoch nr 47\n",
      "training loss: 0.2102767527103424\n",
      "validation loss: 0.21920007467269897\n",
      "training epoch nr 48\n",
      "training loss: 0.21011102199554443\n",
      "validation loss: 0.2180289626121521\n",
      "training epoch nr 49\n",
      "training loss: 0.21066749095916748\n",
      "validation loss: 0.2188877910375595\n",
      "training epoch nr 50\n",
      "training loss: 0.21017193794250488\n",
      "validation loss: 0.21683861315250397\n",
      "training epoch nr 51\n",
      "training loss: 0.20991124212741852\n",
      "validation loss: 0.2178160548210144\n",
      "training epoch nr 52\n",
      "training loss: 0.21007396280765533\n",
      "validation loss: 0.21733488142490387\n",
      "training epoch nr 53\n",
      "training loss: 0.2100435197353363\n",
      "validation loss: 0.21813996136188507\n",
      "training epoch nr 54\n",
      "training loss: 0.20978723466396332\n",
      "validation loss: 0.217543825507164\n",
      "training epoch nr 55\n",
      "training loss: 0.20957203209400177\n",
      "validation loss: 0.21851271390914917\n",
      "training epoch nr 56\n",
      "training loss: 0.20934876799583435\n",
      "validation loss: 0.21727171540260315\n",
      "training epoch nr 57\n",
      "training loss: 0.20935943722724915\n",
      "validation loss: 0.2184213399887085\n",
      "training epoch nr 58\n",
      "training loss: 0.20920580625534058\n",
      "validation loss: 0.2205134630203247\n",
      "training epoch nr 59\n",
      "training loss: 0.20910140872001648\n",
      "validation loss: 0.2191203087568283\n",
      "training epoch nr 60\n",
      "training loss: 0.2092055082321167\n",
      "validation loss: 0.2183746099472046\n",
      "training epoch nr 61\n",
      "training loss: 0.20893655717372894\n",
      "validation loss: 0.21929915249347687\n",
      "training epoch nr 62\n",
      "training loss: 0.20886936783790588\n",
      "validation loss: 0.21825052797794342\n",
      "training epoch nr 63\n",
      "training loss: 0.20858928561210632\n",
      "validation loss: 0.21946635842323303\n",
      "training epoch nr 64\n",
      "training loss: 0.20855316519737244\n",
      "validation loss: 0.2216918021440506\n",
      "training epoch nr 65\n",
      "training loss: 0.20883849263191223\n",
      "validation loss: 0.22172759473323822\n",
      "training epoch nr 66\n",
      "training loss: 0.20848943293094635\n",
      "validation loss: 0.21890977025032043\n",
      "training epoch nr 67\n",
      "training loss: 0.20861448347568512\n",
      "validation loss: 0.2215527445077896\n",
      "training epoch nr 68\n",
      "training loss: 0.2085878849029541\n",
      "validation loss: 0.2189335823059082\n",
      "training epoch nr 69\n",
      "training loss: 0.2079867720603943\n",
      "validation loss: 0.2178523689508438\n",
      "training epoch nr 70\n",
      "training loss: 0.20796337723731995\n",
      "validation loss: 0.2192636877298355\n",
      "training epoch nr 71\n",
      "training loss: 0.20818878710269928\n",
      "validation loss: 0.21994681656360626\n",
      "training epoch nr 72\n",
      "training loss: 0.20800399780273438\n",
      "validation loss: 0.21949905157089233\n",
      "training epoch nr 73\n",
      "training loss: 0.20768354833126068\n",
      "validation loss: 0.22050508856773376\n",
      "training epoch nr 74\n",
      "training loss: 0.2078426331281662\n",
      "validation loss: 0.21951758861541748\n",
      "training epoch nr 75\n",
      "training loss: 0.20762377977371216\n",
      "validation loss: 0.21999089419841766\n",
      "training epoch nr 76\n",
      "training loss: 0.20784950256347656\n",
      "validation loss: 0.22168834507465363\n",
      "training epoch nr 77\n",
      "training loss: 0.20771145820617676\n",
      "validation loss: 0.22060982882976532\n",
      "training epoch nr 78\n",
      "training loss: 0.20740360021591187\n",
      "validation loss: 0.2205580770969391\n",
      "training epoch nr 79\n",
      "training loss: 0.2071956992149353\n",
      "validation loss: 0.22009599208831787\n",
      "training epoch nr 80\n",
      "training loss: 0.20754727721214294\n",
      "validation loss: 0.22159403562545776\n",
      "training epoch nr 81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.20738515257835388\n",
      "validation loss: 0.2205781489610672\n",
      "training epoch nr 82\n",
      "training loss: 0.20726615190505981\n",
      "validation loss: 0.22012102603912354\n",
      "training epoch nr 83\n",
      "training loss: 0.2067732959985733\n",
      "validation loss: 0.22185204923152924\n",
      "training epoch nr 84\n",
      "training loss: 0.20722928643226624\n",
      "validation loss: 0.22160471975803375\n",
      "training epoch nr 85\n",
      "training loss: 0.2067716419696808\n",
      "validation loss: 0.2204953283071518\n",
      "training epoch nr 86\n",
      "training loss: 0.20669607818126678\n",
      "validation loss: 0.22109541296958923\n",
      "training epoch nr 87\n",
      "training loss: 0.20698295533657074\n",
      "validation loss: 0.2215975821018219\n",
      "training epoch nr 88\n",
      "training loss: 0.20681658387184143\n",
      "validation loss: 0.2212776094675064\n",
      "training epoch nr 89\n",
      "training loss: 0.20656268298625946\n",
      "validation loss: 0.2221362292766571\n",
      "training epoch nr 90\n",
      "training loss: 0.20615017414093018\n",
      "validation loss: 0.225407674908638\n",
      "training epoch nr 91\n",
      "training loss: 0.20641562342643738\n",
      "validation loss: 0.22149650752544403\n",
      "training epoch nr 92\n",
      "training loss: 0.20648238062858582\n",
      "validation loss: 0.22226567566394806\n",
      "training epoch nr 93\n",
      "training loss: 0.20605731010437012\n",
      "validation loss: 0.22128117084503174\n",
      "training epoch nr 94\n",
      "training loss: 0.2063455432653427\n",
      "validation loss: 0.22350911796092987\n",
      "training epoch nr 95\n",
      "training loss: 0.20599056780338287\n",
      "validation loss: 0.22359547019004822\n",
      "training epoch nr 96\n",
      "training loss: 0.2061683088541031\n",
      "validation loss: 0.2245873063802719\n",
      "training epoch nr 97\n",
      "training loss: 0.20580622553825378\n",
      "validation loss: 0.22218002378940582\n",
      "training epoch nr 98\n",
      "training loss: 0.2058948427438736\n",
      "validation loss: 0.22197288274765015\n",
      "training epoch nr 99\n",
      "training loss: 0.20565791428089142\n",
      "validation loss: 0.22542965412139893\n",
      "Training model nr 9...\n",
      "Running a fully supervised training. Sig/bkg labels will be known!\n",
      "training epoch nr 0\n",
      "training loss: 0.2707507908344269\n",
      "validation loss: 0.2343040406703949\n",
      "training epoch nr 1\n",
      "training loss: 0.2307104766368866\n",
      "validation loss: 0.22695697844028473\n",
      "training epoch nr 2\n",
      "training loss: 0.22621728479862213\n",
      "validation loss: 0.22568118572235107\n",
      "training epoch nr 3\n",
      "training loss: 0.22311392426490784\n",
      "validation loss: 0.22299423813819885\n",
      "training epoch nr 4\n",
      "training loss: 0.22169648110866547\n",
      "validation loss: 0.22322861850261688\n",
      "training epoch nr 5\n",
      "training loss: 0.2207993119955063\n",
      "validation loss: 0.22175659239292145\n",
      "training epoch nr 6\n",
      "training loss: 0.2201952338218689\n",
      "validation loss: 0.219767227768898\n",
      "training epoch nr 7\n",
      "training loss: 0.21943068504333496\n",
      "validation loss: 0.22051496803760529\n",
      "training epoch nr 8\n",
      "training loss: 0.21878455579280853\n",
      "validation loss: 0.21998940408229828\n",
      "training epoch nr 9\n",
      "training loss: 0.21770137548446655\n",
      "validation loss: 0.21706773340702057\n",
      "training epoch nr 10\n",
      "training loss: 0.21756160259246826\n",
      "validation loss: 0.2192155122756958\n",
      "training epoch nr 11\n",
      "training loss: 0.21703708171844482\n",
      "validation loss: 0.21936649084091187\n",
      "training epoch nr 12\n",
      "training loss: 0.21692782640457153\n",
      "validation loss: 0.2182423174381256\n",
      "training epoch nr 13\n",
      "training loss: 0.21618349850177765\n",
      "validation loss: 0.2179805040359497\n",
      "training epoch nr 14\n",
      "training loss: 0.21565018594264984\n",
      "validation loss: 0.21874836087226868\n",
      "training epoch nr 15\n",
      "training loss: 0.21561093628406525\n",
      "validation loss: 0.21810242533683777\n",
      "training epoch nr 16\n",
      "training loss: 0.21524222195148468\n",
      "validation loss: 0.21902653574943542\n",
      "training epoch nr 17\n",
      "training loss: 0.21517391502857208\n",
      "validation loss: 0.2165028601884842\n",
      "training epoch nr 18\n",
      "training loss: 0.21459868550300598\n",
      "validation loss: 0.21868127584457397\n",
      "training epoch nr 19\n",
      "training loss: 0.21467457711696625\n",
      "validation loss: 0.21685568988323212\n",
      "training epoch nr 20\n",
      "training loss: 0.21412768959999084\n",
      "validation loss: 0.2181672304868698\n",
      "training epoch nr 21\n",
      "training loss: 0.2141411304473877\n",
      "validation loss: 0.21624454855918884\n",
      "training epoch nr 22\n",
      "training loss: 0.21403342485427856\n",
      "validation loss: 0.21775303781032562\n",
      "training epoch nr 23\n",
      "training loss: 0.21391640603542328\n",
      "validation loss: 0.21827326714992523\n",
      "training epoch nr 24\n",
      "training loss: 0.21388749778270721\n",
      "validation loss: 0.21791738271713257\n",
      "training epoch nr 25\n",
      "training loss: 0.21358154714107513\n",
      "validation loss: 0.21832767128944397\n",
      "training epoch nr 26\n",
      "training loss: 0.2134852558374405\n",
      "validation loss: 0.21758787333965302\n",
      "training epoch nr 27\n",
      "training loss: 0.2128097414970398\n",
      "validation loss: 0.21606771647930145\n",
      "training epoch nr 28\n",
      "training loss: 0.21297413110733032\n",
      "validation loss: 0.2191677987575531\n",
      "training epoch nr 29\n",
      "training loss: 0.21295784413814545\n",
      "validation loss: 0.21869230270385742\n",
      "training epoch nr 30\n",
      "training loss: 0.21314789354801178\n",
      "validation loss: 0.21938765048980713\n",
      "training epoch nr 31\n",
      "training loss: 0.21248604357242584\n",
      "validation loss: 0.21712923049926758\n",
      "training epoch nr 32\n",
      "training loss: 0.21220356225967407\n",
      "validation loss: 0.21685616672039032\n",
      "training epoch nr 33\n",
      "training loss: 0.21219542622566223\n",
      "validation loss: 0.2186511754989624\n",
      "training epoch nr 34\n",
      "training loss: 0.21243001520633698\n",
      "validation loss: 0.21781958639621735\n",
      "training epoch nr 35\n",
      "training loss: 0.21221119165420532\n",
      "validation loss: 0.21680715680122375\n",
      "training epoch nr 36\n",
      "training loss: 0.21171854436397552\n",
      "validation loss: 0.21814806759357452\n",
      "training epoch nr 37\n",
      "training loss: 0.21188636124134064\n",
      "validation loss: 0.2159167379140854\n",
      "training epoch nr 38\n",
      "training loss: 0.2116711586713791\n",
      "validation loss: 0.21920497715473175\n",
      "training epoch nr 39\n",
      "training loss: 0.21140852570533752\n",
      "validation loss: 0.2170821726322174\n",
      "training epoch nr 40\n",
      "training loss: 0.21141840517520905\n",
      "validation loss: 0.21662667393684387\n",
      "training epoch nr 41\n",
      "training loss: 0.21120643615722656\n",
      "validation loss: 0.2176104485988617\n",
      "training epoch nr 42\n",
      "training loss: 0.21110449731349945\n",
      "validation loss: 0.216311976313591\n",
      "training epoch nr 43\n",
      "training loss: 0.21123676002025604\n",
      "validation loss: 0.21797655522823334\n",
      "training epoch nr 44\n",
      "training loss: 0.21098169684410095\n",
      "validation loss: 0.21833917498588562\n",
      "training epoch nr 45\n",
      "training loss: 0.21116837859153748\n",
      "validation loss: 0.21797491610050201\n",
      "training epoch nr 46\n",
      "training loss: 0.21104349195957184\n",
      "validation loss: 0.2179274559020996\n",
      "training epoch nr 47\n",
      "training loss: 0.2104213982820511\n",
      "validation loss: 0.21815139055252075\n",
      "training epoch nr 48\n",
      "training loss: 0.21074676513671875\n",
      "validation loss: 0.21736668050289154\n",
      "training epoch nr 49\n",
      "training loss: 0.2104305624961853\n",
      "validation loss: 0.21684172749519348\n",
      "training epoch nr 50\n",
      "training loss: 0.21009230613708496\n",
      "validation loss: 0.21848958730697632\n",
      "training epoch nr 51\n",
      "training loss: 0.2103578746318817\n",
      "validation loss: 0.22128890454769135\n",
      "training epoch nr 52\n",
      "training loss: 0.2100335657596588\n",
      "validation loss: 0.21679139137268066\n",
      "training epoch nr 53\n",
      "training loss: 0.21010522544384003\n",
      "validation loss: 0.21843098104000092\n",
      "training epoch nr 54\n",
      "training loss: 0.2096608728170395\n",
      "validation loss: 0.21967965364456177\n",
      "training epoch nr 55\n",
      "training loss: 0.2102862298488617\n",
      "validation loss: 0.2170058637857437\n",
      "training epoch nr 56\n",
      "training loss: 0.20976810157299042\n",
      "validation loss: 0.21932224929332733\n",
      "training epoch nr 57\n",
      "training loss: 0.20983979105949402\n",
      "validation loss: 0.2171403020620346\n",
      "training epoch nr 58\n",
      "training loss: 0.2097192108631134\n",
      "validation loss: 0.21722395718097687\n",
      "training epoch nr 59\n",
      "training loss: 0.20965039730072021\n",
      "validation loss: 0.2174379527568817\n",
      "training epoch nr 60\n",
      "training loss: 0.20924080908298492\n",
      "validation loss: 0.21953976154327393\n",
      "training epoch nr 61\n",
      "training loss: 0.20964382588863373\n",
      "validation loss: 0.21747298538684845\n",
      "training epoch nr 62\n",
      "training loss: 0.20930232107639313\n",
      "validation loss: 0.21840137243270874\n",
      "training epoch nr 63\n",
      "training loss: 0.20948657393455505\n",
      "validation loss: 0.21946586668491364\n",
      "training epoch nr 64\n",
      "training loss: 0.20905423164367676\n",
      "validation loss: 0.21952924132347107\n",
      "training epoch nr 65\n",
      "training loss: 0.20918844640254974\n",
      "validation loss: 0.21825607120990753\n",
      "training epoch nr 66\n",
      "training loss: 0.2090553641319275\n",
      "validation loss: 0.21754243969917297\n",
      "training epoch nr 67\n",
      "training loss: 0.20859096944332123\n",
      "validation loss: 0.21925042569637299\n",
      "training epoch nr 68\n",
      "training loss: 0.20856423676013947\n",
      "validation loss: 0.22022655606269836\n",
      "training epoch nr 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.20866726338863373\n",
      "validation loss: 0.21826374530792236\n",
      "training epoch nr 70\n",
      "training loss: 0.20853741466999054\n",
      "validation loss: 0.2189447283744812\n",
      "training epoch nr 71\n",
      "training loss: 0.20843107998371124\n",
      "validation loss: 0.22029878199100494\n",
      "training epoch nr 72\n",
      "training loss: 0.20848870277404785\n",
      "validation loss: 0.21855010092258453\n",
      "training epoch nr 73\n",
      "training loss: 0.20830926299095154\n",
      "validation loss: 0.2218886762857437\n",
      "training epoch nr 74\n",
      "training loss: 0.2082894891500473\n",
      "validation loss: 0.2184862643480301\n",
      "training epoch nr 75\n",
      "training loss: 0.2084088772535324\n",
      "validation loss: 0.218919575214386\n",
      "training epoch nr 76\n",
      "training loss: 0.20824715495109558\n",
      "validation loss: 0.21870948374271393\n",
      "training epoch nr 77\n",
      "training loss: 0.20800325274467468\n",
      "validation loss: 0.21969862282276154\n",
      "training epoch nr 78\n",
      "training loss: 0.2078389823436737\n",
      "validation loss: 0.2199505865573883\n",
      "training epoch nr 79\n",
      "training loss: 0.2079342007637024\n",
      "validation loss: 0.2201385796070099\n",
      "training epoch nr 80\n",
      "training loss: 0.207558736205101\n",
      "validation loss: 0.223119854927063\n",
      "training epoch nr 81\n",
      "training loss: 0.20773153007030487\n",
      "validation loss: 0.2227531522512436\n",
      "training epoch nr 82\n",
      "training loss: 0.20779278874397278\n",
      "validation loss: 0.21972577273845673\n",
      "training epoch nr 83\n",
      "training loss: 0.20757222175598145\n",
      "validation loss: 0.22023262083530426\n",
      "training epoch nr 84\n",
      "training loss: 0.20752543210983276\n",
      "validation loss: 0.22244861721992493\n",
      "training epoch nr 85\n",
      "training loss: 0.20790013670921326\n",
      "validation loss: 0.2192164957523346\n",
      "training epoch nr 86\n",
      "training loss: 0.20739920437335968\n",
      "validation loss: 0.22167028486728668\n",
      "training epoch nr 87\n",
      "training loss: 0.20731793344020844\n",
      "validation loss: 0.21943067014217377\n",
      "training epoch nr 88\n",
      "training loss: 0.2072489708662033\n",
      "validation loss: 0.22119732201099396\n",
      "training epoch nr 89\n",
      "training loss: 0.20694828033447266\n",
      "validation loss: 0.22098952531814575\n",
      "training epoch nr 90\n",
      "training loss: 0.20736706256866455\n",
      "validation loss: 0.22028878331184387\n",
      "training epoch nr 91\n",
      "training loss: 0.20707868039608002\n",
      "validation loss: 0.21900826692581177\n",
      "training epoch nr 92\n",
      "training loss: 0.20701955258846283\n",
      "validation loss: 0.22104322910308838\n",
      "training epoch nr 93\n",
      "training loss: 0.20675404369831085\n",
      "validation loss: 0.21889418363571167\n",
      "training epoch nr 94\n",
      "training loss: 0.20677810907363892\n",
      "validation loss: 0.2200116068124771\n",
      "training epoch nr 95\n",
      "training loss: 0.2068151980638504\n",
      "validation loss: 0.22081618010997772\n",
      "training epoch nr 96\n",
      "training loss: 0.20687179267406464\n",
      "validation loss: 0.22095291316509247\n",
      "training epoch nr 97\n",
      "training loss: 0.20665594935417175\n",
      "validation loss: 0.2215740829706192\n",
      "training epoch nr 98\n",
      "training loss: 0.206563338637352\n",
      "validation loss: 0.2212633192539215\n",
      "training epoch nr 99\n",
      "training loss: 0.20644499361515045\n",
      "validation loss: 0.221309632062912\n",
      "minimum validation loss epochs: [30 28 44 50 31 46 49 32 33 37]\n",
      "minimum validation loss epochs: [54 37 59 34 24 20 30 49 36 31]\n",
      "minimum validation loss epochs: [40 23 17 34 35 20 33 42 37 48]\n",
      "minimum validation loss epochs: [45 55 20 41 34 46 43 22 39 48]\n",
      "minimum validation loss epochs: [54 45 73 26 38 44 46 48 55 34]\n",
      "minimum validation loss epochs: [38 21 22 23 53 24 43 35 36 42]\n",
      "minimum validation loss epochs: [41 32 26 24 44 37 28 45 43 30]\n",
      "minimum validation loss epochs: [16 32 24 34 35 37 50 17 14 26]\n",
      "minimum validation loss epochs: [16 20 22 21 40 28 33 39 50 36]\n",
      "minimum validation loss epochs: [52 17 37 40 27 42 21 35 49 19]\n"
     ]
    }
   ],
   "source": [
    "train_classifier(**classifier_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ff4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from evaluation_utils import load_predictions, tprs_fprs_sics, minumum_validation_loss_ensemble, compare_on_various_runs\n",
    "# evalautes the ROCs and SICs of one given training with minimum validation loss epoch picking\n",
    "def full_single_evaluation(data_dir, prediction_dir, n_ensemble_epochs=10, extra_signal=True,\n",
    "                           sic_range=(0,20), savefig=None, suppress_show=False, return_all=False):\n",
    "    X_test, y_test, predictions, val_losses = load_predictions(\n",
    "        data_dir, prediction_dir, extra_signal=extra_signal)\n",
    "    if predictions.shape[1]==1: ## check if ensembling done already\n",
    "        min_val_loss_predictions = predictions\n",
    "    else:\n",
    "        min_val_loss_predictions = minumum_validation_loss_ensemble(\n",
    "            predictions, val_losses, n_epochs=n_ensemble_epochs)\n",
    "    tprs, fprs, sics = tprs_fprs_sics(min_val_loss_predictions, y_test, X_test)\n",
    "\n",
    "    return compare_on_various_runs(\n",
    "        [tprs], [fprs], [np.zeros(min_val_loss_predictions.shape[0])], [\"\"],\n",
    "        sic_lim=sic_range, savefig=savefig, only_median=False, continuous_colors=False,\n",
    "        reduced_legend=False, suppress_show=suppress_show, return_all=return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2c6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = full_single_evaluation(save_dir, save_dir, n_ensemble_epochs=10,\n",
    "                           extra_signal=not no_extra_signal, sic_range=(0, 20),\n",
    "                           savefig=os.path.join(save_dir, 'result_SIC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590c972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
